<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Introduction to Causal Inference | SWAG Workshops Repository</title>
  <meta name="description" content="Chapter 7 Introduction to Causal Inference | SWAG Workshops Repository" />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Introduction to Causal Inference | SWAG Workshops Repository" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Introduction to Causal Inference | SWAG Workshops Repository" />
  
  
  

<meta name="author" content="UW Statistical Workshops and Applications Group" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="analyzing-surveys-with-r.html"/>
<link rel="next" href="pooled-p-values.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SWAG Workshops Repository</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#rejection-sampling"><i class="fa fa-check"></i><b>2.2</b> Rejection Sampling</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#sampling-resampling-methods"><i class="fa fa-check"></i><b>2.3</b> Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chapter2.html"><a href="chapter2.html#overview-of-the-sampling-importance-resampling-algorithm"><i class="fa fa-check"></i><b>2.3.1</b> Overview of the Sampling-Importance-Resampling Algorithm</a></li>
<li class="chapter" data-level="2.3.2" data-path="chapter2.html"><a href="chapter2.html#computational-considerations"><i class="fa fa-check"></i><b>2.3.2</b> Computational Considerations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#implementation-of-sampling-resampling-methods"><i class="fa fa-check"></i><b>2.4</b> Implementation of Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chapter2.html"><a href="chapter2.html#illustrative-example-with-binary-data"><i class="fa fa-check"></i><b>2.4.1</b> Illustrative Example with Binary Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="chapter2.html"><a href="chapter2.html#practical-considerations-for-choosing-proposal-distributions"><i class="fa fa-check"></i><b>2.4.2</b> Practical Considerations for Choosing Proposal Distributions</a></li>
<li class="chapter" data-level="2.4.3" data-path="chapter2.html"><a href="chapter2.html#comparing-proposal-distributions"><i class="fa fa-check"></i><b>2.4.3</b> Comparing Proposal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#sampling-resampling-in-multiple-dimensions"><i class="fa fa-check"></i><b>2.5</b> Sampling-Resampling in Multiple Dimensions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="chapter2.html"><a href="chapter2.html#exercise-with-illustrative-example"><i class="fa fa-check"></i><b>2.5.1</b> Exercise with Illustrative Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html"><i class="fa fa-check"></i><b>3</b> Bernstein-von Mises Theorem</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#bayesian-inference"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#theorem"><i class="fa fa-check"></i><b>3.2</b> Theorem</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#importance"><i class="fa fa-check"></i><b>3.2.1</b> Importance</a></li>
<li class="chapter" data-level="3.2.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#required-assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Required Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-1---normal-normal-model"><i class="fa fa-check"></i><b>3.2.3</b> Example 1 - Normal-normal model</a></li>
<li class="chapter" data-level="3.2.4" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-2---bernoulli-beta-model"><i class="fa fa-check"></i><b>3.2.4</b> Example 2 - Bernoulli-Beta Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#limitations"><i class="fa fa-check"></i><b>3.3</b> Limitations</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#other-thoughts-on-consistency"><i class="fa fa-check"></i><b>3.3.1</b> Other thoughts on consistency</a></li>
<li class="chapter" data-level="3.3.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-3---prior-has-zero-density-at-theta_0"><i class="fa fa-check"></i><b>3.3.2</b> Example 3 - Prior has zero density at <span class="math inline">\(\theta_0\)</span></a></li>
<li class="chapter" data-level="3.3.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-4---true-parameter-value-is-on-the-boundary"><i class="fa fa-check"></i><b>3.3.3</b> Example 4 - True parameter value is on the boundary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>4</b> Variational Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="variational-inference.html"><a href="variational-inference.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="variational-inference.html"><a href="variational-inference.html#frequentist-setting"><i class="fa fa-check"></i><b>4.1.1</b> Frequentist Setting</a></li>
<li class="chapter" data-level="4.1.2" data-path="variational-inference.html"><a href="variational-inference.html#bayesian-setting"><i class="fa fa-check"></i><b>4.1.2</b> Bayesian Setting</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variational-inference.html"><a href="variational-inference.html#example-mixture-of-gaussians"><i class="fa fa-check"></i><b>4.2</b> Example: Mixture of Gaussians</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="variational-inference.html"><a href="variational-inference.html#coordinate-ascent-mean-field-variational-inference"><i class="fa fa-check"></i><b>4.2.1</b> Coordinate Ascent Mean-Field Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variational-inference.html"><a href="variational-inference.html#example-stochastic-variational-inference-using-pyro-in-python"><i class="fa fa-check"></i><b>4.3</b> Example: Stochastic Variational Inference using Pyro in Python</a></li>
<li class="chapter" data-level="4.4" data-path="variational-inference.html"><a href="variational-inference.html#takeaways"><i class="fa fa-check"></i><b>4.4</b> Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html"><i class="fa fa-check"></i><b>5</b> Smoothing Techniques</a>
<ul>
<li class="chapter" data-level="5.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#kernel-smoothing-methods"><i class="fa fa-check"></i><b>5.2</b> Kernel Smoothing Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#local-linear-regression"><i class="fa fa-check"></i><b>5.2.1</b> Local linear regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#tuning-parameter-bandwidth-selection"><i class="fa fa-check"></i><b>5.2.2</b> Tuning Parameter (bandwidth) Selection</a></li>
<li class="chapter" data-level="5.2.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#extension-and-example-local-logistic-regression"><i class="fa fa-check"></i><b>5.2.3</b> Extension and example: Local logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#smoothing-spline"><i class="fa fa-check"></i><b>5.3</b> Smoothing Spline</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#computation-1"><i class="fa fa-check"></i><b>5.3.1</b> Computation</a></li>
<li class="chapter" data-level="5.3.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#tuning-parameter-selection"><i class="fa fa-check"></i><b>5.3.2</b> Tuning Parameter Selection</a></li>
<li class="chapter" data-level="5.3.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#extension-and-example-nonparametric-logistic-regression"><i class="fa fa-check"></i><b>5.3.3</b> Extension and example: Nonparametric logistic regression</a></li>
<li class="chapter" data-level="5.3.4" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#take-home-note"><i class="fa fa-check"></i><b>5.3.4</b> Take-home note</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html"><i class="fa fa-check"></i><b>6</b> Analyzing Surveys with R</a>
<ul>
<li class="chapter" data-level="6.1" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#acknowlegdements"><i class="fa fa-check"></i><b>6.1</b> Acknowlegdements</a></li>
<li class="chapter" data-level="6.2" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#introduction-to-thinking-about-surveys"><i class="fa fa-check"></i><b>6.2</b> Introduction to Thinking About Surveys</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#total-survey-error-tse"><i class="fa fa-check"></i><b>6.2.1</b> Total Survey Error (TSE)</a></li>
<li class="chapter" data-level="6.2.2" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#probability-vs.-non-probability-surveys"><i class="fa fa-check"></i><b>6.2.2</b> Probability vs. Non-Probability Surveys</a></li>
<li class="chapter" data-level="6.2.3" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#design-based-vs.-model-based-inference"><i class="fa fa-check"></i><b>6.2.3</b> Design-based vs. Model-based Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#probability-survey-without-non-response-design-based"><i class="fa fa-check"></i><b>6.3</b> Probability Survey without Non-response (design-based)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#some-notation"><i class="fa fa-check"></i><b>6.3.1</b> Some notation</a></li>
<li class="chapter" data-level="6.3.2" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#horvitz-thompson-estimator"><i class="fa fa-check"></i><b>6.3.2</b> Horvitz-Thompson Estimator</a></li>
<li class="chapter" data-level="6.3.3" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#variance-estimation"><i class="fa fa-check"></i><b>6.3.3</b> Variance Estimation</a></li>
<li class="chapter" data-level="6.3.4" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#simple-random-sampling-without-replacement-srswor"><i class="fa fa-check"></i><b>6.3.4</b> Simple Random Sampling Without Replacement (SRSwor)</a></li>
<li class="chapter" data-level="6.3.5" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#stratified-simple-random-sampling-without-replacement-ssrswor"><i class="fa fa-check"></i><b>6.3.5</b> Stratified Simple Random Sampling Without Replacement (SSRSwor)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#post-adjustment-model-assisted"><i class="fa fa-check"></i><b>6.4</b> Post-adjustment (model-assisted)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#post-stratification"><i class="fa fa-check"></i><b>6.4.1</b> Post-stratification</a></li>
<li class="chapter" data-level="6.4.2" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#calibration-dealing-with-multiple-auxilliary-variables"><i class="fa fa-check"></i><b>6.4.2</b> Calibration: Dealing with multiple auxilliary variables</a></li>
<li class="chapter" data-level="6.4.3" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#what-makes-a-good-calibration-variable"><i class="fa fa-check"></i><b>6.4.3</b> What makes a good calibration variable?</a></li>
<li class="chapter" data-level="6.4.4" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#variance-estimation-resampling"><i class="fa fa-check"></i><b>6.4.4</b> Variance Estimation: Resampling</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="analyzing-surveys-with-r.html"><a href="analyzing-surveys-with-r.html#non-probability-surveys-model-based"><i class="fa fa-check"></i><b>6.5</b> Non-probability Surveys (model-based)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html"><i class="fa fa-check"></i><b>7</b> Introduction to Causal Inference</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#basic-concepts"><i class="fa fa-check"></i><b>7.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#notation"><i class="fa fa-check"></i><b>7.1.1</b> Notation</a></li>
<li class="chapter" data-level="7.1.2" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#causal-effect"><i class="fa fa-check"></i><b>7.1.2</b> Causal Effect</a></li>
<li class="chapter" data-level="7.1.3" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#assumptions"><i class="fa fa-check"></i><b>7.1.3</b> Assumptions</a></li>
<li class="chapter" data-level="7.1.4" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#propensity-score"><i class="fa fa-check"></i><b>7.1.4</b> Propensity Score</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#commonly-used-methods"><i class="fa fa-check"></i><b>7.2</b> Commonly Used Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#matching"><i class="fa fa-check"></i><b>7.2.1</b> Matching</a></li>
<li class="chapter" data-level="7.2.2" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#stratification"><i class="fa fa-check"></i><b>7.2.2</b> Stratification</a></li>
<li class="chapter" data-level="7.2.3" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#inverse-probability-weighting"><i class="fa fa-check"></i><b>7.2.3</b> Inverse Probability Weighting</a></li>
<li class="chapter" data-level="7.2.4" data-path="introduction-to-causal-inference.html"><a href="introduction-to-causal-inference.html#doubly-robust-estimation"><i class="fa fa-check"></i><b>7.2.4</b> Doubly Robust Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pooled-p-values.html"><a href="pooled-p-values.html"><i class="fa fa-check"></i><b>8</b> Pooled p-values</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pooled-p-values.html"><a href="pooled-p-values.html#a-motivating-example"><i class="fa fa-check"></i><b>8.1</b> A motivating example</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pooled-p-values.html"><a href="pooled-p-values.html#some-real-data"><i class="fa fa-check"></i><b>8.1.1</b> Some “real” data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pooled-p-values.html"><a href="pooled-p-values.html#pooling-p-values"><i class="fa fa-check"></i><b>8.2</b> Pooling <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="8.3" data-path="pooled-p-values.html"><a href="pooled-p-values.html#adjusting-for-dependence"><i class="fa fa-check"></i><b>8.3</b> Adjusting for dependence</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="pooled-p-values.html"><a href="pooled-p-values.html#the-wrong-way"><i class="fa fa-check"></i><b>8.3.1</b> The wrong way</a></li>
<li class="chapter" data-level="8.3.2" data-path="pooled-p-values.html"><a href="pooled-p-values.html#a-better-way"><i class="fa fa-check"></i><b>8.3.2</b> A better way</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="pooled-p-values.html"><a href="pooled-p-values.html#take-aways"><i class="fa fa-check"></i><b>8.4</b> Take aways</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>9</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SWAG Workshops Repository</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-causal-inference" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Introduction to Causal Inference<a href="introduction-to-causal-inference.html#introduction-to-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Jingyue Huang</em></p>
<p>In this workshop, we will introduce the basic concepts and framework in causal inference, followed by some commonly used methods, including matching, stratification, inverse probability weighting, and doubly robust estimation. Some materials can be found in the books “Causal Inference in Statistics, Social, and Biomedical Science” <span class="citation">(Imbens and Rubin 2015)</span> and “Causal Inference: What If” <span class="citation">(Hernán and Robins 2020)</span>. I also made use of course slides from STAT 931 offered by Professor Yeying Zhu when I was taking this course.</p>
<div id="basic-concepts" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Basic Concepts<a href="introduction-to-causal-inference.html#basic-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For causal inference, we are interested in causation. There are two notions of causation. One is the causes of an outcome, such as “what causes lung cancer?” The other one is the effect of a cause. For example, we may ask: “Does smoking cause lung cancer?” and “How strong is the effect?” In this workshop, we focus on the effect of a cause.</p>
<p><strong>Note: Correlation/association does not imply causation.</strong></p>
<p><em>Example</em>: Weight and hight are associated with each other. But more weight will not cause someone higher.</p>
<div id="notation" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Notation<a href="introduction-to-causal-inference.html#notation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have data on subjects <span class="math inline">\(i=1,\dots, n\)</span></p>
<ul>
<li><span class="math inline">\(X_i=(X_{i1}, \dots, X_{ip})^\intercal\)</span>: covariates/potential <em>confounders</em></li>
<li><span class="math inline">\(T_i\)</span>: treatment assignment; <span class="math inline">\(T_i=1\)</span> if treated and <span class="math inline">\(T_i=0\)</span> if untreated (control)</li>
<li><span class="math inline">\(Y_i\)</span>: observed outcome</li>
</ul>
<p><strong>Potential outcomes</strong></p>
<ul>
<li><span class="math inline">\(Y_{1i}\)</span>: potential outcome if treated</li>
<li><span class="math inline">\(Y_{0i}\)</span>: potential outcome if untreated</li>
<li>Note that <span class="math inline">\(Y_i=T_iY_{1i}+(1-T_i)Y_{0i}\)</span>.</li>
</ul>
</div>
<div id="causal-effect" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Causal Effect<a href="introduction-to-causal-inference.html#causal-effect" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>individual-level</em> causal effect for subject <span class="math inline">\(i\)</span>: <span class="math inline">\(Y_{1i}-Y_{0i}\)</span></p>
<p><strong>Average treatment Effect (ATE)</strong>: <span class="math inline">\(\theta=E(Y_{1i}-Y_{0i})=E(Y_{1i})-E(Y_{0i})\)</span></p>
<p><em>Confounders</em>: Covariates which are associated with the treatment assignment and potential outcomes simultaneously.</p>
<p><em>Example</em>: Gender (<span class="math inline">\(X\)</span>), Smoking (<span class="math inline">\(T\)</span>), Life expectancy (<span class="math inline">\(Y\)</span>)</p>
<p><em>Association</em>: <span class="math inline">\(E(Y_{1i}\mid T_i=1)-E(Y_{0i}\mid T_i=0)\)</span></p>
</div>
<div id="assumptions" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Assumptions<a href="introduction-to-causal-inference.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Strongly Ignorable Treatment Assignment (SITA).</strong> The treatment indicator (<span class="math inline">\(T\)</span>) and the response variables (<span class="math inline">\(Y_{1}\)</span>, <span class="math inline">\(Y_{0}\)</span>) are independent given the set of covariates (<span class="math inline">\(X\)</span>); <span class="math inline">\((Y_{1}, Y_{0}) \perp T\mid X\)</span>.</li>
<li><strong>Stable Unit Treatment Value Assumption (SUTVA).</strong> Each subject’s potential outcomes are not influenced by the actual treatment status of other subjects; <span class="math inline">\((Y_{1i}, Y_{0i}) \perp T_j\)</span> for <span class="math inline">\(i\neq j\)</span>.</li>
<li><strong>Positvity.</strong> <span class="math inline">\(0&lt;P(T=1\mid X=\boldsymbol{x})&lt;1\)</span> for any possible value <span class="math inline">\(\boldsymbol{x}\)</span>.</li>
</ul>
</div>
<div id="propensity-score" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Propensity Score<a href="introduction-to-causal-inference.html#propensity-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>propensity score</strong> for the treatment assignment is defined as the conditional probability of choosing treatment given the covariates and response variables, that is, <span class="math inline">\(\tau=P(T=1\mid Y_{1}, Y_{0}, X)\)</span>.
Under the SITA assumption, it is true that the propensity score <span class="math inline">\(\tau=P(T=1\mid X=x)\)</span>, which is a function of <span class="math inline">\(x\)</span>.</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Propensity score is a balancing score; <span class="math inline">\(X \perp T\mid \tau\)</span>.</li>
<li>If the treatment is strongly ignorable given <span class="math inline">\(X\)</span>, i.e., <span class="math inline">\((Y_{1}, Y_{0}) \perp T\mid X\)</span>, then it is strongly ignorable given <span class="math inline">\(\tau\)</span>, i.e., <span class="math inline">\((Y_{1}, Y_{0}) \perp T\mid \tau\)</span>.</li>
</ul>
<p>Propensity scores are unknown and need to be estimated. Indeed, we model <span class="math inline">\(T\)</span> (assuming binary) as a function of <span class="math inline">\(X\)</span>. Parametric (logistic or probit regression) or nonparametric (random forest, generalized boosted model, etc.) methods can be applied.</p>
</div>
</div>
<div id="commonly-used-methods" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Commonly Used Methods<a href="introduction-to-causal-inference.html#commonly-used-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The general framework for the propensity score based methods involves two steps:</p>
<ul>
<li>Get the estimated propensity scores <span class="math inline">\(\hat{\tau}_i\)</span> for <span class="math inline">\(i=1, \dots, n\)</span> based on the available data <span class="math inline">\((T_i, X_i), i=1, \dots, n\)</span>;</li>
<li>Using <span class="math inline">\(\hat{\tau}_i\)</span> to adjust the original sample and estimate the average treatment effect.</li>
</ul>
<div id="matching" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Matching<a href="introduction-to-causal-inference.html#matching" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Basic idea</em>: <span class="math inline">\((Y_{1}, Y_{0}) \perp T\mid X\)</span>.</p>
<p>For each subject in the treated group, if we can find an untreated subject with the same (or similar) covariates (<span class="math inline">\(X\)</span>), they can form a matched dataset where the property <span class="math inline">\((Y_{1}, Y_{0}) \perp T\)</span> holds. This means we can estimate the average treatment effect as in a randomized study.</p>
<p><em>Problem</em>: If the size of <span class="math inline">\(X\)</span> is moderate or high-dimensional, it is hard to get a matched dataset (curse of dimensionality).</p>
<p><em>Solution</em>: Use <span class="math inline">\(\tau(X)\)</span> instead of <span class="math inline">\(X\)</span> because <span class="math inline">\((Y_{1}, Y_{0}) \perp T\mid \tau\)</span>.</p>
<p><strong>Algorithm</strong>: One-to-one nearest available matching on estimated propensity scores.</p>
<ul>
<li>Randomly order the treated and untreated (control) subjects;</li>
<li>Select the first treated subject and find the control subject with the closest propensity score;</li>
<li>Both subjects are then removed from the pool, and then repeat the second step until all the treated subjects are matched;</li>
<li>Estimate the causal effects as in a randomized study, e.g.,
<span class="math display">\[
\hat{\theta}_{\mathrm{M}}=\frac{\sum_{i\in M}T_i Y_{1i}}{\sum_{i\in M} T_i}-\frac{\sum_{i\in M}(1-T_i) Y_{0i}}{\sum_{i\in M} (1-T_i)}\, ,
\]</span>
where <span class="math inline">\(M\)</span> denotes the matched dataset.</li>
</ul>
<p>There are other versions of the matching algorithm: one-to-one versus one-to-m, with replacement versus without replacement, etc.</p>
</div>
<div id="stratification" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Stratification<a href="introduction-to-causal-inference.html#stratification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Basic idea</em>: Consider strata, <span class="math inline">\(S_1, \dots, S_K\)</span>,
<span class="math display">\[
E(Y_1-Y_0)=\sum_{k=1}^KE(Y_1-Y_0\mid X\in S_k)P(X\in S_k)\, ,
\]</span>
where we have balance within each stratum.</p>
<ul>
<li><span class="math inline">\(E(Y_1-Y_0\mid X\in S_k)\)</span> can be estimated as in a randomized study using data in <span class="math inline">\(S_k\)</span>;</li>
<li><span class="math inline">\(P(X\in S_k)\)</span> can be approximated by (number of subjects in <span class="math inline">\(S_k\)</span>)<span class="math inline">\(/n\)</span>.</li>
</ul>
<p>We may encounter the same problem as before when <span class="math inline">\(X\)</span> is high-dimensional. Again, we can solve this problem by replacing <span class="math inline">\(X\)</span> with the propensity score <span class="math inline">\(\tau(X)\)</span>:
<span class="math display">\[
E(Y_1-Y_0)=\sum_{k=1}^KE(Y_1-Y_0\mid \tau(X)\in S_k)P(\tau(X)\in S_k)\, ,
\]</span>
where individuals have similar, but not identical, values of <span class="math inline">\(\tau\)</span>, in each stratum.</p>
<p><strong>Algorithm</strong>:</p>
<ul>
<li>Divide the units into <span class="math inline">\(K\)</span> subclasses based on the quantiles of <span class="math inline">\(\hat{\tau}_i\)</span>’s (<span class="math inline">\(i=1,...,n\)</span>);</li>
<li>Estimate the average treatment effect within each subclass as in a randomized study and take the average of the estimated values across all the subclasses; mathematically, the estimate for the average treatment effect is given by
<span class="math display">\[
  \hat{\theta}_{\mathrm{S}}=\frac{1}{K}\sum_{
  k=1}^{K}\left(\frac{\sum_{i\in S_k} Y_{1i}T_i}{\sum_{i\in S_k} T_i}-\frac{\sum_{i\in S_k} Y_{0i}(1-T_i)}{\sum_{i\in S_k} (1-T_i)}\right)\, ,
\]</span>
where <span class="math inline">\(S_k\)</span> means the <span class="math inline">\(k\)</span>-th subclass.</li>
</ul>
<p>How to choose <span class="math inline">\(K\)</span>? Usually 5.</p>
</div>
<div id="inverse-probability-weighting" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Inverse Probability Weighting<a href="introduction-to-causal-inference.html#inverse-probability-weighting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Basic idea</em>:</p>
<p><span class="math display">\[
E\left\{\frac{TY}{\tau(X)}\right\}=E(Y_1) \text{ and } E\left\{\frac{(1-T)Y}{1-\tau(X)}\right\}=E(Y_0)\, .
\]</span></p>
<p>The inverse probability weighted estimator of the average treatment effect is defined as
<span class="math display">\[
    \hat{\theta}_{\mathrm{IPW}_1}=\frac{1}{n} \sum_{i=1}^n \frac{T_iY_i}{\hat{\tau}_i}-\frac{1}{n} \sum_{i=1}^n\frac{(1-T_i)Y_i}{1-\hat{\tau}_i}\, .
\]</span></p>
<p>We give each subject a weight <span class="math inline">\(w_i\)</span>, where <span class="math inline">\(w_i=\hat{\tau}_i^{-1}\)</span> for those in the treatment group and <span class="math inline">\(w_i=(1-\hat{\tau}_i)^{-1}\)</span> for those in the control group. By weighting, each subject is replicated <span class="math inline">\(w_i\)</span> times. This creates a psuedo-population in which <span class="math inline">\(T\)</span> and <span class="math inline">\(X\)</span> are not associated anymore (no confounding).</p>
<p>A more efficient estimator is</p>
<p><span class="math display">\[
\hat{\theta}_{\mathrm{IPW}_2}=\left(\sum_{i=1}^n\frac{T_i}{\hat{\tau}_i}\right)^{-1} \sum_{i=1}^n \frac{T_iY_i}{\hat{\tau}_i}-\left(\sum_{i=1}^n\frac{1-T_i}{1-\hat{\tau}_i}\right)^{-1} \sum_{i=1}^n\frac{(1-T_i)Y_i}{1-\hat{\tau}_i}\, .
\]</span></p>
</div>
<div id="doubly-robust-estimation" class="section level3 hasAnchor" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Doubly Robust Estimation<a href="introduction-to-causal-inference.html#doubly-robust-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Problem</em>: If the propensity score model is incorrect, Matching, Stratfication and IPW estimators will be biased.</p>
<p><em>Solution</em>: Combine IPW with the regression modeling approach to protect against model misspecification.</p>
<p>Note that we now have two sets of models:</p>
<ul>
<li>The model for the propensity score;</li>
<li>The models for the potential outcomes:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
m_1(X;{\beta}_1)&amp;={E}(Y_{1}\mid X)={E}(Y_{1}\mid X, T=1)\\
m_0(X;{\beta}_0)&amp;=E(Y_{0}\mid X)={E}(Y_{0}\mid X, T=0)
\end{aligned}
\]</span>
The so-called <strong>doubly robust</strong> estimators are consistent if one of the two sets of models is correctly specified.</p>
<p>The doubly robust estimator for the average treatment effect is
<span class="math display">\[
\hat{\theta}_{\mathrm{DR}}=\frac{1}{n} \sum_{i=1}^{n} \frac{T_{i} Y_{1i}-\left(T_i-\hat{\tau}_i\right)\hat{m}_{1i}}{\hat{\tau}_{i}}-\frac{1}{n} \sum_{i=1}^{n} \frac{(1-T_{i}) Y_{0i}+\left(T_i-\hat{\tau}_i\right)\hat{m}_{0i}}{1-\hat{\tau}_{i}}\, .
\]</span></p>
<p>We can see that this estimator consists of the inverse probability weighted estimator and a second term used for “augmenting”.
So, the doubly robust estimators are also called the <strong>augmented inverse probability weighted (AIPW)</strong> estimators.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="analyzing-surveys-with-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pooled-p-values.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/uwswagclub/workshop-bookdown/edit/master/07-into-to-causal-inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/uwswagclub/workshop-bookdown/blob/master/07-into-to-causal-inference.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
