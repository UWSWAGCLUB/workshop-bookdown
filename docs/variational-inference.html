<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Variational Inference | SWAG Workshops Repository</title>
  <meta name="description" content="Chapter 4 Variational Inference | SWAG Workshops Repository" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Variational Inference | SWAG Workshops Repository" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Variational Inference | SWAG Workshops Repository" />
  
  
  

<meta name="author" content="UW Statistical Workshops and Applications Group" />


<meta name="date" content="2022-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bernstein-von-mises-theorem.html"/>
<link rel="next" href="smoothing-techniques.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SWAG Workshops Repository</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#rejection-sampling"><i class="fa fa-check"></i><b>2.2</b> Rejection Sampling</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#sampling-resampling-methods"><i class="fa fa-check"></i><b>2.3</b> Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chapter2.html"><a href="chapter2.html#overview-of-the-sampling-importance-resampling-algorithm"><i class="fa fa-check"></i><b>2.3.1</b> Overview of the Sampling-Importance-Resampling Algorithm</a></li>
<li class="chapter" data-level="2.3.2" data-path="chapter2.html"><a href="chapter2.html#computational-considerations"><i class="fa fa-check"></i><b>2.3.2</b> Computational Considerations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#implementation-of-sampling-resampling-methods"><i class="fa fa-check"></i><b>2.4</b> Implementation of Sampling-Resampling Methods</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chapter2.html"><a href="chapter2.html#illustrative-example-with-binary-data"><i class="fa fa-check"></i><b>2.4.1</b> Illustrative Example with Binary Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="chapter2.html"><a href="chapter2.html#practical-considerations-for-choosing-proposal-distributions"><i class="fa fa-check"></i><b>2.4.2</b> Practical Considerations for Choosing Proposal Distributions</a></li>
<li class="chapter" data-level="2.4.3" data-path="chapter2.html"><a href="chapter2.html#comparing-proposal-distributions"><i class="fa fa-check"></i><b>2.4.3</b> Comparing Proposal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#sampling-resampling-in-multiple-dimensions"><i class="fa fa-check"></i><b>2.5</b> Sampling-Resampling in Multiple Dimensions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="chapter2.html"><a href="chapter2.html#exercise-with-illustrative-example"><i class="fa fa-check"></i><b>2.5.1</b> Exercise with Illustrative Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html"><i class="fa fa-check"></i><b>3</b> Bernstein-von Mises Theorem</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#bayesian-inference"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#theorem"><i class="fa fa-check"></i><b>3.2</b> Theorem</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#importance"><i class="fa fa-check"></i><b>3.2.1</b> Importance</a></li>
<li class="chapter" data-level="3.2.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#required-assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Required Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-1---normal-normal-model"><i class="fa fa-check"></i><b>3.2.3</b> Example 1 - Normal-normal model</a></li>
<li class="chapter" data-level="3.2.4" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-2---bernoulli-beta-model"><i class="fa fa-check"></i><b>3.2.4</b> Example 2 - Bernoulli-Beta Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#limitations"><i class="fa fa-check"></i><b>3.3</b> Limitations</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#other-thoughts-on-consistency"><i class="fa fa-check"></i><b>3.3.1</b> Other thoughts on consistency</a></li>
<li class="chapter" data-level="3.3.2" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-3---prior-has-zero-density-at-theta_0"><i class="fa fa-check"></i><b>3.3.2</b> Example 3 - Prior has zero density at <span class="math inline">\(\theta_0\)</span></a></li>
<li class="chapter" data-level="3.3.3" data-path="bernstein-von-mises-theorem.html"><a href="bernstein-von-mises-theorem.html#example-4---true-parameter-value-is-on-the-boundary"><i class="fa fa-check"></i><b>3.3.3</b> Example 4 - True parameter value is on the boundary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>4</b> Variational Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="variational-inference.html"><a href="variational-inference.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="variational-inference.html"><a href="variational-inference.html#frequentist-setting"><i class="fa fa-check"></i><b>4.1.1</b> Frequentist Setting</a></li>
<li class="chapter" data-level="4.1.2" data-path="variational-inference.html"><a href="variational-inference.html#bayesian-setting"><i class="fa fa-check"></i><b>4.1.2</b> Bayesian Setting</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variational-inference.html"><a href="variational-inference.html#example-mixture-of-gaussians"><i class="fa fa-check"></i><b>4.2</b> Example: Mixture of Gaussians</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="variational-inference.html"><a href="variational-inference.html#coordinate-ascent-mean-field-variational-inference"><i class="fa fa-check"></i><b>4.2.1</b> Coordinate Ascent Mean-Field Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variational-inference.html"><a href="variational-inference.html#example-stochastic-variational-inference-using-pyro-in-python"><i class="fa fa-check"></i><b>4.3</b> Example: Stochastic Variational Inference using Pyro in Python</a></li>
<li class="chapter" data-level="4.4" data-path="variational-inference.html"><a href="variational-inference.html#takeaways"><i class="fa fa-check"></i><b>4.4</b> Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html"><i class="fa fa-check"></i><b>5</b> Smoothing Techniques</a>
<ul>
<li class="chapter" data-level="5.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#kernel-smoothing-methods"><i class="fa fa-check"></i><b>5.2</b> Kernel Smoothing Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#local-linear-regression"><i class="fa fa-check"></i><b>5.2.1</b> Local linear regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#tuning-parameter-bandwidth-selection"><i class="fa fa-check"></i><b>5.2.2</b> Tuning Parameter (bandwidth) Selection</a></li>
<li class="chapter" data-level="5.2.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#extension-and-example-local-logistic-regression"><i class="fa fa-check"></i><b>5.2.3</b> Extension and example: Local logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#smoothing-spline"><i class="fa fa-check"></i><b>5.3</b> Smoothing Spline</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#computation-1"><i class="fa fa-check"></i><b>5.3.1</b> Computation</a></li>
<li class="chapter" data-level="5.3.2" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#tuning-parameter-selection"><i class="fa fa-check"></i><b>5.3.2</b> Tuning Parameter Selection</a></li>
<li class="chapter" data-level="5.3.3" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#extension-and-example-nonparametric-logistic-regression"><i class="fa fa-check"></i><b>5.3.3</b> Extension and example: Nonparametric logistic regression</a></li>
<li class="chapter" data-level="5.3.4" data-path="smoothing-techniques.html"><a href="smoothing-techniques.html#take-home-note"><i class="fa fa-check"></i><b>5.3.4</b> Take-home note</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SWAG Workshops Repository</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variational-inference" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Variational Inference<a href="variational-inference.html#variational-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Meixi Chen</em></p>
<div id="introduction-3" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction<a href="variational-inference.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Variational inference (VI) is an inference technique that is commonly used to approximate an intractable quantity such as a probability density. In the following, we introduce two scenarios where the VI is often used, one in a frequentist setting and the other in a Bayesian setting.</p>
<div id="frequentist-setting" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Frequentist Setting<a href="variational-inference.html#frequentist-setting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The frequentist example is adapted from the one in <span class="citation">Chen (2020)</span>. Consider the following IID observations <span class="math inline">\(Y_1,\ldots,Y_n\)</span>. Now assume that each <span class="math inline">\(Y_i\)</span> is accompanied with an unobserved latent variable <span class="math inline">\(Z_i\)</span>. That is, the complete data is <span class="math inline">\((Y_1,Z_1),\ldots,(Y_n,Z_n)\)</span>, but we only observe the incomplete data <span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</p>
<p>Assume that we know the parametric model for the complete data is <span class="math inline">\(p(y, z\mid \theta)\)</span>, and our interest lies in estimating <span class="math inline">\(\theta\)</span>. One way to do it is to maximize the observed log-likelihood <span class="math inline">\(\ell(\theta\mid y_1,\ldots,y_n) = \sum_{i=1}^n \log p(y_i\mid\theta)\)</span>, where
<span class="math display" id="eq:pyz-int">\[\begin{equation}
\tag{4.1}
  p(y_i\mid\theta)=\int p(y_i, z_i\mid \theta) \ dz_i.
\end{equation}\]</span>
However, this integral is typically difficult to compute. Many techniques exist to deal with the problem of <a href="variational-inference.html#eq:pyz-int">(4.1)</a> (e.g., MCMC, Laplace approximation, EM). The VI is one such method to solve this problem, which writes
<span class="math display" id="eq:approx-int">\[\begin{equation}
\tag{4.2}
  \begin{aligned}
    p(y\mid \theta) &amp;= \int p(y, z\mid \theta)\ dz\\
    &amp;= \int \frac{p(y, z\mid \theta)}{\color{red}{q(z\mid \omega)}}\color{red}{q(z\mid \omega)} \ dz\\
    &amp;= \mathbb{E}_{Z}\left[\frac{p(y, z\mid \theta)}{\color{red}{q(z\mid \omega)}}\right]
  \end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(Z\sim q(z\mid \omega)\)</span> and <span class="math inline">\(q(\cdot \mid\omega)\)</span> is called the variational distribution and typically has an easy form (e.g. Normal distribution). All possible candidate variational distributions form the variational family <span class="math inline">\(\mathcal{Q}=\{q(\cdot\mid\omega): \ \omega \in \Omega\}\)</span>.</p>
<p>Given <a href="variational-inference.html#eq:approx-int">(4.2)</a>, we can compute the observed log-likelihood as
<span class="math display" id="eq:calc-elbo">\[\begin{equation}
\tag{4.3}
\begin{aligned}
  \ell(\theta\mid y) &amp;= \log p(y\mid \theta)\\
  &amp;= \log \mathbb{E}_{Z}\left[\frac{p(y, z\mid \theta)}{q(z\mid \omega)}\right]\\
  &amp;\ge \mathbb{E}_{Z}\left[\log\frac{p(y, z\mid \theta)}{q(z\mid \omega)}\right] \text{     using Jensen&#39;s inequality}\\
  &amp;=\mathbb{E}_{Z}(\log p(y, z\mid \theta))-\mathbb{E}_{Z}(\log q(z\mid \omega))\\
  &amp;:= \mathrm{ELBO}(\omega,\theta\mid y) := \mathrm{ELBO}(q)
\end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(\mathrm{ELBO}(q)\)</span> is known as the <em>Evidence Lower Bound</em>.</p>
<p>Now, instead of maximizing <span class="math inline">\(\ell(\theta\mid y_1,\ldots,y_n)\)</span>, we can maxmize the ELBO via any numerical optimization algorithm, i.e.,
<span class="math display" id="eq:max-elbo">\[\begin{equation}
\tag{4.4}
  (\hat{\omega}, \hat{\theta}) = \underset{\omega,\theta}{\mathrm{argmax}}\frac{1}{n}\sum_{i=1}^n \mathrm{ELBO}(\omega,\theta\mid y_i).
\end{equation}\]</span></p>
<p><strong>Important notes</strong></p>
<ul>
<li><p>The parametric form of <span class="math inline">\(q(\cdot \mid \omega)\)</span> is up to the modeler. A common choice is Normal. When <span class="math inline">\(q(z \mid \omega)\)</span> is multivariate, i.e., <span class="math inline">\(z, \omega\in\mathbb{R}^n\)</span>, it is common to use the <em>mean-field variational family</em> <span class="math inline">\(q(z\mid \omega)=\prod_{i=1}^n q(z_i\mid \omega_i)\)</span>.</p></li>
<li><p>The VI estimator <span class="math inline">\(\hat{\theta}_{\mathrm{VI}}\)</span> generally does not converge to the MLE because <span class="math inline">\(\hat{\theta}_{\mathrm{VI}}\)</span> depends on the choice of <span class="math inline">\(q(\cdot \mid \omega)\)</span>.</p></li>
</ul>
<p><strong>Uncertainty assessment</strong></p>
<p>One way to assess the uncertainty of the VI estimator <span class="math inline">\(\tilde{\theta}_{\mathrm{VI}}\)</span> is via bootstrapping.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\((Y_1^{(b)}, \ldots,Y_n^{(b)})\)</span> be the <span class="math inline">\(b\)</span>-th bootstrap sample from the original dataset, for <span class="math inline">\(b=1,\ldots, B\)</span>.</p></li>
<li><p>Given the bootstrap sample, we can compute the bootstrap VI estimate <span class="math inline">\(\hat{\theta}_{\mathrm{VI}}^{(b)}\)</span>.</p></li>
<li><p>After repeating the above procedure for <span class="math inline">\(B\)</span> times, we obtain <span class="math inline">\(B\)</span> bootstrap VI estimates: <span class="math inline">\(\hat{\theta}_{\mathrm{VI}}^{(1)}, \ldots, \hat{\theta}_{\mathrm{VI}}^{(B)}\)</span>.</p></li>
<li><p>The bootstrap estimates can be used to calculate the uncertainty of the original bootstrap estimator.</p></li>
</ol>
</div>
<div id="bayesian-setting" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Bayesian Setting<a href="variational-inference.html#bayesian-setting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Bayesian statistics, the VI is often used as an alternative to the traditional MCMC sampling method to estimate the posterior distributions <span class="math inline">\(p(\theta\mid y)\)</span>.</p>
<p>Recall that, according to the Bayes’ rule, the posterior distribution is written as
<span class="math display">\[p(\theta\mid y) = \frac{p(y\mid \theta)p(\theta)}{\color{red}{p(y)}} = \frac{p(y\mid \theta)p(\theta)}{\int p(y\mid \theta) p(\theta)\ d\theta},\]</span>
where <span class="math inline">\(p(y\mid \theta)\)</span> is the likelihood taking a known form and <span class="math inline">\(p(y)\)</span> is a constant. Similar to the problem in <a href="variational-inference.html#eq:pyz-int">(4.1)</a>, the integral <span class="math inline">\(p(y)=\int p(y\mid \theta) p(\theta)\ d\theta\)</span> is typically intractable, which makes calculating the posterior difficult.</p>
<p>What the VI does in this case is to find a variational distribution <span class="math inline">\(q(\theta \mid \omega)\)</span> such that it is close enough to the posterior distribution of interest <span class="math inline">\(p(\theta\mid y)\)</span>. How do we know a distribution is “close enough” to another?</p>
<p>The answer is using the Kullback-Leibler divergence <span class="math inline">\(\mathrm{KL}(Q\Vert P)\)</span>, which measures how different the probability distribution <span class="math inline">\(Q\)</span> is from the reference distribution <span class="math inline">\(P\)</span>.</p>
<p>Therefore, the VI method looks for <span class="math inline">\(q(\theta \mid \omega)\)</span> that minimizes <span class="math inline">\(\mathrm{KL}\big(q(\theta\mid \omega) \Vert p(\theta\mid y)\big)\)</span>, i.e.,
<span class="math display">\[q^*(\theta \mid \omega) = \underset{q(\theta\mid\omega)}{\mathrm{argmin}} \mathrm{KL}\big(q(\theta\mid \omega) \Vert p(\theta\mid y)\big).\]</span></p>
<p>The KL divergence is written as
<span class="math display" id="eq:KL-div">\[\begin{equation}
\tag{4.5}
  \begin{aligned}
    \mathrm{KL}\big(q(\theta\mid \omega) \Vert p(\theta\mid y)\big) &amp;= \mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta\mid \omega)]-\mathbb{E}_{q(\cdot \mid \omega)}[\log p(\theta\mid y)]\\
    &amp;= \mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta\mid \omega)]-\mathbb{E}_{q(\cdot \mid \omega)}\left[\log \frac{p(\theta, y)}{p(y)}\right]\\
    &amp;= \underbrace{\mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta\mid \omega)]-\mathbb{E}_{q(\cdot \mid \omega)}[\log p(\theta, y)]}_{-\mathrm{ELBO(q)}} + \underbrace{\color{red}{\log p(y)}}_{\text{constant}}
  \end{aligned}
\end{equation}\]</span></p>
<p>Noting that <span class="math inline">\(\log p(y)\)</span> is a constant, minimizing the KL divergence <a href="variational-inference.html#eq:KL-div">(4.5)</a> is in fact equivalent to minimizing the nonconstant part of <a href="variational-inference.html#eq:KL-div">(4.5)</a>:
<span class="math display">\[\mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta\mid \omega)]-\mathbb{E}_{q(\cdot \mid \omega)}[\log p(\theta, y)].\]</span>
Then we notice that it is in fact the negative of the ELBO, so minimizing <a href="variational-inference.html#eq:KL-div">(4.5)</a> is in turn equivalent to maximizing the ELBO:
<span class="math display">\[\mathrm{ELBO}(q) = \mathbb{E}_{q(\cdot \mid \omega)}[\log p(\theta, y)]-\mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta\mid \omega)].\]</span></p>
<p>Therefore, we can estimate <span class="math inline">\(\omega\)</span> as
<span class="math display">\[\hat{\omega} = \underset{\omega}{\mathrm{argmax}} \ \mathrm{ELBO}(q) = \underset{\omega}{\mathrm{argmax}} \ \mathrm{ELBO}(\omega\mid y).\]</span></p>
<p>Finally, the posterior of interest is approximated by
<span class="math display">\[p(\theta\mid y) \approx q(\theta \mid \hat{\omega}).\]</span></p>
<p><strong>Important note</strong></p>
<ul>
<li><p>Similar to the frequentist setting, we need to pick the distributional form of <span class="math inline">\(q(\cdot \mid \omega)\)</span>.</p></li>
<li><p>The posterior distribution obtained this way is an approximation rather than the truth, whereas MCMC methods guarantees that the Monte Carlo samples converge to the true posterior distribution.</p></li>
</ul>
<p><strong>Takeaway</strong></p>
<ul>
<li>Only use the VI to estimate the posterior if the dimension of the posterior is too high for sampling, or the model is too complex for MCMC methods to run within a reasonable amount of time (within a day).</li>
</ul>
</div>
</div>
<div id="example-mixture-of-gaussians" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Example: Mixture of Gaussians<a href="variational-inference.html#example-mixture-of-gaussians" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This example is taken from a famous VI tutorial paper by <span class="citation">Blei, Kucukelbir, and McAuliffe (2017)</span>. Consider the following Gaussian mixture model:
<span class="math display" id="eq:mix-gauss">\[\begin{equation}
\tag{4.6}
  \begin{aligned}
    \mu_k &amp;\sim \mathcal{N}(0, \sigma^2), \ \ &amp; k=1,\ldots, K,\\
    c_i &amp;\sim \mathrm{Categorical}(1/K, \ldots, 1/K), \ \ &amp; i=1,\ldots, n,\\
    x_i|c_i, \boldsymbol{\mu} &amp;\sim \mathcal{N}(c_i^T\boldsymbol{\mu}, 1), \ \ &amp; i=1,\ldots,n,
  \end{aligned}
\end{equation}\]</span>
where the prior variance <span class="math inline">\(\sigma^2\)</span> is known, and <span class="math inline">\(c_i=(0,0,\ldots,1,\ldots,0)\)</span> is one-hot encoding.</p>
<p>The parameters of interest are the mean parameters <span class="math inline">\((\mu_1,\ldots,\mu_K)\)</span> and the cluster parameters <span class="math inline">\((c_1,\ldots,c_n)\)</span>. We propose to use the mean-field variational family <span class="math inline">\(q(\boldsymbol{\mu}, \boldsymbol{c})=\prod_{i=1}^Kq(\mu_k)\prod_{i=1}^nq(c_i)\)</span>, where
<span class="math display" id="eq:var-dist">\[\begin{equation}
\tag{4.7}
  \begin{aligned}
    q(\mu_k\mid m_k, s_k^2) &amp;= \mathcal{N}(m_k, s_k^2), \ \ &amp; k=1,\ldots, K\\
    q(c_i\mid \phi_i) &amp;= \mathrm{Categorical}(\phi_{i1}, \ldots, \phi_{iK}), \ \ &amp; i=1,\ldots n.
  \end{aligned}
\end{equation}\]</span></p>
<p>Again, let <span class="math inline">\(\omega=(m, s, \phi)\)</span> denote all the variational parameters, and let <span class="math inline">\(\theta=(\mu_1,\ldots,\mu_K, c_1,\ldots,c_n)\)</span> denote all the parameters of interest. Then the ELBO is written as
<span class="math display" id="eq:elbo-mix-gauss">\[\begin{equation}
\tag{4.8}
  \begin{aligned}
    \mathrm{ELBO}(q) &amp;= \color{blue}{\mathbb{E}_{q(\cdot \mid \omega)}[\log p(\theta, x)]}-\color{purple}{\mathbb{E}_{q(\cdot \mid \omega)}[\log q(\theta \mid \omega)]}\\
    &amp;= \color{blue}{\sum_{k=1}^K \mathbb{E}[\log p(\mu_k); m_k, s_k^2]} \\
    &amp; \ \ \ \ \ \ \color{blue}{+\sum_{i=1}^n \left(\mathbb{E}[\log p(c_i); \phi_i] + \mathbb{E}[\log p(x_i\mid c_i, \mu); \phi, m, s^2]\right)} \\
    &amp; \ \ \ \ \ \ \color{purple}{-\sum_{i=1}^n\mathbb{E}[\log q(c_i\mid \phi_i)] - \sum_{k=1}^K \mathbb{E}[\log q(\mu_k\mid m_k, s_k^2)]}
  \end{aligned}
\end{equation}\]</span>
Note that <span class="math inline">\(\mathbb{E}[a ; b]\)</span> specifies the quantity of interest <span class="math inline">\(a\)</span> depends on the variational parameter <span class="math inline">\(b\)</span>. Each of the above term can be computed in closed form (but omitted here). The next question is, how do we maximize the ELBO?</p>
<div id="coordinate-ascent-mean-field-variational-inference" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Coordinate Ascent Mean-Field Variational Inference<a href="variational-inference.html#coordinate-ascent-mean-field-variational-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The coordinate ascent variational inference (CAVI) is commonly used to solve this optimization problem. It is particularly convenient for the mean-field variational family.</p>
<p><strong>Results:</strong> <span class="citation">(Blei, Kucukelbir, and McAuliffe 2017)</span></p>
<p>Let the <em>full conditional</em> of <span class="math inline">\(\theta_j\)</span> be <span class="math inline">\(p(\theta_j\mid \boldsymbol{\theta}_{-j},x)\)</span>. When all variational distributions <span class="math inline">\(q(\theta_{\ell})\)</span> for <span class="math inline">\(\ell\neq j\)</span> are fixed, the optimal <span class="math inline">\(q(\theta_j)\)</span> is proportionate to the exponentiated expected log complete conditional:
<span class="math display" id="eq:prop-conditional">\[\begin{equation}
\tag{4.9}
  q^*(\theta_j) \propto \exp\left\{\mathbb{E}_{-j}[\log p(\theta_j\mid\boldsymbol{\theta}_{-j}, x)]\right\}
\end{equation}\]</span></p>
<p>This result is used to formulate the CAVI algorithm as follows.</p>
<p><strong>Algorithm: CAVI</strong></p>
<ul>
<li><p><strong>while</strong> the ELBO has not converged <strong>do</strong></p>
<ul>
<li><p><strong>for</strong> <span class="math inline">\(j\in\{1,\ldots,m\}\)</span> <strong>do</strong></p>
<p>Set <span class="math inline">\(q_j(\theta_j) \propto \exp\left\{\mathbb{E}_{-j}[\log p(\theta_j\mid\boldsymbol{\theta}_{-j}, x)]\right\}\)</span></p></li>
<li><p><strong>end</strong></p></li>
<li><p>Compute <span class="math inline">\(\mathrm{ELBO}(q)\)</span></p></li>
</ul></li>
<li><p><strong>end</strong></p></li>
<li><p><strong>Return</strong> <span class="math inline">\(q(\boldsymbol{\theta})\)</span></p></li>
</ul>
<p>For the mixture of Gaussians example, <span class="citation">Blei, Kucukelbir, and McAuliffe (2017)</span> has derived the full conditionals and computed the updating rules for all variational parameters:
<span class="math display" id="eq:full-conds">\[\begin{equation}
\tag{4.10}
  \begin{aligned}
    \phi_{ik} &amp;\propto \exp\{\mathbb{E}[\mu_k;m_k,s_k^2]x_i - \mathbb{E}[\mu_k^2;m_k,s_k^2]/2\} \ \ \text{(normalize afterwards)}\\
    m_k &amp;= \frac{\sum_{i}^n \phi_{ik}x_i}{1/\sigma^2 + \sum_{i=1}^n \phi_{ik}} \\
    s_k^2 &amp;= \frac{1}{1/\sigma^2 +\sum_{i=1}^n \phi_{ik}}
  \end{aligned}
\end{equation}\]</span></p>
<p>Then the full CAVI algorithm for the Gaussian mixture model is given below.</p>
<p><strong>Algorithm: CAVI for Gaussian mixture model</strong></p>
<ul>
<li><p><strong>while</strong> the ELBO has not converged <strong>do</strong></p>
<ul>
<li><p><strong>for</strong> <span class="math inline">\(i\in\{1,\ldots,n\}\)</span> <strong>do</strong></p>
<p>Set <span class="math inline">\(\phi_{ik} \propto \exp\{\mathbb{E}[\mu_k;m_k,s_k^2]x_i - \mathbb{E}[\mu_k^2;m_k,s_k^2]/2\}\)</span></p>
<p><strong>end</strong></p></li>
<li><p><strong>for</strong> <span class="math inline">\(k\in\{1,\ldots,K\}\)</span> <strong>do</strong></p>
<p>Set <span class="math inline">\(m_k = \frac{\sum_{i}^n \phi_{ik}x_i}{1/\sigma^2 + \sum_{i=1}^n \phi_{ik}}\)</span></p>
<p>Set <span class="math inline">\(s_k^2 = \frac{1}{1/\sigma^2 +\sum_{i=1}^n \phi_{ik}}\)</span></p>
<p><strong>end</strong></p></li>
<li><p>Compute <span class="math inline">\(\mathrm{ELBO}(\boldsymbol{m},\boldsymbol{s}^2, \boldsymbol{\phi})\)</span></p></li>
</ul></li>
<li><p><strong>end</strong></p></li>
<li><p><strong>return</strong> <span class="math inline">\(q(\mu_k \mid m_k, s_k^2)\)</span> and <span class="math inline">\(q(c_k\mid \phi_i)\)</span></p></li>
</ul>
</div>
</div>
<div id="example-stochastic-variational-inference-using-pyro-in-python" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Example: Stochastic Variational Inference using Pyro in Python<a href="variational-inference.html#example-stochastic-variational-inference-using-pyro-in-python" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we have seen in the previous example, to use CAVI, we need to compute (and code up) the ELBO and the full conditionals in closed-form. This will be a pain when we have more complex models. When we perform statistical inference in practice, we typically only need to provide a model form. The model can be as simple as a formula such as <code>Response ~ var1 + var2</code> in the <code>glm</code> package, or in a more complicated form such as the Stan language when we perform MCMC sampling using the <code>rstan</code> package. We almost never needed to derive any math!</p>
<p>We can do the same with the VI. That is, we only need to provide the model formulation, and then let the software do the tedious derivation for us. However, there does not exist an automatic and versatile R package for VI. Therefore, we will look at <a href="https://pyro.ai/">Pyro</a>, a probabilistic programming language (PPL) written in Python, which is a very convenient interface for implementing VI for complex models. Note that Pyro is supported by the popular deep learning framework Pytorch on the backend, so models written in Pyro can be easily extended to incorporate neural network architectures. We will go over a simpler example provided by the Pyro team itself. All the following code is in Python and originally provided <a href="https://pyro.ai/examples/intro_long.html#Example:-Geography-and-national-income">here</a>.</p>
<p>To install pyro, simply run <code>pip install pyro-ppl</code> in your terminal. First, we import the required packages and set up some system parameters.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="variational-inference.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb40-2"><a href="variational-inference.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb40-3"><a href="variational-inference.html#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb40-4"><a href="variational-inference.html#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb40-5"><a href="variational-inference.html#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-6"><a href="variational-inference.html#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb40-7"><a href="variational-inference.html#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns <span class="co"># for plotting</span></span>
<span id="cb40-8"><a href="variational-inference.html#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># for plotting</span></span>
<span id="cb40-9"><a href="variational-inference.html#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyro</span>
<span id="cb40-10"><a href="variational-inference.html#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyro.distributions <span class="im">as</span> dist</span>
<span id="cb40-11"><a href="variational-inference.html#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyro.distributions.constraints <span class="im">as</span> constraints</span>
<span id="cb40-12"><a href="variational-inference.html#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="variational-inference.html#cb40-13" aria-hidden="true" tabindex="-1"></a>pyro.enable_validation(<span class="va">True</span>)</span>
<span id="cb40-14"><a href="variational-inference.html#cb40-14" aria-hidden="true" tabindex="-1"></a>pyro.set_rng_seed(<span class="dv">1</span>)</span>
<span id="cb40-15"><a href="variational-inference.html#cb40-15" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%(message)s</span><span class="st">&#39;</span>, level<span class="op">=</span>logging.INFO)</span>
<span id="cb40-16"><a href="variational-inference.html#cb40-16" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">&#39;default&#39;</span>)</span></code></pre></div>
<p>The dataset we will look at in this example studies the relationship between geography and national income. Specifically, we will examine how this relationship is different for nations in Africa and outside of Africa. It was found that, outside of Africa, bad geography is associated with lower GPD, whereas the relationship is reversed in Africa.</p>
<ul>
<li><p>Response variable <code>rgdppc_2000</code>: Real GPD per capita in 2000 (will be log transformed for analysis).</p></li>
<li><p>Predictor <code>rugged</code>: The Terrain Ruggedness Index which measures the topographic heterogeneity of a nation.</p></li>
<li><p>Predictor <code>cont_africa</code>: Whether the nation is in Africa.</p></li>
</ul>
<p>The following plots show the relationships between log GDP and ruggedness index for Non-African nations and African nations, respectively.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="variational-inference.html#cb41-1" aria-hidden="true" tabindex="-1"></a>DATA_URL <span class="op">=</span> <span class="st">&quot;https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv&quot;</span></span>
<span id="cb41-2"><a href="variational-inference.html#cb41-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(DATA_URL, encoding<span class="op">=</span><span class="st">&quot;ISO-8859-1&quot;</span>)</span>
<span id="cb41-3"><a href="variational-inference.html#cb41-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> data[[<span class="st">&quot;cont_africa&quot;</span>, <span class="st">&quot;rugged&quot;</span>, <span class="st">&quot;rgdppc_2000&quot;</span>]]</span>
<span id="cb41-4"><a href="variational-inference.html#cb41-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[np.isfinite(df.rgdppc_2000)]</span>
<span id="cb41-5"><a href="variational-inference.html#cb41-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;rgdppc_2000&quot;</span>] <span class="op">=</span> np.log(df[<span class="st">&quot;rgdppc_2000&quot;</span>]) <span class="co"># log transform the highly-skewed GDP data</span></span>
<span id="cb41-6"><a href="variational-inference.html#cb41-6" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> torch.tensor(df.values, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb41-7"><a href="variational-inference.html#cb41-7" aria-hidden="true" tabindex="-1"></a>is_cont_africa, ruggedness, log_gdp <span class="op">=</span> train[:, <span class="dv">0</span>], train[:, <span class="dv">1</span>], train[:, <span class="dv">2</span>]</span>
<span id="cb41-8"><a href="variational-inference.html#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="variational-inference.html#cb41-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-10"><a href="variational-inference.html#cb41-10" aria-hidden="true" tabindex="-1"></a>african_nations <span class="op">=</span> df[df[<span class="st">&quot;cont_africa&quot;</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb41-11"><a href="variational-inference.html#cb41-11" aria-hidden="true" tabindex="-1"></a>non_african_nations <span class="op">=</span> df[df[<span class="st">&quot;cont_africa&quot;</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb41-12"><a href="variational-inference.html#cb41-12" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>non_african_nations[<span class="st">&quot;rugged&quot;</span>],</span>
<span id="cb41-13"><a href="variational-inference.html#cb41-13" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span>non_african_nations[<span class="st">&quot;rgdppc_2000&quot;</span>],</span>
<span id="cb41-14"><a href="variational-inference.html#cb41-14" aria-hidden="true" tabindex="-1"></a>                ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb41-15"><a href="variational-inference.html#cb41-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">&quot;Terrain Ruggedness Index&quot;</span>,</span>
<span id="cb41-16"><a href="variational-inference.html#cb41-16" aria-hidden="true" tabindex="-1"></a>          ylabel<span class="op">=</span><span class="st">&quot;log GDP (2000)&quot;</span>,</span>
<span id="cb41-17"><a href="variational-inference.html#cb41-17" aria-hidden="true" tabindex="-1"></a>          title<span class="op">=</span><span class="st">&quot;Non African Nations&quot;</span>)</span>
<span id="cb41-18"><a href="variational-inference.html#cb41-18" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>african_nations[<span class="st">&quot;rugged&quot;</span>],</span>
<span id="cb41-19"><a href="variational-inference.html#cb41-19" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span>african_nations[<span class="st">&quot;rgdppc_2000&quot;</span>],</span>
<span id="cb41-20"><a href="variational-inference.html#cb41-20" aria-hidden="true" tabindex="-1"></a>                ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb41-21"><a href="variational-inference.html#cb41-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">&quot;Terrain Ruggedness Index&quot;</span>,</span>
<span id="cb41-22"><a href="variational-inference.html#cb41-22" aria-hidden="true" tabindex="-1"></a>          ylabel<span class="op">=</span><span class="st">&quot;log GDP (2000)&quot;</span>,</span>
<span id="cb41-23"><a href="variational-inference.html#cb41-23" aria-hidden="true" tabindex="-1"></a>          title<span class="op">=</span><span class="st">&quot;African Nations&quot;</span>)</span>
<span id="cb41-24"><a href="variational-inference.html#cb41-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-16-1.png" width="1152" /></p>
<p>A simple model to capture the relationship is
<span class="math display">\[Y = \alpha + \beta_aX_a + \beta_rX_r + \beta_{ar} X_aX_r +\epsilon, \ \ \epsilon\sim\mathcal{N}(0,\sigma^2)\]</span>
where <span class="math inline">\(Y\)</span> is the log GDP, <span class="math inline">\(X_a\)</span> is an indicator for whether the nation is in Africa, <span class="math inline">\(X_r\)</span> is the ruggedness index, and <span class="math inline">\(\epsilon\)</span> is the noise term.</p>
<p>This model is defined as <code>freq_model</code> in the following, where the parameters and the observations are specified using <code>pyro.param</code> and <code>pyro.sample</code>, respectively. A cool feature of Pyro is that its <code>render_model()</code> function allows us to visualize the model.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="variational-inference.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> freq_model(is_cont_africa, ruggedness, log_gdp<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb42-2"><a href="variational-inference.html#cb42-2" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> pyro.param(<span class="st">&quot;a&quot;</span>, <span class="kw">lambda</span>: torch.randn(()))</span>
<span id="cb42-3"><a href="variational-inference.html#cb42-3" aria-hidden="true" tabindex="-1"></a>    b_a <span class="op">=</span> pyro.param(<span class="st">&quot;bA&quot;</span>, <span class="kw">lambda</span>: torch.randn(()))</span>
<span id="cb42-4"><a href="variational-inference.html#cb42-4" aria-hidden="true" tabindex="-1"></a>    b_r <span class="op">=</span> pyro.param(<span class="st">&quot;bR&quot;</span>, <span class="kw">lambda</span>: torch.randn(()))</span>
<span id="cb42-5"><a href="variational-inference.html#cb42-5" aria-hidden="true" tabindex="-1"></a>    b_ar <span class="op">=</span> pyro.param(<span class="st">&quot;bAR&quot;</span>, <span class="kw">lambda</span>: torch.randn(()))</span>
<span id="cb42-6"><a href="variational-inference.html#cb42-6" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pyro.param(<span class="st">&quot;sigma&quot;</span>, <span class="kw">lambda</span>: torch.ones(()), constraint<span class="op">=</span>constraints.positive)</span>
<span id="cb42-7"><a href="variational-inference.html#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="variational-inference.html#cb42-8" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> a <span class="op">+</span> b_a <span class="op">*</span> is_cont_africa <span class="op">+</span> b_r <span class="op">*</span> ruggedness <span class="op">+</span> b_ar <span class="op">*</span> is_cont_africa <span class="op">*</span> ruggedness</span>
<span id="cb42-9"><a href="variational-inference.html#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="variational-inference.html#cb42-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> pyro.plate(<span class="st">&quot;data&quot;</span>, <span class="bu">len</span>(ruggedness)):</span>
<span id="cb42-11"><a href="variational-inference.html#cb42-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pyro.sample(<span class="st">&quot;obs&quot;</span>, dist.Normal(mean, sigma), obs<span class="op">=</span>log_gdp)</span>
<span id="cb42-12"><a href="variational-inference.html#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="variational-inference.html#cb42-13" aria-hidden="true" tabindex="-1"></a>pyro.render_model(freq_model, model_args<span class="op">=</span>(is_cont_africa, ruggedness, log_gdp), </span>
<span id="cb42-14"><a href="variational-inference.html#cb42-14" aria-hidden="true" tabindex="-1"></a>                  render_distributions<span class="op">=</span><span class="va">True</span>, render_params<span class="op">=</span><span class="va">True</span>, filename<span class="op">=</span><span class="st">&quot;freq_model.png&quot;</span>)<span class="op">;</span></span></code></pre></div>
<p><img src="freq_model.png" style="width:70.0%" />
We can also define a Bayesian version of it called <code>bayes_model</code> as follows. This model simply replaces <code>pyro.param</code> with <code>pyro.sample</code>, so that the parameters are viewed as random variables following some (prior) distributions. Similarly, we call <code>render_model()</code> to visualize it. Between the Bayesian model and the frequentist model, we will use the former for demonstration here.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="variational-inference.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bayes_model(is_cont_africa, ruggedness, log_gdp<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb43-2"><a href="variational-inference.html#cb43-2" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> pyro.sample(<span class="st">&quot;a&quot;</span>, dist.Normal(<span class="fl">0.</span>, <span class="fl">10.</span>))</span>
<span id="cb43-3"><a href="variational-inference.html#cb43-3" aria-hidden="true" tabindex="-1"></a>    b_a <span class="op">=</span> pyro.sample(<span class="st">&quot;bA&quot;</span>, dist.Normal(<span class="fl">0.</span>, <span class="fl">1.</span>))</span>
<span id="cb43-4"><a href="variational-inference.html#cb43-4" aria-hidden="true" tabindex="-1"></a>    b_r <span class="op">=</span> pyro.sample(<span class="st">&quot;bR&quot;</span>, dist.Normal(<span class="fl">0.</span>, <span class="fl">1.</span>))</span>
<span id="cb43-5"><a href="variational-inference.html#cb43-5" aria-hidden="true" tabindex="-1"></a>    b_ar <span class="op">=</span> pyro.sample(<span class="st">&quot;bAR&quot;</span>, dist.Normal(<span class="fl">0.</span>, <span class="fl">1.</span>))</span>
<span id="cb43-6"><a href="variational-inference.html#cb43-6" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pyro.sample(<span class="st">&quot;sigma&quot;</span>, dist.Uniform(<span class="fl">0.</span>, <span class="fl">10.</span>))</span>
<span id="cb43-7"><a href="variational-inference.html#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="variational-inference.html#cb43-8" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> a <span class="op">+</span> b_a <span class="op">*</span> is_cont_africa <span class="op">+</span> b_r <span class="op">*</span> ruggedness <span class="op">+</span> b_ar <span class="op">*</span> is_cont_africa <span class="op">*</span> ruggedness</span>
<span id="cb43-9"><a href="variational-inference.html#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="variational-inference.html#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> pyro.plate(<span class="st">&quot;data&quot;</span>, <span class="bu">len</span>(ruggedness)):</span>
<span id="cb43-11"><a href="variational-inference.html#cb43-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pyro.sample(<span class="st">&quot;obs&quot;</span>, dist.Normal(mean, sigma), obs<span class="op">=</span>log_gdp)</span>
<span id="cb43-12"><a href="variational-inference.html#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="variational-inference.html#cb43-13" aria-hidden="true" tabindex="-1"></a>pyro.render_model(bayes_model, model_args<span class="op">=</span>(is_cont_africa, ruggedness, log_gdp), render_distributions<span class="op">=</span><span class="va">True</span>, filename<span class="op">=</span><span class="st">&quot;bayes_model.png&quot;</span>)<span class="op">;</span></span></code></pre></div>
<p><img src="bayes_model.png" style="width:70.0%" /></p>
<p>In the context of Pyro, the variational distribution is called a “guide”. To specify the variational family we want, we need to define a guide program as follows. The code is very similar to that of the model. The custom guide we define below uses mean-field variational inference, i.e., all parameters have independent variational distributions in the form of Gaussian.</p>
<ul>
<li><p><span class="math inline">\(q(\alpha\mid \mu_\alpha, \sigma^2_\alpha) = \mathcal{N}(\mu_\alpha, \sigma^2_\alpha)\)</span></p></li>
<li><p><span class="math inline">\(q(\beta_a\mid \mu_{\beta_a}, \sigma^2_{\beta_a}) = \mathcal{N}(\mu_{\beta_a}, \sigma^2_{\beta_a}a)\)</span></p></li>
<li><p><span class="math inline">\(q(\beta_r\mid \mu_{\beta_r}, \sigma^2_{\beta_r}) = \mathcal{N}(\mu_{\beta_r}, \sigma^2_{\beta_r})\)</span></p></li>
<li><p><span class="math inline">\(q(\beta_{ar}\mid \mu_{\beta_{ar}}, \sigma^2_{\beta_{ar}}) = \mathcal{N}(\mu_{\beta_{ar}}, \sigma^2_{\beta_{ar}})\)</span></p></li>
<li><p><span class="math inline">\(q(\sigma^2\mid \mu_\sigma, \sigma^2_\sigma) = \mathcal{N}(\mu_\sigma, \sigma^2_\sigma)\)</span></p></li>
</ul>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="variational-inference.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_guide(is_cont_africa, ruggedness, log_gdp<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb44-2"><a href="variational-inference.html#cb44-2" aria-hidden="true" tabindex="-1"></a>    a_loc <span class="op">=</span> pyro.param(<span class="st">&#39;a_loc&#39;</span>, <span class="kw">lambda</span>: torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb44-3"><a href="variational-inference.html#cb44-3" aria-hidden="true" tabindex="-1"></a>    a_scale <span class="op">=</span> pyro.param(<span class="st">&#39;a_scale&#39;</span>, <span class="kw">lambda</span>: torch.tensor(<span class="fl">1.</span>),</span>
<span id="cb44-4"><a href="variational-inference.html#cb44-4" aria-hidden="true" tabindex="-1"></a>                         constraint<span class="op">=</span>constraints.positive)</span>
<span id="cb44-5"><a href="variational-inference.html#cb44-5" aria-hidden="true" tabindex="-1"></a>    sigma_loc <span class="op">=</span> pyro.param(<span class="st">&#39;sigma_loc&#39;</span>, <span class="kw">lambda</span>: torch.tensor(<span class="fl">1.</span>),</span>
<span id="cb44-6"><a href="variational-inference.html#cb44-6" aria-hidden="true" tabindex="-1"></a>                             constraint<span class="op">=</span>constraints.positive)</span>
<span id="cb44-7"><a href="variational-inference.html#cb44-7" aria-hidden="true" tabindex="-1"></a>    weights_loc <span class="op">=</span> pyro.param(<span class="st">&#39;weights_loc&#39;</span>, <span class="kw">lambda</span>: torch.randn(<span class="dv">3</span>))</span>
<span id="cb44-8"><a href="variational-inference.html#cb44-8" aria-hidden="true" tabindex="-1"></a>    weights_scale <span class="op">=</span> pyro.param(<span class="st">&#39;weights_scale&#39;</span>, <span class="kw">lambda</span>: torch.ones(<span class="dv">3</span>),</span>
<span id="cb44-9"><a href="variational-inference.html#cb44-9" aria-hidden="true" tabindex="-1"></a>                               constraint<span class="op">=</span>constraints.positive)</span>
<span id="cb44-10"><a href="variational-inference.html#cb44-10" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> pyro.sample(<span class="st">&quot;a&quot;</span>, dist.Normal(a_loc, a_scale))</span>
<span id="cb44-11"><a href="variational-inference.html#cb44-11" aria-hidden="true" tabindex="-1"></a>    b_a <span class="op">=</span> pyro.sample(<span class="st">&quot;bA&quot;</span>, dist.Normal(weights_loc[<span class="dv">0</span>], weights_scale[<span class="dv">0</span>]))</span>
<span id="cb44-12"><a href="variational-inference.html#cb44-12" aria-hidden="true" tabindex="-1"></a>    b_r <span class="op">=</span> pyro.sample(<span class="st">&quot;bR&quot;</span>, dist.Normal(weights_loc[<span class="dv">1</span>], weights_scale[<span class="dv">1</span>]))</span>
<span id="cb44-13"><a href="variational-inference.html#cb44-13" aria-hidden="true" tabindex="-1"></a>    b_ar <span class="op">=</span> pyro.sample(<span class="st">&quot;bAR&quot;</span>, dist.Normal(weights_loc[<span class="dv">2</span>], weights_scale[<span class="dv">2</span>]))</span>
<span id="cb44-14"><a href="variational-inference.html#cb44-14" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pyro.sample(<span class="st">&quot;sigma&quot;</span>, dist.Normal(sigma_loc, torch.tensor(<span class="fl">0.05</span>)))</span>
<span id="cb44-15"><a href="variational-inference.html#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">&quot;a&quot;</span>: a, <span class="st">&quot;b_a&quot;</span>: b_a, <span class="st">&quot;b_r&quot;</span>: b_r, <span class="st">&quot;b_ar&quot;</span>: b_ar, <span class="st">&quot;sigma&quot;</span>: sigma}</span>
<span id="cb44-16"><a href="variational-inference.html#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="variational-inference.html#cb44-17" aria-hidden="true" tabindex="-1"></a>pyro.render_model(custom_guide, model_args<span class="op">=</span>(is_cont_africa, ruggedness, log_gdp), render_params<span class="op">=</span><span class="va">True</span>, filename<span class="op">=</span><span class="st">&quot;custom_guide.png&quot;</span>)<span class="op">;</span></span></code></pre></div>
<p><img src="custom_guide.png" style="width:70.0%" /></p>
<p>To implement variational inference in Pyro, we use its stochastic variational inference functionality <code>pyro.infer.SVI()</code>, which takes in four arguments:</p>
<ul>
<li><p>Model: <code>bayes_model</code></p></li>
<li><p>Guide (Variational distribution): <code>custom_guide</code></p></li>
<li><p>ELBO: We do not need to compute the explicit ELBO. It can be defined by calling <code>pyro.infer.Trace_ELBO()</code>, and this will automatically compute ELBO under the hood for us given model and guide.</p></li>
<li><p>Optimizer: Any optimizer can do, but a popular choice is the Adam optimizer given by <code>pyro.optim.Adam()</code>. Adam is a gradient-based optimization algorithm that computes adaptive learning rates for different parameters.</p></li>
</ul>
<p>The plot below shows the ELBO loss (negative ELBO) as a function of the step index. We see that the optimization procedure has converged after about 500 steps.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="variational-inference.html#cb45-1" aria-hidden="true" tabindex="-1"></a>pyro.clear_param_store()</span>
<span id="cb45-2"><a href="variational-inference.html#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="variational-inference.html#cb45-3" aria-hidden="true" tabindex="-1"></a>auto_guide <span class="op">=</span> pyro.infer.autoguide.AutoNormal(bayes_model)</span>
<span id="cb45-4"><a href="variational-inference.html#cb45-4" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> pyro.optim.Adam({<span class="st">&quot;lr&quot;</span>: <span class="fl">0.02</span>}) </span>
<span id="cb45-5"><a href="variational-inference.html#cb45-5" aria-hidden="true" tabindex="-1"></a>elbo <span class="op">=</span> pyro.infer.Trace_ELBO()</span>
<span id="cb45-6"><a href="variational-inference.html#cb45-6" aria-hidden="true" tabindex="-1"></a>svi <span class="op">=</span> pyro.infer.SVI(bayes_model, auto_guide, adam, elbo)</span>
<span id="cb45-7"><a href="variational-inference.html#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="variational-inference.html#cb45-8" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb45-9"><a href="variational-inference.html#cb45-9" aria-hidden="true" tabindex="-1"></a>vi_start <span class="op">=</span> time.time()</span>
<span id="cb45-10"><a href="variational-inference.html#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):  </span>
<span id="cb45-11"><a href="variational-inference.html#cb45-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> svi.step(is_cont_africa, ruggedness, log_gdp)</span>
<span id="cb45-12"><a href="variational-inference.html#cb45-12" aria-hidden="true" tabindex="-1"></a>    losses.append(loss)</span>
<span id="cb45-13"><a href="variational-inference.html#cb45-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb45-14"><a href="variational-inference.html#cb45-14" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="st">&quot;Elbo loss: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(loss))</span></code></pre></div>
<pre><code>## Elbo loss: 694.9404839277267
## Elbo loss: 524.3822101950645
## Elbo loss: 475.668176651001
## Elbo loss: 399.99088364839554
## Elbo loss: 315.23277366161346
## Elbo loss: 254.76771301031113
## Elbo loss: 248.237025141716
## Elbo loss: 248.42669039964676
## Elbo loss: 248.46450036764145
## Elbo loss: 257.41463327407837</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="variational-inference.html#cb47-1" aria-hidden="true" tabindex="-1"></a>vi_end <span class="op">=</span> time.time()</span>
<span id="cb47-2"><a href="variational-inference.html#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="variational-inference.html#cb47-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb47-4"><a href="variational-inference.html#cb47-4" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span>
<span id="cb47-5"><a href="variational-inference.html#cb47-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;SVI step&quot;</span>)</span>
<span id="cb47-6"><a href="variational-inference.html#cb47-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;ELBO loss&quot;</span>)</span>
<span id="cb47-7"><a href="variational-inference.html#cb47-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-20-3.png" width="480" /></p>
<p>Below is the time taken to optimize the ELBO using the VI method.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="variational-inference.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vi_end<span class="op">-</span>vi_start)</span></code></pre></div>
<pre><code>## 5.5414955615997314</code></pre>
<p>Then we can print out the estimated variational parameters (<span class="math inline">\(\boldsymbol{\omega}\)</span>), which are the mean and variance of each variational Normal distribution.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="variational-inference.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, value <span class="kw">in</span> pyro.get_param_store().items():</span>
<span id="cb50-2"><a href="variational-inference.html#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, pyro.param(name).data.cpu().numpy())</span></code></pre></div>
<pre><code>## AutoNormal.locs.a 9.173145
## AutoNormal.scales.a 0.07036691
## AutoNormal.locs.bA -1.847466
## AutoNormal.scales.bA 0.14070092
## AutoNormal.locs.bR -0.1903212
## AutoNormal.scales.bR 0.044044245
## AutoNormal.locs.bAR 0.35599768
## AutoNormal.scales.bAR 0.07937442
## AutoNormal.locs.sigma -2.205863
## AutoNormal.scales.sigma 0.06052672</code></pre>
<p>Given the variational parameters, we can now sample from the variational distributions of the parameters of interest. The variational distributions can be treated as approximated posterior distributions of these parameters. In particular, we can look at the slopes (log GDP versus ruggedness index) for Non-African nations and African nations.</p>
<ul>
<li><p>Non-African nations: slope = <span class="math inline">\(\beta_r\)</span></p></li>
<li><p>African nations: slope = <span class="math inline">\(\beta_r+\beta_{ar}\)</span></p></li>
</ul>
<p>We will take 800 samples from <span class="math inline">\(q(\beta_{r}\mid \hat{\mu}_{\beta_{r}}, \hat{\sigma}^2_{\beta_{r}})\)</span> and <span class="math inline">\(q(\beta_{ar}\mid \hat{\mu}_{\beta_{ar}}, \hat{\sigma}^2_{\beta_{ar}})\)</span>. The histograms of the samples are plotted side-by-side below.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="variational-inference.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pyro.plate(<span class="st">&quot;samples&quot;</span>, <span class="dv">800</span>, dim<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb52-2"><a href="variational-inference.html#cb52-2" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> auto_guide(is_cont_africa, ruggedness)</span>
<span id="cb52-3"><a href="variational-inference.html#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="variational-inference.html#cb52-4" aria-hidden="true" tabindex="-1"></a>gamma_within_africa <span class="op">=</span> samples[<span class="st">&quot;bR&quot;</span>] <span class="op">+</span> samples[<span class="st">&quot;bAR&quot;</span>]</span>
<span id="cb52-5"><a href="variational-inference.html#cb52-5" aria-hidden="true" tabindex="-1"></a>gamma_outside_africa <span class="op">=</span> samples[<span class="st">&quot;bR&quot;</span>]</span>
<span id="cb52-6"><a href="variational-inference.html#cb52-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb52-7"><a href="variational-inference.html#cb52-7" aria-hidden="true" tabindex="-1"></a>plt.hist(gamma_within_africa.detach().numpy(), label<span class="op">=</span><span class="st">&quot;Arican nations&quot;</span>)<span class="op">;</span></span>
<span id="cb52-8"><a href="variational-inference.html#cb52-8" aria-hidden="true" tabindex="-1"></a>plt.hist(gamma_outside_africa.detach().numpy(), label<span class="op">=</span><span class="st">&quot;Non-Arican nations&quot;</span>)<span class="op">;</span></span>
<span id="cb52-9"><a href="variational-inference.html#cb52-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">&quot;Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;</span>)<span class="op">;</span></span>
<span id="cb52-10"><a href="variational-inference.html#cb52-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Slope of regression line&quot;</span>)</span>
<span id="cb52-11"><a href="variational-inference.html#cb52-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb52-12"><a href="variational-inference.html#cb52-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-23-5.png" width="960" /></p>
<p>Finally, we can compare the VI to MCMC (NUTS, to be specific).</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="variational-inference.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyro.infer <span class="im">import</span> MCMC, NUTS</span>
<span id="cb53-2"><a href="variational-inference.html#cb53-2" aria-hidden="true" tabindex="-1"></a>mcmc <span class="op">=</span> MCMC(NUTS(bayes_model), num_samples<span class="op">=</span><span class="dv">1000</span>, warmup_steps<span class="op">=</span><span class="dv">1000</span>, disable_progbar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-3"><a href="variational-inference.html#cb53-3" aria-hidden="true" tabindex="-1"></a>mc_start <span class="op">=</span> time.time()</span>
<span id="cb53-4"><a href="variational-inference.html#cb53-4" aria-hidden="true" tabindex="-1"></a>mcmc.run(is_cont_africa, ruggedness, log_gdp)</span>
<span id="cb53-5"><a href="variational-inference.html#cb53-5" aria-hidden="true" tabindex="-1"></a>mc_end <span class="op">=</span> time.time()</span>
<span id="cb53-6"><a href="variational-inference.html#cb53-6" aria-hidden="true" tabindex="-1"></a>mc_samples <span class="op">=</span> mcmc.get_samples()</span>
<span id="cb53-7"><a href="variational-inference.html#cb53-7" aria-hidden="true" tabindex="-1"></a>mcmc.summary()</span></code></pre></div>
<pre><code>## 
##                 mean       std    median      5.0%     95.0%     n_eff     r_hat
##          a      9.19      0.14      9.19      8.95      9.42    537.96      1.00
##         bA     -1.86      0.23     -1.86     -2.22     -1.48    497.48      1.00
##        bAR      0.36      0.13      0.36      0.12      0.55    563.94      1.00
##         bR     -0.19      0.08     -0.19     -0.31     -0.05    507.07      1.00
##      sigma      0.95      0.05      0.95      0.86      1.03    648.89      1.00
## 
## Number of divergences: 0</code></pre>
<p>Below is the time taken to draw 2000 MCMC samples (including 1000 warmups). We can see MCMC is almost an order of magnitude slower than the VI.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="variational-inference.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mc_end<span class="op">-</span>mc_start)</span></code></pre></div>
<pre><code>## 38.696908712387085</code></pre>
<p>We can also plot the histograms of the MCMC samples of the two slopes. Compared to the histograms obtained using the VI, these don’t look much different.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="variational-inference.html#cb57-1" aria-hidden="true" tabindex="-1"></a>mc_slope_within_africa <span class="op">=</span> mc_samples[<span class="st">&quot;bR&quot;</span>] <span class="op">+</span> mc_samples[<span class="st">&quot;bAR&quot;</span>]</span>
<span id="cb57-2"><a href="variational-inference.html#cb57-2" aria-hidden="true" tabindex="-1"></a>mc_slope_outside_africa <span class="op">=</span> mc_samples[<span class="st">&quot;bR&quot;</span>]</span>
<span id="cb57-3"><a href="variational-inference.html#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="variational-inference.html#cb57-4" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb57-5"><a href="variational-inference.html#cb57-5" aria-hidden="true" tabindex="-1"></a>plt.hist(mc_slope_within_africa.detach().numpy(), label<span class="op">=</span><span class="st">&quot;Arican nations&quot;</span>)<span class="op">;</span></span>
<span id="cb57-6"><a href="variational-inference.html#cb57-6" aria-hidden="true" tabindex="-1"></a>plt.hist(mc_slope_outside_africa.detach().numpy(), label<span class="op">=</span><span class="st">&quot;Non-Arican nations&quot;</span>)<span class="op">;</span></span>
<span id="cb57-7"><a href="variational-inference.html#cb57-7" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">&quot;MCMC Estimated Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;</span>)<span class="op">;</span></span>
<span id="cb57-8"><a href="variational-inference.html#cb57-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Slope of regression line&quot;</span>)</span>
<span id="cb57-9"><a href="variational-inference.html#cb57-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb57-10"><a href="variational-inference.html#cb57-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-26-7.png" width="960" /></p>
</div>
<div id="takeaways" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Takeaways<a href="variational-inference.html#takeaways" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>In a Bayesian example, the VI is an order of magnitude faster than MCMC while giving similar results. However, the example uses a very simple Normal-Normal model, and the variational distribution employed is also Normal. This could be why the VI performs so well. When the true posterior is multi-model or asymmetric, it is unlikely that using a mean-field Gaussian variational distribution will give as accurate results.</p></li>
<li><p>Since the VI is an optimization-based algorithm, it is possible that the ELBO has not converged to a local minimum (in more complex modelling settings). Diagnostics are required for this.</p></li>
<li><p>The VI is known to underestimate model uncertainty. It is not surprising since it is an approximate inference method. Unless your model is so complicated (e.g., multiple hierarchies, neural nets) that traditional frequentist fitting procedures (<code>lme4</code>, <code>glm</code>) break down or MCMC methods take forever, the VI would not be your first choice.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bernstein-von-mises-theorem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smoothing-techniques.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/uwswagclub/workshop-bookdown/edit/master/04-variational-inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/uwswagclub/workshop-bookdown/blob/master/04-variational-inference.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
