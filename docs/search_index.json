[["index.html", "SWAG Workshops Repository Chapter 1 Introduction", " SWAG Workshops Repository UW Statistical Workshops and Applications Group Chapter 1 Introduction Welcome to the Github repository for the Statistical Workshops and Applications Group (SWAG)! Each workshop will be given its own chapter in this document, where the content for the workshop will be shared. These workshops will be more interactive than presentations or lectures  and each workshop will typically be accompanied by relevant R code. We plan to host one workshop per month on a Thursday 4-5pm at M3 4206. The time and date will be emailed to all statistics and actuarial science grad students in advance. If you are not a SAS grad student and would like to attend a workshop, feel free to contact one of the SWAG organizers (Augustine, Luke, Meixi, and Yiran). You can find our contact info here. Below is the list of past workshops: Sampling-resampling methods (Oct 6, 2022) The Bernstein-von Mises theorem (Oct 19, 2022) Variational inference (Nov 10, 2022) Smoothing techniques (Dec 1, 2022) Analyzing surveys with R (Jan 26, 2023) Introduction to causal inference (Feb 16, 2023) Pooling p-values (Mar 16, 2023) Active learning strategies with p-values (Apr 6, 2023) "],["chapter2.html", "Chapter 2 Sampling-Resampling Methods 2.1 Introduction 2.2 Rejection Sampling 2.3 Sampling-Resampling Methods 2.4 Implementation of Sampling-Resampling Methods 2.5 Sampling-Resampling in Multiple Dimensions", " Chapter 2 Sampling-Resampling Methods Luke Hagar 2.1 Introduction Welcome to the first workshop for the Statistical Workshops and Applications Group (SWAG)! This workshop discusses sampling-resampling methods, mainly in the context of Bayesian inference. 2.2 Rejection Sampling Before introducing sampling-resampling methods, we briefly discuss rejection sampling (Ripley 2009) along with its strengths and shortcomings. In what follows, we discuss the set up for our sampling scenario. Lets suppose we can easily generate a sample from a continuous density function \\(g(\\boldsymbol{\\theta})\\). We call this the proposal distribution. However, we want a sample from a density \\(h(\\boldsymbol{\\theta})\\) such that \\(g(\\boldsymbol{\\theta}) = 0\\) implies \\(h(\\boldsymbol{\\theta}) = 0\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). We have a positive function \\(f(\\boldsymbol{\\theta})\\) that is proportional to \\(h(\\boldsymbol{\\theta})\\). That is, \\(h(\\boldsymbol{\\theta}) = f(\\boldsymbol{\\theta})/\\int f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\). For rejection sampling, we further suppose that there exists an identifiable constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). Lets reinforce this terminology with an example. In this example, we want to sample from \\(h(\\theta) = 12\\theta^2(1-\\theta)\\) for \\(0 &lt; \\theta &lt; 1\\). This is actually a \\(\\text{BETA}(3,2)\\) distribution. For this example, we let \\(f(\\theta) = h(\\theta)\\). We have that \\(f(\\theta) \\le 16/9\\) for all \\(0 &lt; \\theta &lt; 1\\). We can easily sample over the unit interval using the uniform density \\(U(0,1)\\). This corresponds to choosing \\(g(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\). It follows that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le 16/9 = M\\) for all \\(0 &lt; \\theta &lt; 1\\). This scenario is visualized in Figure 2.1. ## compute f and g across the range of theta for plotting purposes theta &lt;- seq(0, 1, by = 0.005) f &lt;- 12 * theta^2 * (1 - theta) g &lt;- rep(1, length(theta)) M &lt;- 16/9 ## generate the plot and corresponding legend plot(theta, f, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;Density&quot;, ylim = c(0, 2.25), xlab = expression(theta)) lines(theta, M * g, lty = 2) legend(&quot;topright&quot;, c(&quot;f&quot;, &quot;M*g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) Figure 2.1: Visualization of rejection sampling functions We now introduce rejection sampling. This process is detailed in Algorithm 1. A sample of \\(\\boldsymbol{\\theta}\\) values accepted via Algorithm 1 is a sample from the density of interest \\(h(\\boldsymbol{\\theta})\\) (Ripley 2009). Algorithm 1 Generate \\(\\boldsymbol{\\theta}\\) from \\(g(\\boldsymbol{\\theta})\\). Generate \\(u \\sim U(0,1)\\). If \\(u \\le f(\\boldsymbol{\\theta})/(Mg(\\boldsymbol{\\theta}))\\), return \\(\\boldsymbol{\\theta}\\); otherwise return to Step 1. We now illustrate how to implement rejection sampling to generate 10000 observations from \\(h(\\boldsymbol{\\theta})\\) for this simple example. The results are visualized in Figure 2.2. ## initialize matrices for accepted and rejected theta values accept_mat &lt;- NULL reject_mat &lt;- NULL ## set seed for reproducibility set.seed(1) stop &lt;- FALSE ## continue until 10000 points are accepted while (stop == FALSE) { ## generate a value from g: U(0,1) theta_temp &lt;- runif(1) ## generate u to decide whether to accept/reject u &lt;- runif(1) ## compute the values for f and g at this theta value f_temp &lt;- 12 * theta_temp^2 * (1 - theta_temp) g_temp &lt;- 1 ## decide whether or not to accept point; we save the rejected ## points and &#39;u&#39; realizations for the plot if (u &lt;= f_temp/(M * g_temp)) { accept_mat &lt;- rbind(accept_mat, c(theta_temp, u)) if (nrow(accept_mat) &gt;= 10000) { stop &lt;- TRUE } } else { reject_mat &lt;- rbind(reject_mat, c(theta_temp, u)) } } ## generate the plot and corresponding legend green points represent ## accepted (theta, u) combinations red points represent rejected ## (theta, u) combinations plot(theta, f, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;M*u&quot;, ylim = c(0, 2.25), xlab = expression(theta), lwd = 1.5) lines(theta, M * g, lty = 2, lwd = 1.5) legend(&quot;topright&quot;, c(&quot;f&quot;, &quot;M*g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) points(accept_mat[, 1], M * accept_mat[, 2], col = adjustcolor(&quot;seagreen&quot;, 0.05), pch = 19) points(reject_mat[, 1], M * reject_mat[, 2], col = adjustcolor(&quot;firebrick&quot;, 0.05), pch = 19) Figure 2.2: Visualization of rejection sampling with accepted points (green) and rejected points (red) The points in Figure 2.2 depict which \\(\\theta\\) values were accepted (green) and rejected (red) by rejection sampling. The resulting sample appears to come from the \\(\\text{BETA}(3,2)\\) distribution. We make two final remarks about rejection sampling that bear relevance to sampling-resampling methods. To implement rejection sampling, we must identify a constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). This choice for \\(M\\) is important. For this example, we could have chosen \\(M &gt; 16/9\\), but the sampling procedure would have been less efficient. Choosing \\(M &lt; 16/9\\) would have returned a sample that is not from the \\(\\text{BETA}(3,2)\\) distribution. We need to carefully consider the support of \\(h(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\). Choosing \\(g(\\boldsymbol{\\theta})\\) to be \\(U(0.25, 0.75)\\) would not have been appropriate for this example. Choosing \\(g(\\boldsymbol{\\theta})\\) to be \\(U(-1, 1)\\) would be fine but less efficient. When using sampling-resampling methods, remark 1 is often not of concern. 2.3 Sampling-Resampling Methods 2.3.1 Overview of the Sampling-Importance-Resampling Algorithm We now discuss how to obtain an approximate sample from the density \\(h(\\boldsymbol{\\theta})\\) using a particular weighted sampling-resampling method, which is a variant of the bootstrap procedure (Efron 1982). This method was first formally proposed in Rubin (1987) and Rubin (1988) as the sampling-importance-resampling (SIR) algorithm. We introduce this procedure via Algorithm 2, which uses notation from Smith and Gelfand (1992). Algorithm 2 Generate \\(\\boldsymbol{\\theta}_i, ~ i = 1,...,n\\) from \\(g(\\boldsymbol{\\theta})\\). For each \\(\\boldsymbol{\\theta}_i\\), compute \\(\\omega_i = f(\\boldsymbol{\\theta}_i)/g(\\boldsymbol{\\theta_i})\\). Let \\(q_i = \\omega_i/\\sum_{j=1}^n\\omega_j\\) for \\(i = 1,...,n\\). Draw \\(\\{\\boldsymbol{\\theta}^*_1,..., \\boldsymbol{\\theta}^*_m\\}\\) from the discrete distribution over \\(\\{\\boldsymbol{\\theta}_1, ..., \\boldsymbol{\\theta}_n \\}\\) with replacement, where mass \\(q_i\\) is placed on \\(\\boldsymbol{\\theta}_i\\). Under several conditions, \\(\\boldsymbol{\\theta}^*\\) is approximately distributed according to \\(h(\\boldsymbol{\\theta})\\). Algorithm 2 returns approximate samples from \\(h(\\boldsymbol{\\theta})\\) when \\(n\\) is sufficiently large and \\(supp(h) \\subseteq supp(g)\\), where \\(supp()\\) refers to the support of a probability distribution. How large \\(n\\) must be depends on the extent to which \\(h(\\boldsymbol{\\theta})\\) resembles \\(g(\\boldsymbol{\\theta})\\). Moreover, \\(m\\) is typically smaller than \\(n\\); Rubin (1987) stated that specifying \\(m\\) and \\(n\\) such that their ratio \\(n/m\\) is at least 20 should be sufficient in most scenarios. When implementing this sampling-resampling procedure, we must still be mindful of the supports of \\(h(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\). However, we do not need to identify a constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). We only require a function \\(g(\\boldsymbol{\\theta})\\) that we can readily sample from and a function \\(f(\\boldsymbol{\\theta})\\) that is proportional to our density of interest \\(h(\\boldsymbol{\\theta})\\). For Bayesian inference, we often want to sample from a posterior distribution of interest \\(\\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) for \\(\\boldsymbol{\\theta}\\) given observed data \\(\\boldsymbol{x}\\). Let \\(L()\\) be the likelihood function and \\(p()\\) be the prior for \\(\\boldsymbol{\\theta}\\). The posterior distribution communicates which values \\(\\boldsymbol{\\theta}\\) are plausible given the observed data and our prior beliefs. By Bayes Theorem, we have that \\[\\pi(\\boldsymbol{\\theta}| \\boldsymbol{x}) = \\frac{L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})}{\\int L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}} \\propto L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta}).\\] Thus, we have \\(h(\\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) and \\(f(\\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})\\) using the notation from earlier. For a given \\(\\boldsymbol{\\theta}\\) value, it is straightforward to compute \\(f(\\boldsymbol{\\theta})\\); however, we are often not able to compute \\(h(\\boldsymbol{\\theta})\\) directly. As such, the SIR algorithm goes well with the framework for Bayesian inference. It can be used to generate an approximate sample from the posterior distribution of interest \\(h(\\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) and serves as an alternative to more standard Markov chain Monte Carlo (MCMC) methods in many scenarios. The SIR algorithm can be used with non-Bayesian methods, but the remainder of this workshop focuses on implementing the SIR algorithm to conduct approximate posterior sampling. 2.3.2 Computational Considerations The SIR algorithm often produces more stable results when \\(\\omega_i, i = 1,...,n\\) are computed on the logarithmic scale. This is consistent with other methods for Bayesian inference that do not involve sampling-resampling. Particularly when \\(\\boldsymbol{\\theta}\\) is multivariate, \\(f(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\) can take small positive values that are extremely close to 0. Computing \\(\\text{log}(\\omega_i)\\) helps make underflow errors less common, as we will see shortly. We have that \\[\\text{log}(\\omega_i) = \\text{log}(f(\\boldsymbol{\\theta}_i)) - \\text{log}(g(\\boldsymbol{\\theta}_i)).\\] In the context of Bayesian inference, \\(f(\\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})\\) is generally the product of several terms, so \\(\\text{log}(f(\\boldsymbol{\\theta}_i))\\) is often easier to manipulate than \\(f(\\boldsymbol{\\theta}_i)\\). When \\(g(\\boldsymbol{\\theta})\\) is a well known probability distribution, we can often compute \\(\\text{log}(g(\\boldsymbol{\\theta}_i))\\) using built-in R functions. For instance, if \\(g(\\boldsymbol{\\theta})\\) is the standard normal distribution, we can compute \\(\\text{log}(g(\\boldsymbol{\\theta}_i))\\) as \\(\\texttt{dnorm(}\\boldsymbol{\\theta}_i\\texttt{, log = TRUE)}\\). We must then exponentiate the \\(\\text{log}(\\omega_i)\\) values to compute the normalized weights \\(q_i, ~ i = 1, ..., n\\). In certain situations, we proceed with caution to avoid more underflow errors. Lets consider the following example with only three \\(\\text{log}(\\omega_i)\\) values: \\(\\text{log}(\\omega_1) = 0\\), \\(\\text{log}(\\omega_2) = 1\\), and \\(\\text{log}(\\omega_3) = 2\\). For this example, we can exponentiate and standardize these weights without issues using the following R code. ## define log weights log_omega &lt;- c(0, 1, 2) ## exponentiate the weights omega &lt;- exp(log_omega) ## standardize the weights and print results by enclosing assignment ## command with () (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 Here, we standardize the weights after exponentiation. That being the case, we can apply a translation to the weights on the logarithmic scale without impacting the final standardized weights. For this example, the maximum \\(\\text{log}(\\omega_i)\\) is \\(\\text{log}(\\omega_3) = 2\\). Lets subtract 2 from each original \\(\\text{log}(\\omega_i)\\) value before exponentiation and output the results. ## define log weights log_omega &lt;- c(0, 1, 2) ## subtract 2 from each log_omega value and exponentiate the weights omega &lt;- exp(log_omega - max(log_omega)) ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 The final standardized weights are the same! This trick is useful when the \\(\\text{log}(\\omega_i)\\) values are very small (or very large). Lets now imagine that \\(\\text{log}(\\omega_1) = -800\\), \\(\\text{log}(\\omega_2) = -799\\), and \\(\\text{log}(\\omega_3) = -798\\). Lets see what occurs if we try to exponentiate these values. ## define log weights log_omega &lt;- c(-800, -799, -798) ## exponentiate the weights (omega &lt;- exp(log_omega)) ## [1] 0 0 0 ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] NaN NaN NaN This produces an underflow error. The values for \\(\\omega_i, ~ i = 1, 2, 3\\) are so small that they are rounded down to 0. But, we can apply a translation to the \\(\\text{log}(\\omega_i)\\) values by subtracting \\(\\text{max}_{i = 1, 2, 3}~\\text{log}(\\omega_i) = -798\\) from each value. This produces the same final standardized weights as for the earlier example  which makes sense because this example simply subtracted 800 from each of the earlier \\(\\text{log}(\\omega_i)\\) values. ## define log weights log_omega &lt;- c(-800, -799, -798) ## subtract max(log_omega) from each log_omega value and exponentiate ## the weights omega &lt;- exp(log_omega - max(log_omega)) ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 2.4 Implementation of Sampling-Resampling Methods 2.4.1 Illustrative Example with Binary Data Here, we demonstrate how to implement the SIR algorithm to faciliate posterior sampling for a simple example. In this example, we assume that we observe binary data \\(x_1, ..., x_N\\) such that \\(x_j \\sim \\text{BIN}(1, \\theta)\\). We assume that the prior for \\(\\theta\\) is an uninformative \\(\\text{BETA}(1,1)\\) prior, which implies that \\(p(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\). In this case, the posterior \\(h(\\theta) = \\pi(\\theta| \\boldsymbol{x})\\) is such that \\[h(\\boldsymbol{\\theta}) \\propto L(\\theta; \\boldsymbol{x}) = \\prod_{j=1}^N\\theta^{x_j}(1 - \\theta)^{1-x_j} = \\theta^{\\sum_{j=1}^Nx_j}(1-\\theta)^{N-\\sum_{j=1}^Nx_j} = f(\\theta).\\] Lets assume that we have \\(N = 20\\) observations and that 15 of these Bernoulli trials were successful (\\(\\sum_{j=1}^N x_j = 15\\)). It follows that \\(f(\\theta) = \\theta^{15}(1-\\theta)^5\\) for \\(0 &lt; \\theta &lt; 1\\). For this example, \\(h(\\theta)\\) is actually known: it is a \\(\\text{BETA}(16, 6)\\) distribution. Therefore, we do not need to use sampling-resampling methods to sample from \\(h(\\boldsymbol{\\theta})\\). However, because \\(h(\\theta)\\) is known, we can use this example to explore the performance of the SIR algorithm for different proposal distributions \\(g(\\boldsymbol{\\theta})\\). The first proposal distribution that we consider is \\(g(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\) (i.e., the \\(U(0,1)\\) distribution). We plot \\(h(\\theta)\\) and \\(g(\\theta)\\) in Figure 2.3. ## compute h and g across the range of theta for plotting purposes theta &lt;- seq(0, 1, by = 0.005) h &lt;- dbeta(theta, 16, 6) g &lt;- rep(1, length(theta)) ## generate the plot and corresponding legend plot(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;Density&quot;, ylim = c(0, 4.5), xlab = expression(theta)) lines(theta, g, lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) Figure 2.3: Visualization of posterior and uniform sampling function While \\(h(\\theta)\\) and \\(g(\\theta)\\) have the same support, it does not appear that \\(U(0,1)\\) will be a very efficient proposal distribution for this example. Values of \\(\\theta_i\\) close to 0 that are generated via \\(g(\\theta)\\) will be assigned standardized weights of \\(q_i \\approx 0\\). We now show how to implement the entire SIR algorithm for this example with the uniform sampling function. We use the settings \\(n\\) = 1 million and \\(m = 50000\\). The results are visualized in Figure 2.4. ## input the sample of 20 observations into R x &lt;- c(rep(1, 15), rep(0, 5)) ## extract the number of observations (N) and successes N &lt;- length(x) sum_x &lt;- sum(x) ## define SIR algorithm settings n &lt;- 1e+06 m &lt;- 50000 ## define a function that is proportional to the posterior (on the ## logarithmic scale) propLog &lt;- function(theta, obs, sum_data) { ## return -Inf is theta is not between 0 and 1 (this will ## exponentiate to give q_i = 0) return(ifelse(theta &gt; 0 &amp; theta &lt; 1, sum_data * log(theta) + (obs - sum_data) * log(1 - theta), -Inf)) } set.seed(2) ## sample from proposal distribution samp &lt;- runif(n) ## form importance sampling weights on log-scale; here, log(g(theta)) ## = log(1) = 0 for all 0 &lt; theta &lt; 1 w1 &lt;- propLog(samp, N, sum_x) - 0 w1 &lt;- exp(w1 - max(w1)) q1 &lt;- w1/sum(w1) ## resample to create approximate sample from posterior inds1 &lt;- sample(seq(1, n, by = 1), size = m, replace = TRUE, prob = q1) post1 &lt;- samp[inds1] ## generate the plot and corresponding legend hist(post1, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2, 1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 4.5)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, g, lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) Figure 2.4: Histogram of posterior sample 1 obtained using SIR (with densities for \\(h\\) and \\(g\\)) The uniform proposal distribution appears to be serviceable for this example. The density curve for \\(h(\\theta)\\)  which is not known exactly in most situations  agrees well with histogram of the approximate posterior sample. However, the simulation results confirm that this distribution is not very efficient. The histogram of the standardized weights for this simulation is given in Figure 2.5. ## generate the plot and corresponding legend hist(q1, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, xlab = expression(q), ylab = &quot;Frequency&quot;) Figure 2.5: Histogram of standardized weights from SIR algorithm (uniform proposal) If we were to use \\(h(\\theta)\\) as the proposal, all weights should be equal to \\(n^{-1} = 1000000^{-1} = 1 \\times 10^{-6}\\). In Figure 2.5, we can see that many of the standarized weights are extremely close to 0, and these \\(\\theta\\) values are very unlikely to be selected during the weighted sampling process. In contrast, some of the weights are more than 4 times greater than \\(1 \\times 10^{-6}\\). If we do not generate many points from \\(g(\\theta)\\) (\\(n\\) is small), then our posterior sample may not have many unique points. This issue can be overcome by generating a very large sample of points from the proposal distribution. We now discuss other potential choices for the proposal distribution for this example. 2.4.2 Practical Considerations for Choosing Proposal Distributions Diffuse distributions that are easy to sample from are often popular choices for proposal distributions. In particular, the uniform and normal distributions are often parameterized appropriately to serve this purpose. There are adaptive extensions to the SIR algorithm, including those by West (1993) and Givens and Raftery (1996). These methods typically specify a diffuse proposal distribution to obtain an initial sample from \\(h(\\boldsymbol{\\theta})\\). This initial sample from \\(h(\\boldsymbol{\\theta})\\) then informs a better proposal distribution and the SIR algorithm is run again. This process is repeated as necessary. We do not discuss these methods further in this workshop; we focus on the case where an approximate sample is obtained by running the SIR algorithm one time with a given proposal distribution. We now show how to implement the SIR algorithm for this example with the normal proposal density \\(N(0.75, 0.15)\\), where \\(0.15\\) is the standard deviation. We discuss why this is an appropriate proposal distribution shortly. The results are visualized in Figure 2.6. set.seed(3) ## define parameters for proposal distribution mu_g &lt;- 0.75 sigma_g &lt;- 0.15 ## sample from proposal distribution samp &lt;- rnorm(n, mu_g, sigma_g) ## form importance sampling weights on log-scale; w2 &lt;- propLog(samp, N, sum_x) - dnorm(samp, mu_g, sigma_g, log = TRUE) w2 &lt;- exp(w2 - max(w2)) q2 &lt;- w2/sum(w2) ## resample to create approximate sample from posterior inds2 &lt;- sample(seq(1, n, by = 1), size = m, replace = TRUE, prob = q2) post2 &lt;- samp[inds2] ## generate the plot and corresponding legend hist(post2, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2, 1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 4.5)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, dnorm(theta, mu_g, sigma_g), lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) Figure 2.6: Histogram of posterior sample 2 obtained using SIR (with densities for \\(h\\) and \\(g\\)) We can see the \\(N(0.75, 0.15)\\) proposal distribution performs well for this example. The density curve for \\(h(\\theta)\\) coincides with histogram of the approximate posterior sample. We can see that the density curve for this proposal distribution \\(g(\\theta)\\) resembles the posterior of interest \\(h(\\theta)\\) more than the uniform proposal distribution. In fact, the modes of \\(h(\\theta)\\) and \\(g(\\theta)\\) are the same: 0.75. We note that the support of the normal distribution is not restricted to the unit interval. However, this is not a problem. The \\(\\theta_i\\) values generated via \\(g(\\theta)\\) such that \\(\\theta_i \\notin (0,1)\\) are assigned standardized sampling weights of \\(q_i = 0\\). The histogram of the standardized weights for this simulation, given in Figure 2.7, suggests that this proposal distribution is more efficient than the previous one: the standardized weights take values closer to \\(1 \\times 10^{-6}\\) than for the previous example. We discuss numerical methods to compare proposal distributions in the next subsection. ## generate the plot and corresponding legend hist(q2, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, xlab = expression(q), ylab = &quot;Frequency&quot;) Figure 2.7: Histogram of standardized weights from SIR algorithm (normal proposal 1) We now explore what occurs when we implement the SIR algorithm for this example with an inappropriate proposal distribution. We choose the normal proposal density \\(N(0.85, 0.05)\\), where \\(0.05\\) is the standard deviation. We discuss why this is not an appropriate proposal distribution shortly. The results are visualized in Figure 2.8. set.seed(5) ## define parameters for proposal distribution mu_g &lt;- 0.85 sigma_g &lt;- 0.05 ## sample from proposal distribution samp &lt;- rnorm(n, mu_g, sigma_g) ## form importance sampling weights on log-scale; w3 &lt;- propLog(samp, N, sum_x) - dnorm(samp, mu_g, sigma_g, log = TRUE) w3 &lt;- exp(w3 - max(w3)) q3 &lt;- w3/sum(w3) ## resample to create approximate sample from posterior inds3 &lt;- sample(seq(1, n, by = 1), size = m, replace = TRUE, prob = q3) post3 &lt;- samp[inds3] ## generate the plot and corresponding legend hist(post3, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2, 1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 8)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, dnorm(theta, mu_g, sigma_g), lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1, 2), bty = &quot;n&quot;) Figure 2.8: Histogram of posterior sample 3 obtained using SIR (with densities for \\(h\\) and \\(g\\)) We can see the \\(N(0.85, 0.05)\\) proposal distribution does not perform well for this example. The density curve for \\(h(\\theta)\\) deviates histogram of the approximate posterior sample. Although the support for \\(g(\\theta)\\) is \\(\\theta \\in \\mathbb{R}\\), \\(g(\\theta)\\) is practically 0 for \\(\\theta &lt; 0.6\\). It is extremely unlikely that we will sample \\(\\theta\\) values less than 0.6 for finite \\(n\\), and they will not appear in the final sample. In practice, we do not know \\(h(\\theta)\\), so we cannot compare the distribution of the sample to the density of \\(h(\\theta)\\). However, we can compare the distribution of the approximate posterior sample to the proposal \\(g(\\theta)\\). We can see that many of the \\(\\theta\\) values in our sample correspond to extreme values of the proposal distribution \\(g(\\theta)\\); this suggests that the proposal distribution may not be diffuse enough. If \\(\\boldsymbol{\\theta}\\) is multivariate, then we could compare low dimensional projections of the sample distribution and \\(g(\\boldsymbol{\\theta})\\). ## return maximum standardized weight (max(q3)) ## [1] 0.01455077 The maximum standardized weight for this simulation is 0.01455. This is 14550 times larger than \\(1 \\times 10^{-6}\\), which indicates that the proposal distribution does not resemble the density of interest \\(h(\\theta)\\) very much. 2.4.3 Comparing Proposal Distributions Here, we consider two numerical metrics to evaluate the suitability of the proposal distribution that are presented in Givens and Raftery (1996). If the typically unknown \\(h(\\boldsymbol{\\theta})\\) were used as a proposal distribution, all standardized sampling weights would take a value of \\(1 \\times 10^{-6}\\). When most sampling weights take values close to \\(1 \\times 10^{-6}\\), our final approximate sample of size \\(m\\) should contain many unique points. That is, our sample should not be comprised of several points that are each selected many times. We let \\(Q\\) be the number of unique points in our final sample of \\(m\\) values. When all sampling weights are constant (i.e., equal to \\(1/n\\)), then \\(\\mathbb{E}(Q) \\approx n(1- \\text{exp}(-m/n))\\). We then define \\[U = \\frac{Q}{n(1- \\text{exp}(-m/n))}.\\] \\(U\\) generally takes values between 0 and 1. We note that \\(U\\) can take values of greater than 1, particularly when the proposal density closely resembles \\(h(\\boldsymbol{\\theta})\\). However, this is not likely to be the case when \\(g(\\boldsymbol{\\theta})\\) and \\(h(\\boldsymbol{\\theta})\\) differ substantially. Generally, larger values of \\(U\\) are preferred. We now compute the \\(U\\) values for each of the three proposal densities considered for this illustrative example: \\(U(0,1)\\), \\(N(0.75, 0.15)\\), and \\(N(0.85, 0.05)\\). ## compute denominator for U (number of m = 50000 points expected to ## be unique under proposal h) (denom &lt;- n * (1 - exp(-m/n))) ## [1] 48770.58 ## compute U for U(0,1) (u1 &lt;- round(length(unique(inds1))/denom, 4)) ## [1] 0.9511 ## compute U for N(0.75, 0.15) (u2 &lt;- round(length(unique(inds2))/denom, 4)) ## [1] 0.9943 ## compute U for N(0.85, 0.05) (u3 &lt;- round(length(unique(inds3))/denom, 4)) ## [1] 0.6875 The \\(U\\) values are 0.9511 for \\(U(0,1)\\), 0.9943 for \\(N(0.75, 0.15)\\), and 0.6875 for \\(N(0.85, 0.05)\\). The \\(N(0.75, 0.15)\\) proposal distribution has the largest \\(U\\) value, which agrees with informal insights drawn from the histograms of the standardized sampling weights. Similarly, we could also consider the mean squared distance between \\(n\\) times the standardized sampling weights and 1. To this end, we let \\[D = n^{-1}\\sum_{i=1}^n(n\\times q_i - 1)^2 = n\\sum_{i=1}^n(q_i - n^{-1})^2,\\] where \\(n\\) = 1 million for this example. This metric \\(D\\) penalizes proposal densities that give rise to situations where a few small regions of its support yield extremely large standardized sampling weights. Generally, smaller values of \\(D\\) are preferred. We now compute the \\(D\\) values for each of the three proposal densities considered for this illustrative example. ## compute D for U(0,1) (d1 &lt;- round(n * sum((q1 - 1/n)^2), 3)) ## [1] 2.046 ## compute D for N(0.75, 0.15) (d2 &lt;- round(n * sum((q2 - 1/n)^2), 3)) ## [1] 0.291 ## compute D for N(0.85, 0.05) (d3 &lt;- round(n * sum((q3 - 1/n)^2), 3)) ## [1] 658.621 The \\(D\\) values are 2.046 for \\(U(0,1)\\), 0.291 for \\(N(0.75, 0.15)\\), and 658.621 for \\(N(0.85, 0.05)\\). The \\(N(0.75, 0.15)\\) proposal distribution has the smallest \\(D\\) value. Givens and Raftery (1996) suggested comparing competing proposal distributions using the ratio of their D values. In this case, the \\(N(0.75, 0.15)\\) proposal is about 7 times better than the \\(U(0,1)\\) proposal. 2.5 Sampling-Resampling in Multiple Dimensions 2.5.1 Exercise with Illustrative Example We now briefly consider how to implement the SIR algorithm for a multivariate example. In this example, we assume that we observe data \\(x_1, x_2 &gt;0\\). Here, we assume that \\(\\boldsymbol{\\theta} = (\\alpha, \\beta)^T\\) for \\(0 &lt; \\alpha &lt; 1\\) and \\(0 &lt; \\beta &lt; 1\\). We independently assume a \\(U(0,1)\\) prior for \\(\\alpha\\) and \\(\\beta\\), which implies that \\(p(\\alpha, \\beta) = 1\\) for \\(0 &lt; \\alpha, \\beta &lt; 1\\). In this case, the posterior \\(h(\\alpha, \\beta) = \\pi(\\alpha, \\beta| \\boldsymbol{x})\\) is proportional to the likelihood function that we define below: \\[h(\\alpha, \\beta) \\propto L(\\alpha, \\beta; \\boldsymbol{x}) = \\alpha(1-\\alpha)\\beta(1-\\beta)\\text{exp}(-x_1\\alpha -\\beta^{x_2}) = f(\\alpha,\\beta).\\] Lets assume that we observe \\(x_1 = 3\\) and \\(x_2 = 5\\) We will use the prior \\(p(\\alpha, \\beta) = 1\\) for \\(0 &lt; \\alpha, \\beta &lt; 1\\) as the proposal density \\(g(\\alpha, \\beta)\\). You can complete the following code block to generate a contour plot of the posterior sample, depicted in Figure 2.9. A helper function is provided to compute \\(\\text{log}(f(\\alpha, \\beta))\\). In general, it can be more difficult to chose proposal densities for multivariate posteriors of interest, especially when the support is unbounded. Adaptive sampling-resampling methods can be quite useful in these situations. Moreover, we generally require larger values for \\(n\\) and \\(m\\) used with the SIR algorithm when the domain over which we want to sample is multivariate. ## define SIR algorithm settings n &lt;- 2500000 m &lt;- 125000 ## define a function that is proportional to the posterior (on the ## logarithmic scale) propLog &lt;- function(alpha, beta) { return(log(alpha) + log(1 - alpha) + log(beta) + log(1 - beta) - 3 * alpha - beta^5) } set.seed(6) ## sample from proposal distribution (for alpha and beta) samp_alpha &lt;- runif(n) samp_beta &lt;- runif(n) ## form importance sampling weights on log-scale w &lt;- propLog(samp_alpha, samp_beta) w &lt;- exp(w - max(w)) q &lt;- w/sum(w) ## resample to create approximate sample from posterior inds &lt;- sample(seq(1, n, by = 1), size = m, replace = TRUE, prob = q) post_alpha &lt;- samp_alpha[inds] post_beta &lt;- samp_beta[inds] library(MASS) ## generate a contour plot for posterior den &lt;- kde2d(x = post_alpha, y = post_beta, n = 100) zlim &lt;- range(den$z) contour(den$x, den$y, den$z, col = &quot;grey10&quot;, xlim = c(0, 1), ylim = c(0, 1), levels = pretty(zlim, 10), lwd = 1, xlab = expression(alpha), ylab = expression(beta)) Figure 2.9: Contour plot of posterior sample obtained using SIR for multivariate example "],["bernstein-von-mises-theorem.html", "Chapter 3 Bernstein-von Mises Theorem 3.1 Introduction 3.2 Theorem 3.3 Limitations", " Chapter 3 Bernstein-von Mises Theorem Augustine Wigle 3.1 Introduction The Bernstein-von Mises theorem (or BVM theorem) connects Bayesian and frequentist inference. In this workshop, we will state the theorem, talk about its importance, and touch on some of the required assumptions. We will work through some examples in R which let us visualize the theorem, and finally, we will talk about violations of the assumptions. This workshop is intended to introduce you to the theorem and encourage you to consider when it applies. It is not a rigourous treatment of the theorem - for those interested in the more technical details, we recommend Van der Vaart (2000) and Kleijn and Van der Vaart (2012). In this section we will give a brief review of Bayesian inference, the posterior distribution, and credible intervals. 3.1.1 Bayesian Inference Since this theorem applies to Bayesian models, we will give a quick review of Bayesian inference. In Bayesian inference, our state of knowledge about anything unknown is described by a probability distribution. Bayesian statistical conclusions about a parameter \\(\\theta\\) are made in terms of probabilistic statements conditional on the observed data \\(y\\). The distribution of interest is therefore \\(p(\\theta \\mid y)\\), the posterior distribution. We first specify a model which provides the joint probability distribution, that is, \\(p(\\theta, y) = p(\\theta)p(y\\mid \\theta)\\), where \\(p(\\theta)\\) is the prior distribution, which describes prior beliefs about the parameter(s) \\(\\theta\\), and \\(p(y\\mid \\theta)\\) is the data distribution of the likelihood. Then, Bayes theorem shows how to obtain the posterior distribution: \\[\\begin{equation*} p(\\theta \\mid y) = \\frac{p(\\theta)p(y\\mid \\theta)}{p(y)} \\end{equation*}\\] In most models, \\(p(\\theta\\mid y)\\) does not have a known parametric form and computational methods are required to overcome this problem, such as Markov Chain Monte Carlo (MCMC), or sampling-resampling techniques, covered in Chapter 2. In some special cases of likelihood and prior distribution combinations, the posterior can be determined analytically. These are called conjugate models. In the examples throughout this workshop, we will use conjugate models to avoid the need for fancy sampling techniques. Point estimates for \\(\\theta\\) can be derived from the posterior distribution, for example, by taking the posterior median or mean. Credible intervals are the Bayesian version of confidence intervals. A credible interval of credibility level \\(100\\times(1-\\alpha)\\) are defined as sets where the posterior probability of \\(\\theta\\) in the set is \\(100\\times(1-\\alpha)\\) and can be obtained in a variety of ways, such as by taking the lower and upper \\(\\alpha/2\\) quantiles of the posterior distribution. 3.2 Theorem A succinct statement of the theorem is as follows (for a more detailed and technical statement and proof, see Van der Vaart (2000)): Theorem 3.1 Under certain assumptions, a posterior distribution converges to a multivariate normal distribution centred at the maximum likelihood estimate \\(\\hat \\theta\\) and with covariance matrix given by \\(n^{-1}I(\\theta_0)^{-1}\\) as the sample size \\(n \\rightarrow \\infty\\) , where \\(\\theta_0\\) is the true population parameter and \\(I(\\theta_0)\\) is the Fisher information matrix evaluated at \\(\\theta_0\\). Or in other words, as you get more data, the posterior looks more and more like the sampling distribution of the MLE (if the assumptions are satisfied). 3.2.1 Importance The BVM theorem is useful because it provides a frequentist justification for Bayesian inference. The main takeaway of the theorem is that Bayesian inference is asymptotically correct from a frequentist perspective. In particular, Bayesian credible intervals are asymptotically confidence intervals. Another way to think about the theorems interpretation is that the influence of the prior disappears and the posterior becomes normal once you observe enough data. This is analogous to a pure frequentist approach where there is no prior information and the sampling distribution of the MLE becomes normal as you observe more and more data. 3.2.2 Required Assumptions Of course, for the theorem to hold, we require several assumptions to be satisfied. We are only going to touch on some of the more important conditions and we will not go into the technical details for this workshop (for technical details see Van der Vaart (2000)). Some of the more important assumptions are: The log-likelihood is smooth The MLE is consistent The prior distribution has non-zero density in a neighborhood of the true value \\(\\theta_0\\) The true parameter value is on the interior of the parameter space The model has a finite and fixed number of parameters We will discuss when some of these conditions may be violated and see some examples. We will start with an example where this theorem holds! 3.2.3 Example 1 - Normal-normal model Lets consider a nice example: we observe \\(Y_1, \\dots, Y_n\\) from a \\(N(\\theta, 1)\\) distribution. We are interested in estimating \\(\\theta\\). Lets also suppose that the true value of \\(\\theta\\) is 0. Bayesian Approach We will use a normal prior for \\(\\theta\\), that is, \\(\\theta \\sim N(0,1)\\). This is actually an example of a conjugate distribution and so there is an analytical solution for the posterior, which will make plotting the solution very convenient: \\[\\begin{equation*} \\theta \\mid Y_1, \\dots, Y_n \\sim N(\\frac{\\sum_{i=1}^n Y_i}{n+1}, \\frac{1}{n + 1}). \\end{equation*}\\] Frequentist Approach The MLE of \\(\\theta\\) is the sample mean, \\(\\hat\\theta = \\sum_{i=1}^nY_i/n\\) and the Fisher information is \\(1\\). The sampling distribution of \\(\\hat \\theta\\) is \\[\\begin{equation*} \\hat \\theta\\sim N(0, \\frac{1}{n}). \\end{equation*}\\] Lets look at what happens as we observe more and more data in R. # Set true param value theta_true &lt;- 0 # Functions to calculate the posterior mean and sd post_mean &lt;- function(x) { n &lt;- length(x) sum(x)/(n+1) } post_sd &lt;- function(x) { n &lt;- length(x) sqrt(1/(n+1)) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(x) { sqrt(1/length(x)) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rnorm(10, mean = theta_true, sd = 1) y_med &lt;- rnorm(50, mean = theta_true, sd = 1) y_large &lt;- rnorm(100, mean = theta_true, sd = 1) # Set up Plotting x_vals &lt;- seq(-1, 1, by = 0.01) par(mfrow = c(3,1)) # Plot the asymptotic distribution of MLE and posterior distribution for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 1.3)) lines(x_vals, dnorm(x_vals, mean = post_mean(y_small), sd = post_sd(y_small)), col = &quot;navy&quot;) abline(v = 0, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 50 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 50&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_med), sd = post_sd(y_med)), col = &quot;navy&quot;) abline(v = 0, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_large), sd = post_sd(y_large)), col = &quot;navy&quot;) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1,1)) abline(v = 0, lty = 2) 3.2.4 Example 2 - Bernoulli-Beta Model In the previous example, we didnt actually need Bernstein-von Mises to get asymptotic normality of the posterior, because the posterior distribution was already normal by definition regardless of how much data we had. Lets look at another example where we can see the posterior getting more normal as we observe more data. Again, we will take advantage of conjugacy for computational convenience. This time, consider observing \\(Y_1, \\dots, Y_n\\) from \\(Bernoulli(p)\\), where the true value of \\(p\\) is 0.5. Bayesian Approach We will use a \\(Beta(1, 5)\\) distribution to take advantage of conjugacy. Then the posterior distribution for \\(p\\) is: \\[\\begin{equation*} p\\mid Y_1,\\dots, Y_n \\sim Beta(1+ \\sum_{i=1}^n Y_i, n+5 - \\sum_{i=1}^n Y_i) \\end{equation*}\\] Frequentist Approach The MLE \\(\\hat p = \\sum_{i=1}^n Y_i/n\\) and the Fisher information is \\(1/p(1-p)\\). The sampling distribution is then \\[\\begin{equation*} \\hat p \\sim N(0.5, \\frac{p(1-p)}{n}) \\end{equation*}\\] p_true &lt;- 0.5 # Functions to calculate the posterior parameters post_alpha &lt;- function(x) { 1+sum(x) } post_beta &lt;- function(x) { n &lt;- length(x) n+5-sum(x) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(p, x) { n &lt;- length(x) sqrt(p*(1-p)/n) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rbinom(10, size = 1, prob = p_true) y_med &lt;- rbinom(50, size = 1, prob = p_true) y_large &lt;- rbinom(200, size = 1, prob = p_true) # Set up Plotting x_vals &lt;- seq(0, 1, by = 0.01) par(mfrow = c(3,1)) # Plot the asymptotic distribution of MLE and posterior distribution for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(p_true, y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 3)) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_small), shape2 = post_beta(y_small)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 50 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(p_true, y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 50&quot;) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_med), shape2 = post_beta(y_med)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(p_true, y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 200&quot;) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_large), shape2 = post_beta(y_large)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1,1)) 3.3 Limitations We do need to be careful with applying Bernstein-von Mises to make sure important assumptions are satisfied! Several of these examples were motivated by this blog post by Dan Simpson which I found entertaining and thought-provoking (Simpson 2017). Here are some examples of when important assumptions of BVM might be violated: The log-likelihood being smooth may be violated in situations were we want to integrate out nuisance parameters, because this can cause spikes in the log likelihood. Our model may not have a fixed number of parameters, for example, in a multi-level model where observing more data may require including more categories and therefore more parameters. Some models have infinite-dimensional parameters, such as models which use non-parametric effects to model unknown functions. The prior may give zero density to the true parameter value - see example 3 \\(\\theta_0\\) may be on the boundary of the parameter space - see example 4 The MLE may not be consistent when the log-likelihood is multimodal, such as in some mixture models. Some other situations where the MLE may not be consistent arise when other assumptions are violated, like the number of parameters increasing with \\(n\\) or the true parameter value being on the boundary of the parameter space. 3.3.1 Other thoughts on consistency Other considerations around consistency which I found interesting are raised by Dan Simpson in his blog post mentioned above. When we consider consistency, we need to consider how the data were collected, and consider what it means to have independent replicates of the same experiment. A lot of datasets are observational, and so there is no guarantee that the data can actually be used to give a consistent estimator of the parameter we want to estimate, regardless of how many times we conduct the experiment. Similarly, collecting a lot of data can take a long time, and over time, the underlying process that we are trying to study may change. This will also present a challenge for consistency. 3.3.2 Example 3 - Prior has zero density at \\(\\theta_0\\) An example of this would be using a uniform prior for a standard deviation and choosing an upper bound which is less than the true standard deviation. For this talk, we wont do an example where the prior gives 0 density to the true value because for these models we would need to use some computational tools tog et the posterior distribution, but we can look at what happens when the prior gives very little density to the true value. Lets return to the first example, where we have collected data from a normal distribution with known variance 1 and we want to estimate its mean. In the first example, we used a prior centred at 0 with variance 1, and the true value happened to be 0. What if the true mean is actually 1000? The prior \\(N(0,1)\\) is now very informative, and will give very little density (but still non-zero) to the true parameter value. Now we have: Bayesian Approach Recall that the posterior is: \\[\\begin{equation*} \\theta \\mid Y_1, \\dots, Y_n \\sim N(\\frac{\\sum_{i=1}^n Y_i}{n+1}, \\frac{1}{n + 1}). \\end{equation*}\\] Frequentist Approach The asymptotic distribution of the MLE is: \\[\\begin{equation*} \\hat \\theta\\sim N(1000, \\frac{1}{n}). \\end{equation*}\\] # Set true param value theta_true &lt;- 1000 # Functions to calculate the posterior mean and sd post_mean &lt;- function(x) { n &lt;- length(x) sum(x)/(n+1) } post_sd &lt;- function(x) { n &lt;- length(x) sqrt(1/(n+1)) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(x) { sqrt(1/length(x)) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rnorm(10, mean = theta_true, sd = 1) y_med &lt;- rnorm(100, mean = theta_true, sd = 1) y_large &lt;- rnorm(1000, mean = theta_true, sd = 1) # Set up Plotting x_vals &lt;- seq(985, 1001, by = 0.01) par(mfrow = c(3,1)) # Plot the asymptotic distribution of MLE and posterior distribution for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 1.3)) lines(x_vals, dnorm(x_vals, mean = post_mean(y_small), sd = post_sd(y_small)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_med), sd = post_sd(y_med)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution for n = 1000 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 1000&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_large), sd = post_sd(y_large)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1,1)) 3.3.3 Example 4 - True parameter value is on the boundary Lets return to the Bernoulli example from before, but this time we will see what happens when the probability is on the boundary, that is, the true value of \\(p = 1\\). Bayesian Approach Recall the posterior distribution for \\(p\\) with a \\(Beta(1, 5)\\) prior is: \\[\\begin{equation*} p\\mid Y_1,\\dots, Y_n \\sim Beta(1+ \\sum_{i=1}^n Y_i, n+5 - \\sum_{i=1}^n Y_i) \\end{equation*}\\] Frequentist Approach The MLE \\(\\hat p = \\sum_{i=1}^n Y_i/n\\) and the Fisher information is \\(1/p(1-p)\\). The sampling distribution is then \\[\\begin{equation*} \\hat p \\sim N(1, 0). \\end{equation*}\\] Note that the variance of the MLEs sampling distribution is now zero, so in other words, it is just a point mass at \\(p=1\\)! Lets see what happens to the posterior. p_true &lt;- 1 # Functions to calculate the posterior mean and sd post_alpha &lt;- function(x) { 1+sum(x) } post_beta &lt;- function(x) { n &lt;- length(x) n+5-sum(x) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(p, x) { n &lt;- length(x) sqrt(p*(1-p)/n) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rbinom(10, size = 1, prob = p_true) y_med &lt;- rbinom(100, size = 1, prob = p_true) y_large &lt;- rbinom(1000, size = 1, prob = p_true) # Set up Plotting x_vals &lt;- seq(0, 1, by = 0.01) par(mfrow = c(3,1)) # Plot the asymptotic distribution of MLE and posterior distribution for n = 10 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_small), shape2 = post_beta(y_small)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 3)) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) # Plot the asymptotic distribution of MLE and posterior distribution for n = 50 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_med), shape2 = post_beta(y_med)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) # Plot the asymptotic distribution of MLE and posterior distribution for n = 100 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_large), shape2 = post_beta(y_large)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 1000&quot;) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1,1)) "],["variational-inference.html", "Chapter 4 Variational Inference 4.1 Introduction 4.2 Example: Mixture of Gaussians 4.3 Example: Stochastic Variational Inference using Pyro in Python 4.4 Takeaways", " Chapter 4 Variational Inference Meixi Chen 4.1 Introduction Variational inference (VI) is an inference technique that is commonly used to approximate an intractable quantity such as a probability density. In the following, we introduce two scenarios where the VI is often used, one in a frequentist setting and the other in a Bayesian setting. 4.1.1 Frequentist Setting The frequentist example is adapted from the one in Chen (2020). Consider the following IID observations \\(Y_1,\\ldots,Y_n\\). Now assume that each \\(Y_i\\) is accompanied with an unobserved latent variable \\(Z_i\\). That is, the complete data is \\((Y_1,Z_1),\\ldots,(Y_n,Z_n)\\), but we only observe the incomplete data \\(Y_1,\\ldots,Y_n\\). Assume that we know the parametric model for the complete data is \\(p(y, z\\mid \\theta)\\), and our interest lies in estimating \\(\\theta\\). One way to do it is to maximize the observed log-likelihood \\(\\ell(\\theta\\mid y_1,\\ldots,y_n) = \\sum_{i=1}^n \\log p(y_i\\mid\\theta)\\), where \\[\\begin{equation} \\tag{4.1} p(y_i\\mid\\theta)=\\int p(y_i, z_i\\mid \\theta) \\ dz_i. \\end{equation}\\] However, this integral is typically difficult to compute. Many techniques exist to deal with the problem of (4.1) (e.g., MCMC, Laplace approximation, EM). The VI is one such method to solve this problem, which writes \\[\\begin{equation} \\tag{4.2} \\begin{aligned} p(y\\mid \\theta) &amp;= \\int p(y, z\\mid \\theta)\\ dz\\\\ &amp;= \\int \\frac{p(y, z\\mid \\theta)}{\\color{red}{q(z\\mid \\omega)}}\\color{red}{q(z\\mid \\omega)} \\ dz\\\\ &amp;= \\mathbb{E}_{Z}\\left[\\frac{p(y, z\\mid \\theta)}{\\color{red}{q(z\\mid \\omega)}}\\right] \\end{aligned} \\end{equation}\\] where \\(Z\\sim q(z\\mid \\omega)\\) and \\(q(\\cdot \\mid\\omega)\\) is called the variational distribution and typically has an easy form (e.g. Normal distribution). All possible candidate variational distributions form the variational family \\(\\mathcal{Q}=\\{q(\\cdot\\mid\\omega): \\ \\omega \\in \\Omega\\}\\). Given (4.2), we can compute the observed log-likelihood as \\[\\begin{equation} \\tag{4.3} \\begin{aligned} \\ell(\\theta\\mid y) &amp;= \\log p(y\\mid \\theta)\\\\ &amp;= \\log \\mathbb{E}_{Z}\\left[\\frac{p(y, z\\mid \\theta)}{q(z\\mid \\omega)}\\right]\\\\ &amp;\\ge \\mathbb{E}_{Z}\\left[\\log\\frac{p(y, z\\mid \\theta)}{q(z\\mid \\omega)}\\right] \\text{ using Jensen&#39;s inequality}\\\\ &amp;=\\mathbb{E}_{Z}(\\log p(y, z\\mid \\theta))-\\mathbb{E}_{Z}(\\log q(z\\mid \\omega))\\\\ &amp;:= \\mathrm{ELBO}(\\omega,\\theta\\mid y) := \\mathrm{ELBO}(q) \\end{aligned} \\end{equation}\\] where \\(\\mathrm{ELBO}(q)\\) is known as the Evidence Lower Bound. Now, instead of maximizing \\(\\ell(\\theta\\mid y_1,\\ldots,y_n)\\), we can maxmize the ELBO via any numerical optimization algorithm, i.e., \\[\\begin{equation} \\tag{4.4} (\\hat{\\omega}, \\hat{\\theta}) = \\underset{\\omega,\\theta}{\\mathrm{argmax}}\\frac{1}{n}\\sum_{i=1}^n \\mathrm{ELBO}(\\omega,\\theta\\mid y_i). \\end{equation}\\] Important notes The parametric form of \\(q(\\cdot \\mid \\omega)\\) is up to the modeler. A common choice is Normal. When \\(q(z \\mid \\omega)\\) is multivariate, i.e., \\(z, \\omega\\in\\mathbb{R}^n\\), it is common to use the mean-field variational family \\(q(z\\mid \\omega)=\\prod_{i=1}^n q(z_i\\mid \\omega_i)\\). The VI estimator \\(\\hat{\\theta}_{\\mathrm{VI}}\\) generally does not converge to the MLE because \\(\\hat{\\theta}_{\\mathrm{VI}}\\) depends on the choice of \\(q(\\cdot \\mid \\omega)\\). Uncertainty assessment One way to assess the uncertainty of the VI estimator \\(\\tilde{\\theta}_{\\mathrm{VI}}\\) is via bootstrapping. Let \\((Y_1^{(b)}, \\ldots,Y_n^{(b)})\\) be the \\(b\\)-th bootstrap sample from the original dataset, for \\(b=1,\\ldots, B\\). Given the bootstrap sample, we can compute the bootstrap VI estimate \\(\\hat{\\theta}_{\\mathrm{VI}}^{(b)}\\). After repeating the above procedure for \\(B\\) times, we obtain \\(B\\) bootstrap VI estimates: \\(\\hat{\\theta}_{\\mathrm{VI}}^{(1)}, \\ldots, \\hat{\\theta}_{\\mathrm{VI}}^{(B)}\\). The bootstrap estimates can be used to calculate the uncertainty of the original bootstrap estimator. 4.1.2 Bayesian Setting In Bayesian statistics, the VI is often used as an alternative to the traditional MCMC sampling method to estimate the posterior distributions \\(p(\\theta\\mid y)\\). Recall that, according to the Bayes rule, the posterior distribution is written as \\[p(\\theta\\mid y) = \\frac{p(y\\mid \\theta)p(\\theta)}{\\color{red}{p(y)}} = \\frac{p(y\\mid \\theta)p(\\theta)}{\\int p(y\\mid \\theta) p(\\theta)\\ d\\theta},\\] where \\(p(y\\mid \\theta)\\) is the likelihood taking a known form and \\(p(y)\\) is a constant. Similar to the problem in (4.1), the integral \\(p(y)=\\int p(y\\mid \\theta) p(\\theta)\\ d\\theta\\) is typically intractable, which makes calculating the posterior difficult. What the VI does in this case is to find a variational distribution \\(q(\\theta \\mid \\omega)\\) such that it is close enough to the posterior distribution of interest \\(p(\\theta\\mid y)\\). How do we know a distribution is close enough to another? The answer is using the Kullback-Leibler divergence \\(\\mathrm{KL}(Q\\Vert P)\\), which measures how different the probability distribution \\(Q\\) is from the reference distribution \\(P\\). Therefore, the VI method looks for \\(q(\\theta \\mid \\omega)\\) that minimizes \\(\\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big)\\), i.e., \\[q^*(\\theta \\mid \\omega) = \\underset{q(\\theta\\mid\\omega)}{\\mathrm{argmin}} \\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big).\\] The KL divergence is written as \\[\\begin{equation} \\tag{4.5} \\begin{aligned} \\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big) &amp;= \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta\\mid y)]\\\\ &amp;= \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}\\left[\\log \\frac{p(\\theta, y)}{p(y)}\\right]\\\\ &amp;= \\underbrace{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)]}_{-\\mathrm{ELBO(q)}} + \\underbrace{\\color{red}{\\log p(y)}}_{\\text{constant}} \\end{aligned} \\end{equation}\\] Noting that \\(\\log p(y)\\) is a constant, minimizing the KL divergence (4.5) is in fact equivalent to minimizing the nonconstant part of (4.5): \\[\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)].\\] Then we notice that it is in fact the negative of the ELBO, so minimizing (4.5) is in turn equivalent to maximizing the ELBO: \\[\\mathrm{ELBO}(q) = \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)].\\] Therefore, we can estimate \\(\\omega\\) as \\[\\hat{\\omega} = \\underset{\\omega}{\\mathrm{argmax}} \\ \\mathrm{ELBO}(q) = \\underset{\\omega}{\\mathrm{argmax}} \\ \\mathrm{ELBO}(\\omega\\mid y).\\] Finally, the posterior of interest is approximated by \\[p(\\theta\\mid y) \\approx q(\\theta \\mid \\hat{\\omega}).\\] Important note Similar to the frequentist setting, we need to pick the distributional form of \\(q(\\cdot \\mid \\omega)\\). The posterior distribution obtained this way is an approximation rather than the truth, whereas MCMC methods guarantees that the Monte Carlo samples converge to the true posterior distribution. Takeaway Only use the VI to estimate the posterior if the dimension of the posterior is too high for sampling, or the model is too complex for MCMC methods to run within a reasonable amount of time (within a day). 4.2 Example: Mixture of Gaussians This example is taken from a famous VI tutorial paper by Blei, Kucukelbir, and McAuliffe (2017). Consider the following Gaussian mixture model: \\[\\begin{equation} \\tag{4.6} \\begin{aligned} \\mu_k &amp;\\sim \\mathcal{N}(0, \\sigma^2), \\ \\ &amp; k=1,\\ldots, K,\\\\ c_i &amp;\\sim \\mathrm{Categorical}(1/K, \\ldots, 1/K), \\ \\ &amp; i=1,\\ldots, n,\\\\ x_i|c_i, \\boldsymbol{\\mu} &amp;\\sim \\mathcal{N}(c_i^T\\boldsymbol{\\mu}, 1), \\ \\ &amp; i=1,\\ldots,n, \\end{aligned} \\end{equation}\\] where the prior variance \\(\\sigma^2\\) is known, and \\(c_i=(0,0,\\ldots,1,\\ldots,0)\\) is one-hot encoding. The parameters of interest are the mean parameters \\((\\mu_1,\\ldots,\\mu_K)\\) and the cluster parameters \\((c_1,\\ldots,c_n)\\). We propose to use the mean-field variational family \\(q(\\boldsymbol{\\mu}, \\boldsymbol{c})=\\prod_{i=1}^Kq(\\mu_k)\\prod_{i=1}^nq(c_i)\\), where \\[\\begin{equation} \\tag{4.7} \\begin{aligned} q(\\mu_k\\mid m_k, s_k^2) &amp;= \\mathcal{N}(m_k, s_k^2), \\ \\ &amp; k=1,\\ldots, K\\\\ q(c_i\\mid \\phi_i) &amp;= \\mathrm{Categorical}(\\phi_{i1}, \\ldots, \\phi_{iK}), \\ \\ &amp; i=1,\\ldots n. \\end{aligned} \\end{equation}\\] Again, let \\(\\omega=(m, s, \\phi)\\) denote all the variational parameters, and let \\(\\theta=(\\mu_1,\\ldots,\\mu_K, c_1,\\ldots,c_n)\\) denote all the parameters of interest. Then the ELBO is written as \\[\\begin{equation} \\tag{4.8} \\begin{aligned} \\mathrm{ELBO}(q) &amp;= \\color{blue}{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, x)]}-\\color{purple}{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta \\mid \\omega)]}\\\\ &amp;= \\color{blue}{\\sum_{k=1}^K \\mathbb{E}[\\log p(\\mu_k); m_k, s_k^2]} \\\\ &amp; \\ \\ \\ \\ \\ \\ \\color{blue}{+\\sum_{i=1}^n \\left(\\mathbb{E}[\\log p(c_i); \\phi_i] + \\mathbb{E}[\\log p(x_i\\mid c_i, \\mu); \\phi, m, s^2]\\right)} \\\\ &amp; \\ \\ \\ \\ \\ \\ \\color{purple}{-\\sum_{i=1}^n\\mathbb{E}[\\log q(c_i\\mid \\phi_i)] - \\sum_{k=1}^K \\mathbb{E}[\\log q(\\mu_k\\mid m_k, s_k^2)]} \\end{aligned} \\end{equation}\\] Note that \\(\\mathbb{E}[a ; b]\\) specifies the quantity of interest \\(a\\) depends on the variational parameter \\(b\\). Each of the above term can be computed in closed form (but omitted here). The next question is, how do we maximize the ELBO? 4.2.1 Coordinate Ascent Mean-Field Variational Inference The coordinate ascent variational inference (CAVI) is commonly used to solve this optimization problem. It is particularly convenient for the mean-field variational family. Results: (Blei, Kucukelbir, and McAuliffe 2017) Let the full conditional of \\(\\theta_j\\) be \\(p(\\theta_j\\mid \\boldsymbol{\\theta}_{-j},x)\\). When all variational distributions \\(q(\\theta_{\\ell})\\) for \\(\\ell\\neq j\\) are fixed, the optimal \\(q(\\theta_j)\\) is proportionate to the exponentiated expected log complete conditional: \\[\\begin{equation} \\tag{4.9} q^*(\\theta_j) \\propto \\exp\\left\\{\\mathbb{E}_{-j}[\\log p(\\theta_j\\mid\\boldsymbol{\\theta}_{-j}, x)]\\right\\} \\end{equation}\\] This result is used to formulate the CAVI algorithm as follows. Algorithm: CAVI while the ELBO has not converged do for \\(j\\in\\{1,\\ldots,m\\}\\) do Set \\(q_j(\\theta_j) \\propto \\exp\\left\\{\\mathbb{E}_{-j}[\\log p(\\theta_j\\mid\\boldsymbol{\\theta}_{-j}, x)]\\right\\}\\) end Compute \\(\\mathrm{ELBO}(q)\\) end Return \\(q(\\boldsymbol{\\theta})\\) For the mixture of Gaussians example, Blei, Kucukelbir, and McAuliffe (2017) has derived the full conditionals and computed the updating rules for all variational parameters: \\[\\begin{equation} \\tag{4.10} \\begin{aligned} \\phi_{ik} &amp;\\propto \\exp\\{\\mathbb{E}[\\mu_k;m_k,s_k^2]x_i - \\mathbb{E}[\\mu_k^2;m_k,s_k^2]/2\\} \\ \\ \\text{(normalize afterwards)}\\\\ m_k &amp;= \\frac{\\sum_{i}^n \\phi_{ik}x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_{ik}} \\\\ s_k^2 &amp;= \\frac{1}{1/\\sigma^2 +\\sum_{i=1}^n \\phi_{ik}} \\end{aligned} \\end{equation}\\] Then the full CAVI algorithm for the Gaussian mixture model is given below. Algorithm: CAVI for Gaussian mixture model while the ELBO has not converged do for \\(i\\in\\{1,\\ldots,n\\}\\) do Set \\(\\phi_{ik} \\propto \\exp\\{\\mathbb{E}[\\mu_k;m_k,s_k^2]x_i - \\mathbb{E}[\\mu_k^2;m_k,s_k^2]/2\\}\\) end for \\(k\\in\\{1,\\ldots,K\\}\\) do Set \\(m_k = \\frac{\\sum_{i}^n \\phi_{ik}x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_{ik}}\\) Set \\(s_k^2 = \\frac{1}{1/\\sigma^2 +\\sum_{i=1}^n \\phi_{ik}}\\) end Compute \\(\\mathrm{ELBO}(\\boldsymbol{m},\\boldsymbol{s}^2, \\boldsymbol{\\phi})\\) end return \\(q(\\mu_k \\mid m_k, s_k^2)\\) and \\(q(c_k\\mid \\phi_i)\\) 4.3 Example: Stochastic Variational Inference using Pyro in Python As we have seen in the previous example, to use CAVI, we need to compute (and code up) the ELBO and the full conditionals in closed-form. This will be a pain when we have more complex models. When we perform statistical inference in practice, we typically only need to provide a model form. The model can be as simple as a formula such as Response ~ var1 + var2 in the glm package, or in a more complicated form such as the Stan language when we perform MCMC sampling using the rstan package. We almost never needed to derive any math! We can do the same with the VI. That is, we only need to provide the model formulation, and then let the software do the tedious derivation for us. However, there does not exist an automatic and versatile R package for VI. Therefore, we will look at Pyro, a probabilistic programming language (PPL) written in Python, which is a very convenient interface for implementing VI for complex models. Note that Pyro is supported by the popular deep learning framework Pytorch on the backend, so models written in Pyro can be easily extended to incorporate neural network architectures. We will go over a simpler example provided by the Pyro team itself. All the following code is in Python and originally provided here. To install pyro, simply run pip install pyro-ppl in your terminal. First, we import the required packages and set up some system parameters. import os import logging import time import torch import numpy as np import pandas as pd import seaborn as sns # for plotting import matplotlib.pyplot as plt # for plotting import pyro import pyro.distributions as dist import pyro.distributions.constraints as constraints pyro.enable_validation(True) pyro.set_rng_seed(1) logging.basicConfig(format=&#39;%(message)s&#39;, level=logging.INFO) plt.style.use(&#39;default&#39;) The dataset we will look at in this example studies the relationship between geography and national income. Specifically, we will examine how this relationship is different for nations in Africa and outside of Africa. It was found that, outside of Africa, bad geography is associated with lower GPD, whereas the relationship is reversed in Africa. Response variable rgdppc_2000: Real GPD per capita in 2000 (will be log transformed for analysis). Predictor rugged: The Terrain Ruggedness Index which measures the topographic heterogeneity of a nation. Predictor cont_africa: Whether the nation is in Africa. The following plots show the relationships between log GDP and ruggedness index for Non-African nations and African nations, respectively. DATA_URL = &quot;https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv&quot; data = pd.read_csv(DATA_URL, encoding=&quot;ISO-8859-1&quot;) df = data[[&quot;cont_africa&quot;, &quot;rugged&quot;, &quot;rgdppc_2000&quot;]] df = df[np.isfinite(df.rgdppc_2000)] df[&quot;rgdppc_2000&quot;] = np.log(df[&quot;rgdppc_2000&quot;]) # log transform the highly-skewed GDP data train = torch.tensor(df.values, dtype=torch.float) is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2] fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True) african_nations = df[df[&quot;cont_africa&quot;] == 1] non_african_nations = df[df[&quot;cont_africa&quot;] == 0] sns.scatterplot(x=non_african_nations[&quot;rugged&quot;], y=non_african_nations[&quot;rgdppc_2000&quot;], ax=ax[0]) ax[0].set(xlabel=&quot;Terrain Ruggedness Index&quot;, ylabel=&quot;log GDP (2000)&quot;, title=&quot;Non African Nations&quot;) sns.scatterplot(x=african_nations[&quot;rugged&quot;], y=african_nations[&quot;rgdppc_2000&quot;], ax=ax[1]) ax[1].set(xlabel=&quot;Terrain Ruggedness Index&quot;, ylabel=&quot;log GDP (2000)&quot;, title=&quot;African Nations&quot;) plt.show() A simple model to capture the relationship is \\[Y = \\alpha + \\beta_aX_a + \\beta_rX_r + \\beta_{ar} X_aX_r +\\epsilon, \\ \\ \\epsilon\\sim\\mathcal{N}(0,\\sigma^2)\\] where \\(Y\\) is the log GDP, \\(X_a\\) is an indicator for whether the nation is in Africa, \\(X_r\\) is the ruggedness index, and \\(\\epsilon\\) is the noise term. This model is defined as freq_model in the following, where the parameters and the observations are specified using pyro.param and pyro.sample, respectively. A cool feature of Pyro is that its render_model() function allows us to visualize the model. def freq_model(is_cont_africa, ruggedness, log_gdp=None): a = pyro.param(&quot;a&quot;, lambda: torch.randn(())) b_a = pyro.param(&quot;bA&quot;, lambda: torch.randn(())) b_r = pyro.param(&quot;bR&quot;, lambda: torch.randn(())) b_ar = pyro.param(&quot;bAR&quot;, lambda: torch.randn(())) sigma = pyro.param(&quot;sigma&quot;, lambda: torch.ones(()), constraint=constraints.positive) mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness with pyro.plate(&quot;data&quot;, len(ruggedness)): return pyro.sample(&quot;obs&quot;, dist.Normal(mean, sigma), obs=log_gdp) pyro.render_model(freq_model, model_args=(is_cont_africa, ruggedness, log_gdp), render_distributions=True, render_params=True, filename=&quot;freq_model.png&quot;); We can also define a Bayesian version of it called bayes_model as follows. This model simply replaces pyro.param with pyro.sample, so that the parameters are viewed as random variables following some (prior) distributions. Similarly, we call render_model() to visualize it. Between the Bayesian model and the frequentist model, we will use the former for demonstration here. def bayes_model(is_cont_africa, ruggedness, log_gdp=None): a = pyro.sample(&quot;a&quot;, dist.Normal(0., 10.)) b_a = pyro.sample(&quot;bA&quot;, dist.Normal(0., 1.)) b_r = pyro.sample(&quot;bR&quot;, dist.Normal(0., 1.)) b_ar = pyro.sample(&quot;bAR&quot;, dist.Normal(0., 1.)) sigma = pyro.sample(&quot;sigma&quot;, dist.Uniform(0., 10.)) mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness with pyro.plate(&quot;data&quot;, len(ruggedness)): return pyro.sample(&quot;obs&quot;, dist.Normal(mean, sigma), obs=log_gdp) pyro.render_model(bayes_model, model_args=(is_cont_africa, ruggedness, log_gdp), render_distributions=True, filename=&quot;bayes_model.png&quot;); In the context of Pyro, the variational distribution is called a guide. To specify the variational family we want, we need to define a guide program as follows. The code is very similar to that of the model. The custom guide we define below uses mean-field variational inference, i.e., all parameters have independent variational distributions in the form of Gaussian. \\(q(\\alpha\\mid \\mu_\\alpha, \\sigma^2_\\alpha) = \\mathcal{N}(\\mu_\\alpha, \\sigma^2_\\alpha)\\) \\(q(\\beta_a\\mid \\mu_{\\beta_a}, \\sigma^2_{\\beta_a}) = \\mathcal{N}(\\mu_{\\beta_a}, \\sigma^2_{\\beta_a}a)\\) \\(q(\\beta_r\\mid \\mu_{\\beta_r}, \\sigma^2_{\\beta_r}) = \\mathcal{N}(\\mu_{\\beta_r}, \\sigma^2_{\\beta_r})\\) \\(q(\\beta_{ar}\\mid \\mu_{\\beta_{ar}}, \\sigma^2_{\\beta_{ar}}) = \\mathcal{N}(\\mu_{\\beta_{ar}}, \\sigma^2_{\\beta_{ar}})\\) \\(q(\\sigma^2\\mid \\mu_\\sigma, \\sigma^2_\\sigma) = \\mathcal{N}(\\mu_\\sigma, \\sigma^2_\\sigma)\\) def custom_guide(is_cont_africa, ruggedness, log_gdp=None): a_loc = pyro.param(&#39;a_loc&#39;, lambda: torch.tensor(0.)) a_scale = pyro.param(&#39;a_scale&#39;, lambda: torch.tensor(1.), constraint=constraints.positive) sigma_loc = pyro.param(&#39;sigma_loc&#39;, lambda: torch.tensor(1.), constraint=constraints.positive) weights_loc = pyro.param(&#39;weights_loc&#39;, lambda: torch.randn(3)) weights_scale = pyro.param(&#39;weights_scale&#39;, lambda: torch.ones(3), constraint=constraints.positive) a = pyro.sample(&quot;a&quot;, dist.Normal(a_loc, a_scale)) b_a = pyro.sample(&quot;bA&quot;, dist.Normal(weights_loc[0], weights_scale[0])) b_r = pyro.sample(&quot;bR&quot;, dist.Normal(weights_loc[1], weights_scale[1])) b_ar = pyro.sample(&quot;bAR&quot;, dist.Normal(weights_loc[2], weights_scale[2])) sigma = pyro.sample(&quot;sigma&quot;, dist.Normal(sigma_loc, torch.tensor(0.05))) return {&quot;a&quot;: a, &quot;b_a&quot;: b_a, &quot;b_r&quot;: b_r, &quot;b_ar&quot;: b_ar, &quot;sigma&quot;: sigma} pyro.render_model(custom_guide, model_args=(is_cont_africa, ruggedness, log_gdp), render_params=True, filename=&quot;custom_guide.png&quot;); To implement variational inference in Pyro, we use its stochastic variational inference functionality pyro.infer.SVI(), which takes in four arguments: Model: bayes_model Guide (Variational distribution): custom_guide ELBO: We do not need to compute the explicit ELBO. It can be defined by calling pyro.infer.Trace_ELBO(), and this will automatically compute ELBO under the hood for us given model and guide. Optimizer: Any optimizer can do, but a popular choice is the Adam optimizer given by pyro.optim.Adam(). Adam is a gradient-based optimization algorithm that computes adaptive learning rates for different parameters. The plot below shows the ELBO loss (negative ELBO) as a function of the step index. We see that the optimization procedure has converged after about 500 steps. pyro.clear_param_store() auto_guide = pyro.infer.autoguide.AutoNormal(bayes_model) adam = pyro.optim.Adam({&quot;lr&quot;: 0.02}) elbo = pyro.infer.Trace_ELBO() svi = pyro.infer.SVI(bayes_model, auto_guide, adam, elbo) losses = [] vi_start = time.time() for step in range(1000): loss = svi.step(is_cont_africa, ruggedness, log_gdp) losses.append(loss) if step % 100 == 0: logging.info(&quot;Elbo loss: {}&quot;.format(loss)) ## Elbo loss: 694.9404839277267 ## Elbo loss: 524.3822101950645 ## Elbo loss: 475.668176651001 ## Elbo loss: 399.99088364839554 ## Elbo loss: 315.23277366161346 ## Elbo loss: 254.76771301031113 ## Elbo loss: 248.237025141716 ## Elbo loss: 248.42669039964676 ## Elbo loss: 248.46450036764145 ## Elbo loss: 257.41463327407837 vi_end = time.time() plt.figure(figsize=(5, 2)) plt.plot(losses) plt.xlabel(&quot;SVI step&quot;) plt.ylabel(&quot;ELBO loss&quot;) plt.show() Below is the time taken to optimize the ELBO using the VI method. print(vi_end-vi_start) ## 7.659164667129517 Then we can print out the estimated variational parameters (\\(\\boldsymbol{\\omega}\\)), which are the mean and variance of each variational Normal distribution. for name, value in pyro.get_param_store().items(): print(name, pyro.param(name).data.cpu().numpy()) ## AutoNormal.locs.a 9.173145 ## AutoNormal.scales.a 0.07036691 ## AutoNormal.locs.bA -1.847466 ## AutoNormal.scales.bA 0.14070092 ## AutoNormal.locs.bR -0.1903212 ## AutoNormal.scales.bR 0.044044245 ## AutoNormal.locs.bAR 0.35599768 ## AutoNormal.scales.bAR 0.07937442 ## AutoNormal.locs.sigma -2.205863 ## AutoNormal.scales.sigma 0.06052672 Given the variational parameters, we can now sample from the variational distributions of the parameters of interest. The variational distributions can be treated as approximated posterior distributions of these parameters. In particular, we can look at the slopes (log GDP versus ruggedness index) for Non-African nations and African nations. Non-African nations: slope = \\(\\beta_r\\) African nations: slope = \\(\\beta_r+\\beta_{ar}\\) We will take 800 samples from \\(q(\\beta_{r}\\mid \\hat{\\mu}_{\\beta_{r}}, \\hat{\\sigma}^2_{\\beta_{r}})\\) and \\(q(\\beta_{ar}\\mid \\hat{\\mu}_{\\beta_{ar}}, \\hat{\\sigma}^2_{\\beta_{ar}})\\). The histograms of the samples are plotted side-by-side below. with pyro.plate(&quot;samples&quot;, 800, dim=-1): samples = auto_guide(is_cont_africa, ruggedness) gamma_within_africa = samples[&quot;bR&quot;] + samples[&quot;bAR&quot;] gamma_outside_africa = samples[&quot;bR&quot;] fig = plt.figure(figsize=(10, 6)) plt.hist(gamma_within_africa.detach().numpy(), label=&quot;Arican nations&quot;); plt.hist(gamma_outside_africa.detach().numpy(), label=&quot;Non-Arican nations&quot;); fig.suptitle(&quot;Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;); plt.xlabel(&quot;Slope of regression line&quot;) plt.legend() plt.show() Finally, we can compare the VI to MCMC (NUTS, to be specific). from pyro.infer import MCMC, NUTS mcmc = MCMC(NUTS(bayes_model), num_samples=1000, warmup_steps=1000, disable_progbar=True) mc_start = time.time() mcmc.run(is_cont_africa, ruggedness, log_gdp) mc_end = time.time() mc_samples = mcmc.get_samples() mcmc.summary() ## ## mean std median 5.0% 95.0% n_eff r_hat ## a 9.19 0.13 9.19 8.97 9.40 415.75 1.00 ## bA -1.85 0.21 -1.85 -2.21 -1.52 421.86 1.00 ## bAR 0.35 0.13 0.35 0.15 0.56 419.50 1.00 ## bR -0.19 0.07 -0.19 -0.31 -0.07 422.04 1.00 ## sigma 0.95 0.05 0.95 0.87 1.03 489.20 1.00 ## ## Number of divergences: 0 Below is the time taken to draw 2000 MCMC samples (including 1000 warmups). We can see MCMC is almost an order of magnitude slower than the VI. print(mc_end-mc_start) ## 55.938464641571045 We can also plot the histograms of the MCMC samples of the two slopes. Compared to the histograms obtained using the VI, these dont look much different. mc_slope_within_africa = mc_samples[&quot;bR&quot;] + mc_samples[&quot;bAR&quot;] mc_slope_outside_africa = mc_samples[&quot;bR&quot;] fig = plt.figure(figsize=(10, 6)) plt.hist(mc_slope_within_africa.detach().numpy(), label=&quot;Arican nations&quot;); plt.hist(mc_slope_outside_africa.detach().numpy(), label=&quot;Non-Arican nations&quot;); fig.suptitle(&quot;MCMC Estimated Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;); plt.xlabel(&quot;Slope of regression line&quot;) plt.legend() plt.show() 4.4 Takeaways In a Bayesian example, the VI is an order of magnitude faster than MCMC while giving similar results. However, the example uses a very simple Normal-Normal model, and the variational distribution employed is also Normal. This could be why the VI performs so well. When the true posterior is multi-model or asymmetric, it is unlikely that using a mean-field Gaussian variational distribution will give as accurate results. Since the VI is an optimization-based algorithm, it is possible that the ELBO has not converged to a local minimum (in more complex modelling settings). Diagnostics are required for this. The VI is known to underestimate model uncertainty. It is not surprising since it is an approximate inference method. Unless your model is so complicated (e.g., multiple hierarchies, neural nets) that traditional frequentist fitting procedures (lme4, glm) break down or MCMC methods take forever, the VI would not be your first choice. "],["smoothing-techniques.html", "Chapter 5 Smoothing Techniques 5.1 Introduction 5.2 Kernel Smoothing Methods 5.3 Smoothing Spline", " Chapter 5 Smoothing Techniques Jie Jian Smoothing techniques are frequently used to reduce the variability of the observational data and enhance prediction accuracy, while still providing an adequate fit. In this workshop, we will introduce two popular smoothing methods, kernel smoothing and smoothing splines, from their formulations to the computations. Each method has a smoothing parameter that controls the amount of roughness. We will demonstrate how to choose the optimal tuning parameters from the perspective of variance-bias trade-off. Some examples of how to apply the two methods will be provided. Part of the codes and materials come from Hastie et al. (2009) and James et al. (2013). 5.1 Introduction A fundamental problem in statistical learning is to use a data set \\(\\{ (x_i,y_i) \\}_{i=1}^n\\) to learn a function \\(f(\\cdot)\\) such that \\(f(x_i)\\) fit the data \\(y_i\\) well, so we can use the estimated function \\(f\\) on the new data \\(x\\) to predict the future outcome \\(y\\). In a statistical analysis, we have two sources of information, data and models. Data is unbiased but it contains noise, and models involve more constraints so contain less noise but introduce bias. In between the two extremes, we can use smoothing techniques to extract more information from the data and meanwhile control the variance. 5.1.0.1 Example: Boston house value Housing values and other information about Boston suburbs. The variable lstat measures the percentage of individuals with lower socioeconomic status. The variable medv records median house values for 506 neighborhoods around Boston. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select y = Boston$medv x = Boston$lstat y.lab = &quot;Median Property Value&quot; x.lab = &quot;Lower Status (%)&quot; plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;&quot;, bty = &quot;l&quot;) library(splines) library(FNN) xrange &lt;- extendrange(x) Xorder &lt;- order(x) xnew &lt;- seq(min(xrange), max(xrange), length.out = 500) fit1 &lt;- knn.reg(x, y = y, k = 5) fit2 &lt;- lm(y ~ bs(x, degree = 3, df = 4)) ypred2 &lt;- predict(fit2, newdata = data.frame(x = xnew)) plot(x, y, col = &quot;grey80&quot;, pch = 19, cex = 0.5, main = &quot;Fits with different \\&quot;smoothness\\&quot;&quot;, xlab = x.lab, ylab = y.lab) lines(x[Xorder], fit1$pred[Xorder], col = adjustcolor(&quot;steelblue&quot;, 0.5), lwd = 2) lines(xnew, ypred2, col = &quot;darkgreen&quot;, lwd = 2) legend(max(xrange) - 15, max(extendrange(y)), legend = c(&quot;5 nearest neighbours&quot;, &quot;B-spline df=4&quot;), lty = c(1, 1), lwd = 2, col = c(&quot;steelblue&quot;, &quot;darkgreen&quot;)) 5.2 Kernel Smoothing Methods A natural way to achieve smoothness is to utilize the local information of data to compute the fit, so that the the estimated model would not change too much over the data. In the kernel smoothing method, for any data point \\(x_i\\), the value of the function at the point \\(f(x_0)\\) is estimated using the combination of nearby observations. The contribution of each observation \\(x_i\\) is calculated using a weight function, defined as Kernel \\(K_\\lambda(x_0,x_i)\\), related to the distance between \\(x_0\\) and \\(x_i\\). The parameter \\(\\lambda\\) controls the width of the neighborhood. 5.2.1 Local linear regression Locally weighted regression solves a separate weighted least squares problem at each target point \\(x_0\\): \\[\\min\\limits_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda (x_0,x_i) [y_i - \\alpha(x_0)-\\beta(x_0)x_i]^2.\\] The estimated function at each target point \\(x_0\\) is \\(\\hat{f}(x_0)=\\hat{\\alpha} (x_0)+\\hat{\\beta}(x_0) x_i\\). We can only use it to fit at a single point. The smoothing parameter \\(\\lambda\\) in the kernel function, which determines the width of the local neighborhood, has to be determined. Large \\(\\lambda\\) implies lower variance (averages over more observations) but higher bias (we essentially assume the true function is constant within the window). 5.2.1.1 Computation For each target point \\(x_0\\), \\[\\begin{bmatrix} \\hat{\\alpha} [x_0] \\\\ \\hat{\\beta} [x_0] \\end{bmatrix}=\\arg\\min\\limits_{\\alpha,\\beta} \\sum_{i=1}^N K_\\lambda (x_0,x_i)\\cdot (y_i-\\alpha-(x_i-x_0) \\beta)^2,\\] if we define an \\(n-\\)by\\(-n\\) weighting matrix \\[W_h(x_0) = diag(K_\\lambda(x_0,x_1),\\cdots,K_\\lambda(x_0,x_n)) ,\\] then we can rewrite this optimization as \\[\\begin{bmatrix} \\hat{\\alpha} [x_0] \\\\ \\hat{\\beta} [x_0] \\end{bmatrix}=\\arg\\min\\limits_{\\alpha,\\beta} \\| W_h(x_0) \\cdot (Y- [\\mathbf{1}_n\\ \\ X_0] \\begin{bmatrix}\\alpha \\\\ \\beta \\end{bmatrix})^2\\|^2_2,\\] which is a OLS optimization \\[\\begin{bmatrix} \\hat{\\alpha}[x_0] \\\\ \\hat{\\beta} [x_0]\\end{bmatrix}=([\\mathbf{1}_n\\ \\ X_0]&#39; W_h(x_0) [\\mathbf{1}_n\\ \\ X_0])^{-1}([\\mathbf{1}_n\\ \\ X_0]&#39;W_h(x_0) Y).\\] 5.2.1.2 One-line implementation in R: loess loess function in R fits a LOcally wEighted Sum of Squares estimate. The local neighborhood determined by either span: the portion of points in the local neighborhood enp.target: effective degrees of freedom The kernel is Tukeys tri-cube. Degree can be 0, 1, or 2. The default is a local quadratic. Up to 4 predictors. fit1 = loess(y ~ x, span = 0.05) fit2 = loess(y ~ x, span = 0.3) plot(x, y, col = &quot;grey80&quot;, pch = 19, cex = 0.5, main = &quot;loess with different spans&quot;, xlab = x.lab, ylab = y.lab) lines(x[Xorder], predict(fit1, x[Xorder]), col = adjustcolor(&quot;steelblue&quot;, 0.5), lwd = 2) lines(x[Xorder], predict(fit2, x[Xorder]), col = &quot;darkgreen&quot;, lwd = 2) legend(max(xrange) - 15, max(extendrange(y)), legend = c(&quot;span=0.05&quot;, &quot;span=0.3&quot;), lty = c(1, 1), lwd = 2, col = c(&quot;steelblue&quot;, &quot;darkgreen&quot;)) 5.2.1.3 Bias and vairance \\(\\text{bias}(\\hat{\\alpha} [x_0])=O(h^2)\\) and \\(var (\\hat{\\alpha}[x_0])=O_p (\\frac{1}{nh^d}),\\) where \\(h\\) is the bandwidth and \\(d\\) is the dimensionality of \\(x\\). Theoretically, we can pick the bandwidth \\(h\\) in some optimal sense such that \\(h=\\arg\\max\\limits_{h} (c_1 h^4 + c_2 \\frac{1}{nh^2})\\). Therefore, the asymptotic rate for the bandwidth \\(h\\) is \\(h=O(n^{-1/(d+4)})\\). [derivation skipped] 5.2.2 Tuning Parameter (bandwidth) Selection The tuning parameter \\(\\lambda\\) of the kernel \\(K_\\lambda\\) controls the width of the averaging window. - if the window is narrow, \\(\\hat{f}(x_0)\\) is an average of a small number of \\(y_i\\) close to \\(x_0\\), and its variance will be relatively large while the bias will tend to be small. - if the window is wide, the variance of \\(\\hat{f}(x_0)\\) will be small and the bias will be higher. Choosing the bandwidth is a bias-variance trade-off. Leave-one-out cross validation (LOOCV) For each \\(i=1,\\cdots,n\\), compute the estimator \\(\\hat{f}_\\lambda^{(-i)} (x)\\), where \\(\\hat{f}_\\lambda^{(-i)} (x)\\) is comupted without using observation \\(i\\). The estimated MSE is given by \\[\\hat{\\text{MSE}}(\\lambda)=\\frac{1}{n} \\sum_i (y_i-\\hat{f}_\\lambda^{(-i)} (x_i))^2.\\] [derivation needed] 5.2.3 Extension and example: Local logistic regression We can extend the local kernel smoothing method to \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) l(y_i,x_i^T \\beta(x_0)),\\] where \\(l(y_i,x_i^T \\beta(x_0))\\) is replaced by the specific log-likelihood. For example, in the local logistic regression where we consider logistic regression with a single quantitative input \\(X\\). The local log-odds at a target \\(x_0\\) is \\[\\log \\frac{\\mathbb{P}(Y=1|X=x_0)}{\\mathbb{P}(Y=0|X=x_0)}=\\alpha(x_0)+\\beta(x_0)x_i.\\] The objective function that we need to maximize is adjusted as \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) [y_i \\log p(x_i) + (1-y_i) \\log (1-p(x_i))].\\] 5.3 Smoothing Spline When fitting a function \\(f(\\cdot)\\) to a set of data, the first thing that we want is to find some function that fits the observed data well: that is, we want the residual sum of squares which measures the goodness of fit, \\(RSS=\\sum_{i=1}^{n} (y_i - f(x_i))^2\\), to be small. If this is our only repuirement, then we can even make RSS to zero by simply interpolating all observational data (a non-linear function that passes through exactly every data point). Such flexible function would overfit the data and lead to a large variance. Therefore, we not only require the RSS to be small, but also the function to be smooth. To achieve the smoothness of the estimated function, some penalty can be added to control the variability in \\(f(x)\\). Considering the two requirements, we are seeking a function \\(f(x)\\) that minimizes \\[\\underbrace{\\sum_{i=1}^{n} (y_i-f(x_i))^2}_\\text{Model fit} + \\lambda \\underbrace{\\int f&#39;&#39;(x)^2 dx}_\\text{Penalty term}\\] where \\(\\lambda\\) is a nonnegative tuning parameter. The function \\(f(x)\\) is known as a smoothing spline. Model fit: different from the general linear regression problem where the function \\(f(x)\\) is linear and we only need to estimate the coefficients of the predictors, here we minimize the objective function with respect to \\(f(x)\\). Penalty term: the second term penalizes curvature in the function. Tuning parameter: when \\(\\lambda =0\\) we get a wiggly non-linear function, which has a high variance and low bias; As \\(\\lambda\\) increases the estimated function will be smoother; When \\(\\lambda \\rightarrow \\infty\\), \\(f&#39;&#39;\\) will be zero everywhere which leads to a linear model \\(f(x)=\\beta_0+\\beta_1 x\\), which has a low variance but high bias. 5.3.1 Computation From the perspective of functional space Gu (2013), the smoothing spline is the function \\(f_\\lambda\\) that minimizes the spline functional \\[E[f]=\\sum_{i=1}^{n} (y_i-f(x_i))^2 + \\lambda \\int f&#39;&#39;(x)^2 dx\\] in the Sobolev space \\(W_2 (\\mathbb{R})\\) of functions with square integrable second derivative: \\[f_\\lambda = \\arg \\min_{f\\in W_2 (\\mathbb{R})} E[f].\\] Sobolev space is an infinite-dimensional function space, where the second term is defined. It can be shown that the solution to \\(\\min E[f]\\) is an explicit, finite-dimensional, unique minimizer which is a natural cubic spline with knots at the unique values of the predictor values \\(x_i\\). Since the solution is a natural spline, we can write it as \\(f(x)=\\sum_{i=1}^n N_j (x) \\theta_j\\), where the \\(N_j (x)\\) are an N-dimensional set of basis functions for representing the family of natural splines. The criterion reduces to \\[RSS(\\theta,\\lambda)=(y-N\\theta)^\\top (y-N\\theta) + \\lambda \\theta^\\top \\Omega_N \\theta,\\] where \\(\\{ N \\}_{ij}=N_j (x_i)\\) and \\(\\{ \\Omega_N \\}_{jk}=\\int N&#39;&#39;_j (t) N&#39;&#39;_k (t) dt\\). The solution is easily seen to be \\[\\hat{\\theta} = (N^\\top N + \\lambda \\Omega_N)^{-1} N^T y.\\] The fitted smoothing spline is given by \\[\\hat{f} (x) = \\sum_{j=1}^N N_j (x) \\hat{\\theta}_j.\\] Although natural splines provide a basis for smoothing splines, it is computationally more convenient to operate in the larger space of unconstrained B-splines. We write \\(f(x)=\\sum_{i=1}^{N+4}B_i(x)\\theta_i\\) (just to replace the basis functions from natural cubic splines to B-splines). 5.3.2 Tuning Parameter Selection The tuning parameter controls the tradeoff between smoothness of the estimate and fidelity to the data. In statistical terms, it controls the tradeoff between bias and variance. The optimal value of \\(\\lambda\\) has to be estimated from the data, usually by cross-validation or generalized cross-validation. Cross validation Generalized cross-validation Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline. Although a smoothing spline has \\(n\\) parameters and hence \\(n\\) nominal degrees of freedom, these \\(n\\) parameters are heavily constrained or shrunk down. We use the effective degrees of freedom as a measure of the flexibility of the smoothing spline. We can write \\[\\hat{f}_\\lambda = S_\\lambda y,\\] where \\(\\hat{f}\\) is the solution for a given \\(\\lambda\\) and \\(S_\\lambda\\) is an \\(n-\\)by\\(-n\\) matrix. The effective degrees of freedom is defined as the sum of the diagonal elements of the matrix \\(S_\\lambda\\): \\[df_\\lambda = \\sum_{i=1}^n \\{S_\\lambda \\}_{ii}.\\] The generalized cross validation (GCV) for such linear smoother is \\[GCV(\\hat{f})=\\frac{1}{n} \\sum_{i=1}^n (\\frac{y_i - \\hat{f}(x_i)}{1-tr (S)/n})^2.\\] [derivation needed] library(splines) smooth1 = smooth.spline(x, y, df = 3) smooth2 = smooth.spline(x, y, cv = TRUE) par(mfrow = c(1, 2)) plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (3 df)&quot;, bty = &quot;l&quot;) lines(smooth1, lwd = 2, col = &quot;brown&quot;) plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (CV)&quot;, bty = &quot;l&quot;) lines(smooth2, lwd = 2, col = &quot;darkorange&quot;) 5.3.3 Extension and example: Nonparametric logistic regression The smoothing spline problem we talked above is posed in a regression setting, as the model fit part is \\(\\sum_{i=1}^{n} (y_i-f(x_i))^2\\). The technique can be extended to other problems, as long as we adjust the model fit term in the objective function to some likelihood-based formula. Here, we consider logistic regression with a single quantitative input \\(X\\). \\[\\log \\frac{\\mathbb{P}(Y=1|X=x)}{\\mathbb{P}(Y=0|X=x)}=f(x).\\] Our target is to fit a smooth function \\(f(x)\\) to the logit, so that the conditional probability \\(\\mathbb{P}(Y=1|x)=\\frac{e^{f(x)}}{1+e^{f(x)}}\\) is also smooth, which can be used for classification or risk measurement. In order to incorporate the smoothness penalty, the penalized log-likelihood criterion is \\[l(f;\\lambda)=\\sum_{i=1}^{N} [y_i \\log p(x_i) + (1-y_i) \\log (1-p(x_i))]-\\frac{1}{2} \\lambda \\int \\{f&#39;&#39;(t) \\}^2 dt.\\] It also can be shown that the solution to the optimization problem is the natural spline, so we can also express the function as \\(f(x)=\\sum_{i=1}^N N_j (x) \\theta_j\\) where \\(N\\) is the matrix of natural spline, or further it can be replaced by the B-spline for the same reason we have in the regression setting (approximation). 5.3.4 Take-home note Smoothing techniques can incorporate with a variety of statistical methods. We introduced two methods: Local linear regression: fit locally using some kernel function to measure the weight of each data point. \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) l(y_i,x_i^T \\beta(x_0))\\] Smoothing spline: add a penalty term in the objective function to prevent the function that needs to be estimated from not being smooth. \\[\\max \\sum_{i=1}^N l(x_i,y_i) -\\lambda P(f)\\] "],["analyzing-surveys-with-r.html", "Chapter 6 Analyzing Surveys with R 6.1 Acknowlegdements 6.2 Introduction to Thinking About Surveys 6.3 Probability Survey without Non-response (design-based) 6.4 Post-adjustment (model-assisted) 6.5 Non-probability Surveys (model-based)", " Chapter 6 Analyzing Surveys with R Gradon Nicholls 6.1 Acknowlegdements Much of this workshop was informed by the book Practical tools for designing and weighting survey samples (Valliant, Dever, and Kreuter 2013), which is the survey practitioners bible. The standard reference on calibration: (Deville, Srndal, and Sautory 1993) Ive also made use of course notes from STAT 332 offered by UWaterloo. There is also this webpage on specifying survey designs in R. 6.2 Introduction to Thinking About Surveys 6.2.1 Total Survey Error (TSE) The Total Survey Error (TSE) Framework (taken from Groves and Lyberg 2010) TSE is divided into two parts: Errors of Measurement: Does the question wording capture the underlying concept? Does the question wording affect how it is answered? Are there psychological processes that cause respondents to give biased answers? Errors of Representation: Do our survey respondents look like the population we are interested in? For this, we focus on errors of representation, which involve the following groups: Target Population: The population which we would like to infer something about. Sampling Frame: A list of addresses/phone numbers/etc. from which we will draw our sample. Sample: The subset of the sampling frame which we will attempt to recruit to our survey, Respondents: Those in the sample who agree to complete our survey. TSE allows us to consider total error in our survey estimates as a sequential series of errors. Much of survey research is devoted to zeroing on each type of error to study its causes and potential solutions. The types of errors of representation are: Coverage Error: Does our frame cover the entire target population? Sampling Error: Error caused by how we draw the sample. In a probability survey, this is generally the only portion we have full control over. Nonresponse Error: Do those who agree to our survey differ from tose who dont respond? 6.2.2 Probability vs. Non-Probability Surveys A probability survey is one where everyone in the sampling frame has a known and non-zero probability of being included in the sample. For example, we have a list of all addresses in Canada, and we take a random sample of addresses for our study. A non-probability survey is one where such an inclusion probability is not known. For example, we take out ads on websites recruiting participants to join our opt-in panel. In this case we have no way of knowing how likely it is that each member of the target population makes it into our panel. 6.2.3 Design-based vs. Model-based Inference Design-based: rely fully on the sampling design Model-assisted: sampling design is the basis (e.g. probability survey with model for non-response) Model-based: rely fully on a model (e.g. calibration weights in a non-probability survey) 6.3 Probability Survey without Non-response (design-based) 6.3.1 Some notation \\(i\\): a member of the population \\(U\\): the set of all members of the population \\(S\\): our randomly drawn sample \\(I_i = I(i \\in S)\\): inclusion indicator \\(\\pi_i = \\Pr(i \\in S)\\): inclusion probability \\(\\pi_{ij} = \\Pr(i,j \\in S)\\): joint inclusion probability \\(n\\): sample size \\(N\\): population size 6.3.2 Horvitz-Thompson Estimator Lets say we are interested in estimating \\(\\mu_U\\), the population total. I.e. \\(\\mu_U = \\sum\\limits_{i \\in U} y_i\\). Taking a census would be too expensive, so instead we want to use a weighted average from our sample data: \\(\\hat \\mu = (1/N)\\sum\\limits_{i \\in S} w_i y_i\\). Question: how do we select \\(w_i\\) such that our estimate is unbiased? \\[ \\begin{aligned} &amp;\\text{we desire that }E[\\hat \\mu] = \\mu_U \\\\ \\implies&amp; E\\left[\\frac{1}{N}\\sum_{i \\in S} w_i y_i\\right] = \\mu_U \\\\ \\implies&amp; E\\left[\\frac{1}{N}\\sum_{i \\in U} I_i w_i y_i\\right] = \\mu_U \\\\ \\implies&amp; \\frac{1}{N}\\sum_{i \\in U} E[I_i] w_i y_i = \\mu_U \\\\ \\implies&amp; \\frac{1}{N}\\sum_{i \\in U} \\pi_i w_i y_i = \\mu_U \\\\ \\implies&amp; \\frac{1}{N}\\sum_{i \\in U} \\pi_i w_i y_i = \\frac{1}{N}\\sum_{i \\in U} y_i \\\\ \\implies&amp; \\text{we should set } w_i = 1/\\pi_i \\end{aligned} \\] By setting \\(w_i = 1/\\pi_i\\), we have derived the Horvitz-Thompson estimator. In words, HT tells us to weight each sample observation by the inverse of their inclusion probability. To get the HT estimator of the mean, we simply divide by \\(N\\). I.e. \\(\\hat \\mu = \\frac{1}{N}\\sum_{i \\in S} (1/\\pi_i) y_i\\). 6.3.3 Variance Estimation Most generally, the variance of the HT estimator for the total is, \\[ \\text{Var}(\\hat t) = \\sum_{i \\in U} \\pi_i(1-\\pi_i) \\left(\\frac{y_i}{\\pi_i}\\right)^2 + \\sum_{i,j \\in U; i \\neq j} (\\pi_{ij} - \\pi_i\\pi_j)\\frac{y_i}{\\pi_i}\\frac{y_j}{\\pi_j} \\] which we can in principle estimate using our sample data. Notice that it relies on joint inclusion probailities \\(\\pi_{ij}\\)! 6.3.4 Simple Random Sampling Without Replacement (SRSwor) We select \\(n\\) individuals from a population of \\(N\\) completely at random. That is, \\(\\pi_i = \\frac{n}{N}\\) and \\(\\pi_{ij} = \\frac{n}{N}\\frac{n-1}{N-1}\\). We have that \\(\\hat \\mu = (1/N)\\sum_{i \\in S} \\frac{N}{n} y_i = (1/n) \\sum_{i \\in S} y_i\\) and \\(\\text{Var}(\\hat \\mu) = (1 - \\frac{n}{N})\\frac{\\sigma^2}{n}\\). We call \\(f = \\frac{n}{N}\\) the sampling fraction and \\(1-f\\) the finite population correction factor (fpc). When \\(n \\to \\infty\\) and \\(N-n \\to \\infty\\), we have that \\(\\hat \\mu\\) is approximately normally distributed. Lets implement this in R using our own formulas: # package associated with Valliant, Devant &amp; Kreuter, includes data pacman::p_load(&quot;PracTools&quot;) # bring data into our environment data(nhis.large) nhis.large.clean = nhis.large %&gt;% filter(!is.na(educ)) %&gt;% mutate(hasdegree = as.numeric(educ&gt;2)) N = nrow(nhis.large.clean) # draw a SRSwor of size n=300 set.seed(310732) n=300; f = n/N S = sample(N,size=n,replace=FALSE) nhis.sample = nhis.large.clean[S,] # population mean mu_U = mean(nhis.large.clean$hasdegree) # 95% CI for the mean using survey methods mu_hat = mean(nhis.sample$hasdegree) SE_mu = sqrt(1 - f)*sd(nhis.sample$hasdegree)/sqrt(n) c(mu_hat - 1.96*SE_mu, mu_hat + 1.96*SE_mu) ## [1] 0.1703366 0.2629967 # 95% CI using 231 methods mu_hat = mean(nhis.sample$hasdegree) SE_mu = sd(nhis.sample$hasdegree)/sqrt(n) c(mu_hat - 1.96*SE_mu, mu_hat + 1.96*SE_mu) # pretty much identical! --&gt; why? ## [1] 0.1699696 0.2633638 For more complicated designs, it will be useful to learn how to use the survey package in R: # survey package pacman::p_load(&quot;survey&quot;) # append sampling probabilities to the data nhis.sample = nhis.sample %&gt;% mutate(pr_incl = n/N) # append population size to the data nhis.sample = nhis.sample %&gt;% mutate(N = N) # specify the survey design SRSdesign = survey::svydesign( ids=~1, # simple one-stage design strata=NULL, # no strata probs=~pr_incl, # prob = n/N data=nhis.sample, # sample data fpc=~N) # population size # estimate the mean, 95% confidence interval muhat = svymean(~hasdegree,design=SRSdesign) confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.1703375 0.2629959 # you could use a t distn if you want to, but it has no theoretical backing! confint(muhat,df=199) ## 2.5 % 97.5 % ## hasdegree 0.170054 0.2632793 Lets visualise the distribution of our estimator for different choices of \\(n\\). # set seed set.seed(310732) # generate plots in 2 by 2 array par(mfrow=c(2,2)) # n = 5 n = 5 estimates = rep(NA,1000) for (i in 1:1000) { S = sample(N,size=n,replace=FALSE) estimates[i] = mean(nhis.large.clean[S,]$hasdegree) } hist(estimates,freq=FALSE,main=&quot;SRSwor, n=5, N-n=19154&quot;) lines(density(estimates)) # n = 200 n = 200 estimates = rep(NA,1000) for (i in 1:1000) { S = sample(N,size=n,replace=FALSE) estimates[i] = mean(nhis.large.clean[S,]$hasdegree) } hist(estimates,freq=FALSE,main=&quot;SRSwor, n=200, N-n=18959&quot;) lines(density(estimates)) # n = 500 n = 500 estimates = rep(NA,1000) for (i in 1:1000) { S = sample(N,size=n,replace=FALSE) estimates[i] = mean(nhis.large.clean[S,]$hasdegree) } hist(estimates,freq=FALSE,main=&quot;SRSwor, n=500, N-n=18659&quot;) lines(density(estimates)) # n = 19000 n = 19000 estimates = rep(NA,1000) for (i in 1:1000) { S = sample(N,size=n,replace=FALSE) estimates[i] = mean(nhis.large.clean[S,]$hasdegree) } hist(estimates,freq=FALSE,main=&quot;SRSwor, n=19100, N-n=59&quot;) lines(density(estimates)) 6.3.5 Stratified Simple Random Sampling Without Replacement (SSRSwor) Imagine we have \\(H\\) strata (e.g. provinces) indexed by \\(h\\). In each stratum, there is a population \\(N_h\\), such that \\(N = \\sum_{h=1}^H N_h\\). We conduct a SRSwor of size \\(n_h\\) in each stratum. Then, the inclusion probability for an individual in stratum \\(h\\) is \\(\\pi_i^h = \\frac{n_h}{N_h}\\). The joint inclusion probability for two individuals in stratum \\(h\\) is \\(\\pi_{ij}^h = \\frac{n_h}{N_h}\\frac{n_h-1}{N_h-1}\\). The joint inclusion probability for two individuals in different strata is \\(\\pi_{ij}^{hh&#39;} = \\frac{n_h}{N_h} \\frac{n_{h&#39;}}{N_{h&#39;}}\\). We have that \\(\\hat \\mu = \\sum_{h=1}^H \\frac{N_h}{N} \\hat \\mu_h\\) where \\(\\hat \\mu_h\\) is the SRSwor estimator within each stratum. Furthermore, \\(\\text{Var}(\\hat \\mu) = \\sum\\limits_{h=1}^H \\left(\\frac{N_h}{N}\\right)^2 (1-f_h) \\frac{\\sigma^2_h}{n_h}\\). In R, lets simulate a stratified simple random sample: # we want to stratify by race N_h = table(nhis.large.clean$race) # want to obtain estimates by subgroup: oversample the smaller groups n_h = c(&quot;1&quot;=100,&quot;2&quot;=100,&quot;3&quot;=100) # sampling fractions by stratum f = n_h / N_h # draw a SSRSwor set.seed(87876) S = survey::stratsample( strata = nhis.large.clean$race, counts = n_h) nhis.sample = nhis.large.clean[S,] Now we estimate: # append sampling probabilities to the data nhis.sample = nhis.sample %&gt;% mutate(pr_incl = f[race]) # append population size to the data nhis.sample = nhis.sample %&gt;% mutate(N = N_h[race]) # specify the survey design SSRSdesign = survey::svydesign( ids=~1, # simple one-stage design strata=~race, # stratify by race probs=~pr_incl, # prob = n/N data=nhis.sample, # sample data fpc=~N) # population size # estimate the mean, 95% confidence interval muhat = svymean(~hasdegree,design=SSRSdesign); muhat ## mean SE ## hasdegree 0.14919 0.0293 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09170761 0.2066743 # mean by race svyby(~hasdegree,by=~race,FUN=svymean, design=SSRSdesign) ## race hasdegree se ## 1 1 0.15 0.03577108 ## 2 2 0.10 0.02961191 ## 3 3 0.30 0.04321784 # compare with unstratified design svyby(~hasdegree,by=~race,FUN=svymean, design=SRSdesign) ## race hasdegree se ## 1 1 0.2280702 0.02761555 ## 2 2 0.1071429 0.04107490 ## 3 3 0.4375000 0.12325037 6.4 Post-adjustment (model-assisted) Lets say weve conducted our survey and have our estimate \\(\\hat \\mu = (1/N) \\sum_{i \\in S} (1/\\pi_i) y_i\\). Now we ask, Can we use information we know about the population to improve our estimate? In other words, we want to consider a post-adjustment to the inverse probability weights so that \\(w_i = (1/\\pi_i) \\theta_i\\). To do this we want to incorporate auxiliary information \\(X_i\\) that we know about each individual \\(i\\), for which we also have information about the population. 6.4.1 Post-stratification For example, consider the HT estimator for stratification: \\[ \\hat \\mu = \\sum_{h=1}^H \\frac{N_h}{N} \\hat \\mu_h \\] Even if we did not stratify in our sampling design, it makes intuitive sense to use this type of estimator after the fact. For example, perhaps we were not able to stratify based on age group, but we asked individuals age on the survey. Using information from the census, we can thus post-stratify (Valliant 1993) our sample based on age group. Lets call this estimator \\(\\hat \\mu_{\\text{PS}}\\) \\[ \\hat \\mu_{\\text{PS}} = \\sum_{h=1}^H \\frac{N_h}{N} \\hat \\mu_{h,\\text{PS}} \\] where \\(\\hat \\mu_{h,\\text{PS}} = \\frac{1}{N_h} \\sum\\limits_{i \\in S} I(i \\in U_h)\\dfrac{N_h}{\\widetilde{n_h}} y_i\\). There is one key difference: in SSRSwor, the researcher chose the sample size \\(n_h\\) within each stratum. Here, the researcher does NOT know the sample size until the sample is taken. In other words, \\(\\widetilde{n_h}\\) is a random variable! Thus, \\(\\hat \\mu_{h,\\text{PS}}\\) is not the nice linear estimator we had before. We no longer have a simple closed formula for the variance of the estimator. Now we must take a different strategy: taylor approximation: equivalent to using the variance formula for an SSRSwor. This would tend to slightly underestimate the variance, because we do not take into account variability from \\(\\widetilde{n_h}\\). resampling methods, e.g. bootstrap Lets take our SRS sample and post-stratify it based on race: # collect population counts in format needed for postStratify popcounts = data.frame(race=c(1,2,3), Freq=as.numeric(table(nhis.large.clean$race))) PSdesign = survey::postStratify( design=SRSdesign, # the initial design strata=~race, # the variable we post-stratify on population=popcounts) # population counts Compare SRS with post-stratified SRS: We see a small adjustment in the overall estimate and standard error: # estimate of overall mean muhat = svymean(~hasdegree,design=SRSdesign); muhat ## mean SE ## hasdegree 0.21667 0.0236 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.1703375 0.2629959 muhat = svymean(~hasdegree,design=PSdesign); muhat ## mean SE ## hasdegree 0.21941 0.0238 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.1728354 0.2659927 Compare SSRS with post-stratified SRS estimates by race: We see that post-stratification is really only a tool to get better overall estimates. Post-stratification still doesnt let us get better estimates within-stratum. # estimate by race svyby(~hasdegree,by=~race,FUN=svymean, design=SSRSdesign) ## race hasdegree se ## 1 1 0.15 0.03577108 ## 2 2 0.10 0.02961191 ## 3 3 0.30 0.04321784 svyby(~hasdegree,by=~race,FUN=svymean, design=PSdesign) ## race hasdegree se ## 1 1 0.2280702 0.02761555 ## 2 2 0.1071429 0.04107490 ## 3 3 0.4375000 0.12325037 6.4.2 Calibration: Dealing with multiple auxilliary variables Lets say we want to post-adjust our estimate on race, but also on gender and age group? In order to use post-stratification, we need to know the entire 3-way table of population counts. For example, we need to know how many white men aged 55+ there are in the population. We can use other calibration techniques (which post-stratification can be considered a general case of) like raking that rely only one one-way tabulations. # define population totals for each raking variable pop.sex = data.frame(sex=c(1,2), Freq=as.numeric(table(nhis.large.clean$sex))) pop.age.grp = data.frame(age.grp=c(1,2,3,4,5), Freq=as.numeric(table(nhis.large.clean$age.grp))) rakedesign = survey::rake( design=SSRSdesign, sample.margins=list(~sex,~age.grp), population.margins=list(pop.sex,pop.age.grp) ) muhat = svymean(~hasdegree,design=SSRSdesign); muhat ## mean SE ## hasdegree 0.14919 0.0293 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09170761 0.2066743 muhat = svymean(~hasdegree,design=rakedesign); muhat ## mean SE ## hasdegree 0.15249 0.0298 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09410781 0.2108778 6.4.3 What makes a good calibration variable? E.g. see (Lundy and Rao 2022). 6.4.4 Variance Estimation: Resampling Lets see how the bootstrap version of post-stratification: # start with bootsrapped SRS set.seed(1902) bootstrapSRS = survey::as.svrepdesign( design=SRSdesign, type=&quot;bootstrap&quot;, replicates=5000) # apply post-stratification to every replicate weight bootstrapPS = survey::postStratify( design=bootstrapSRS, strata=~race, population=popcounts) muhat = svymean(~hasdegree,design=PSdesign); muhat ## mean SE ## hasdegree 0.21941 0.0238 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.1728354 0.2659927 muhat = svymean(~hasdegree,design=bootstrapPS); muhat ## mean SE ## hasdegree 0.21941 0.0242 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.1719077 0.2669204 How about the raking estimate: # start with bootstrap replicated SSRS set.seed(76573) bootstrapSSRS = survey::as.svrepdesign( design=SSRSdesign, type=&quot;bootstrap&quot;, replicates=5000) # rake each bootstrap replicate bootrakedesign = survey::rake( design=bootstrapSSRS, sample.margins=list(~sex,~age.grp), population.margins=list(pop.sex,pop.age.grp) ) muhat = svymean(~hasdegree,design=SSRSdesign); muhat ## mean SE ## hasdegree 0.14919 0.0293 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09170761 0.2066743 muhat = svymean(~hasdegree,design=rakedesign); muhat ## mean SE ## hasdegree 0.15249 0.0298 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09410781 0.2108778 muhat = svymean(~hasdegree,design=bootrakedesign); muhat ## mean SE ## hasdegree 0.15249 0.0299 confint(muhat) ## 2.5 % 97.5 % ## hasdegree 0.09383431 0.2111513 We can ask the svy commands to produce all replicates of the estimate. Doing this we can get confidence intervals based on the actual distribution of the estimates. allmus = svymean(~hasdegree,design=bootrakedesign,return.replicates=TRUE) hist(allmus$replicates) quantile(allmus$replicates,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 0.09744063 0.21383397 6.5 Non-probability Surveys (model-based) Previously we started with a survey design (design-based approach), and then possibly made adjustments to the design weights in order to improve our estimates. In a non-probability survey context (or probability survey with non-response), we no longer have a proper design. That is, we no longer have full information about the inclusion probabilities \\(\\pi_i\\). A review of inference in non-probability samples can be found in (Elliott and Valliant 2017). To summarise, we can take the same thought process as before. Namely, we have our mean estimate \\(\\hat \\mu = \\sum_{i \\in S} \\frac{1}{\\gamma_i}y_i\\) where \\(\\gamma_i\\) is our pseudo-inclusion probability. That is, we think of it as \\(\\gamma_i = \\Pr(i \\text{ joins our survey})\\). However, we no longer have any degree of control over this. For instance, \\(i\\) joining our survey could include the following steps: \\(i\\) has access to the internet \\(i\\) volunteers for our opt-in survey panel conditional on having access to the internet \\(i\\) is sampled from the panel given they belong to the panel \\(i\\) responds to our survey given we sampled them from the panel We must assume that each step has some non-zero probability of occurring. Furthermore, we must assume that these probabilities rely only on some variables \\(X\\) which we observe in both the sample and the population. That is we estimate using \\(\\hat \\mu (X) = \\sum_{i \\in S} w_i(x_i; \\beta) y_i\\) where \\(w_i(x_i; \\beta)\\) is our psuedo-weight which can be computed using calibration methods discussed above. This has direct parallels with causal analysis: if we dont have a properly randomized experiment, then we attempt to make the treatment and control groups as similar as possible (using inverse probabliity weighting, propensity scores, matching, etc.) such that any difference that remains must be due to the treatment. However, this relies on the assumption that we have selection only on the observables \\(X\\)! "],["introduction-to-causal-inference.html", "Chapter 7 Introduction to Causal Inference 7.1 Basic Concepts 7.2 Commonly Used Methods", " Chapter 7 Introduction to Causal Inference Jingyue Huang In this workshop, we will introduce the basic concepts and framework in causal inference, followed by some commonly used methods, including matching, stratification, inverse probability weighting, and doubly robust estimation. Some materials can be found in the books Causal Inference in Statistics, Social, and Biomedical Science (Imbens and Rubin 2015) and Causal Inference: What If (Hernn and Robins 2020). I also made use of course slides from STAT 931 offered by Professor Yeying Zhu when I was taking this course. 7.1 Basic Concepts For causal inference, we are interested in causation. There are two notions of causation. One is the causes of an outcome, such as what causes lung cancer? The other one is the effect of a cause. For example, we may ask: Does smoking cause lung cancer? and How strong is the effect? In this workshop, we focus on the effect of a cause. Note: Correlation/association does not imply causation. Example: Weight and hight are associated with each other. But more weight will not cause someone higher. 7.1.1 Notation Suppose we have data on subjects \\(i=1,\\dots, n\\) \\(X_i=(X_{i1}, \\dots, X_{ip})^\\intercal\\): covariates/potential confounders \\(T_i\\): treatment assignment; \\(T_i=1\\) if treated and \\(T_i=0\\) if untreated (control) \\(Y_i\\): observed outcome Potential outcomes \\(Y_{1i}\\): potential outcome if treated \\(Y_{0i}\\): potential outcome if untreated Note that \\(Y_i=T_iY_{1i}+(1-T_i)Y_{0i}\\). 7.1.2 Causal Effect The individual-level causal effect for subject \\(i\\): \\(Y_{1i}-Y_{0i}\\) Average treatment Effect (ATE): \\(\\theta=E(Y_{1i}-Y_{0i})=E(Y_{1i})-E(Y_{0i})\\) Confounders: Covariates which are associated with the treatment assignment and potential outcomes simultaneously. Example: Gender (\\(X\\)), Smoking (\\(T\\)), Life expectancy (\\(Y\\)) Association: \\(E(Y_{1i}\\mid T_i=1)-E(Y_{0i}\\mid T_i=0)\\) 7.1.3 Assumptions Strongly Ignorable Treatment Assignment (SITA). The treatment indicator (\\(T\\)) and the response variables (\\(Y_{1}\\), \\(Y_{0}\\)) are independent given the set of covariates (\\(X\\)); \\((Y_{1}, Y_{0}) \\perp T\\mid X\\). Stable Unit Treatment Value Assumption (SUTVA). Each subjects potential outcomes are not influenced by the actual treatment status of other subjects; \\((Y_{1i}, Y_{0i}) \\perp T_j\\) for \\(i\\neq j\\). Positvity. \\(0&lt;P(T=1\\mid X=\\boldsymbol{x})&lt;1\\) for any possible value \\(\\boldsymbol{x}\\). 7.1.4 Propensity Score The propensity score for the treatment assignment is defined as the conditional probability of choosing treatment given the covariates and response variables, that is, \\(\\tau=P(T=1\\mid Y_{1}, Y_{0}, X)\\). Under the SITA assumption, it is true that the propensity score \\(\\tau=P(T=1\\mid X=x)\\), which is a function of \\(x\\). Properties: Propensity score is a balancing score; \\(X \\perp T\\mid \\tau\\). If the treatment is strongly ignorable given \\(X\\), i.e., \\((Y_{1}, Y_{0}) \\perp T\\mid X\\), then it is strongly ignorable given \\(\\tau\\), i.e., \\((Y_{1}, Y_{0}) \\perp T\\mid \\tau\\). Propensity scores are unknown and need to be estimated. Indeed, we model \\(T\\) (assuming binary) as a function of \\(X\\). Parametric (logistic or probit regression) or nonparametric (random forest, generalized boosted model, etc.) methods can be applied. 7.2 Commonly Used Methods The general framework for the propensity score based methods involves two steps: Get the estimated propensity scores \\(\\hat{\\tau}_i\\) for \\(i=1, \\dots, n\\) based on the available data \\((T_i, X_i), i=1, \\dots, n\\); Using \\(\\hat{\\tau}_i\\) to adjust the original sample and estimate the average treatment effect. 7.2.1 Matching Basic idea: \\((Y_{1}, Y_{0}) \\perp T\\mid X\\). For each subject in the treated group, if we can find an untreated subject with the same (or similar) covariates (\\(X\\)), they can form a matched dataset where the property \\((Y_{1}, Y_{0}) \\perp T\\) holds. This means we can estimate the average treatment effect as in a randomized study. Problem: If the size of \\(X\\) is moderate or high-dimensional, it is hard to get a matched dataset (curse of dimensionality). Solution: Use \\(\\tau(X)\\) instead of \\(X\\) because \\((Y_{1}, Y_{0}) \\perp T\\mid \\tau\\). Algorithm: One-to-one nearest available matching on estimated propensity scores. Randomly order the treated and untreated (control) subjects; Select the first treated subject and find the control subject with the closest propensity score; Both subjects are then removed from the pool, and then repeat the second step until all the treated subjects are matched; Estimate the causal effects as in a randomized study, e.g., \\[ \\hat{\\theta}_{\\mathrm{M}}=\\frac{\\sum_{i\\in M}T_i Y_{1i}}{\\sum_{i\\in M} T_i}-\\frac{\\sum_{i\\in M}(1-T_i) Y_{0i}}{\\sum_{i\\in M} (1-T_i)}\\, , \\] where \\(M\\) denotes the matched dataset. There are other versions of the matching algorithm: one-to-one versus one-to-m, with replacement versus without replacement, etc. 7.2.2 Stratification Basic idea: Consider strata, \\(S_1, \\dots, S_K\\), \\[ E(Y_1-Y_0)=\\sum_{k=1}^KE(Y_1-Y_0\\mid X\\in S_k)P(X\\in S_k)\\, , \\] where we have balance within each stratum. \\(E(Y_1-Y_0\\mid X\\in S_k)\\) can be estimated as in a randomized study using data in \\(S_k\\); \\(P(X\\in S_k)\\) can be approximated by (number of subjects in \\(S_k\\))\\(/n\\). We may encounter the same problem as before when \\(X\\) is high-dimensional. Again, we can solve this problem by replacing \\(X\\) with the propensity score \\(\\tau(X)\\): \\[ E(Y_1-Y_0)=\\sum_{k=1}^KE(Y_1-Y_0\\mid \\tau(X)\\in S_k)P(\\tau(X)\\in S_k)\\, , \\] where individuals have similar, but not identical, values of \\(\\tau\\), in each stratum. Algorithm: Divide the units into \\(K\\) subclasses based on the quantiles of \\(\\hat{\\tau}_i\\)s (\\(i=1,...,n\\)); Estimate the average treatment effect within each subclass as in a randomized study and take the average of the estimated values across all the subclasses; mathematically, the estimate for the average treatment effect is given by \\[ \\hat{\\theta}_{\\mathrm{S}}=\\frac{1}{K}\\sum_{ k=1}^{K}\\left(\\frac{\\sum_{i\\in S_k} Y_{1i}T_i}{\\sum_{i\\in S_k} T_i}-\\frac{\\sum_{i\\in S_k} Y_{0i}(1-T_i)}{\\sum_{i\\in S_k} (1-T_i)}\\right)\\, , \\] where \\(S_k\\) means the \\(k\\)-th subclass. How to choose \\(K\\)? Usually 5. 7.2.3 Inverse Probability Weighting Basic idea: \\[ E\\left\\{\\frac{TY}{\\tau(X)}\\right\\}=E(Y_1) \\text{ and } E\\left\\{\\frac{(1-T)Y}{1-\\tau(X)}\\right\\}=E(Y_0)\\, . \\] The inverse probability weighted estimator of the average treatment effect is defined as \\[ \\hat{\\theta}_{\\mathrm{IPW}_1}=\\frac{1}{n} \\sum_{i=1}^n \\frac{T_iY_i}{\\hat{\\tau}_i}-\\frac{1}{n} \\sum_{i=1}^n\\frac{(1-T_i)Y_i}{1-\\hat{\\tau}_i}\\, . \\] We give each subject a weight \\(w_i\\), where \\(w_i=\\hat{\\tau}_i^{-1}\\) for those in the treatment group and \\(w_i=(1-\\hat{\\tau}_i)^{-1}\\) for those in the control group. By weighting, each subject is replicated \\(w_i\\) times. This creates a psuedo-population in which \\(T\\) and \\(X\\) are not associated anymore (no confounding). A more efficient estimator is \\[ \\hat{\\theta}_{\\mathrm{IPW}_2}=\\left(\\sum_{i=1}^n\\frac{T_i}{\\hat{\\tau}_i}\\right)^{-1} \\sum_{i=1}^n \\frac{T_iY_i}{\\hat{\\tau}_i}-\\left(\\sum_{i=1}^n\\frac{1-T_i}{1-\\hat{\\tau}_i}\\right)^{-1} \\sum_{i=1}^n\\frac{(1-T_i)Y_i}{1-\\hat{\\tau}_i}\\, . \\] 7.2.4 Doubly Robust Estimation Problem: If the propensity score model is incorrect, Matching, Stratfication and IPW estimators will be biased. Solution: Combine IPW with the regression modeling approach to protect against model misspecification. Note that we now have two sets of models: The model for the propensity score; The models for the potential outcomes: \\[ \\begin{aligned} m_1(X;{\\beta}_1)&amp;={E}(Y_{1}\\mid X)={E}(Y_{1}\\mid X, T=1)\\\\ m_0(X;{\\beta}_0)&amp;=E(Y_{0}\\mid X)={E}(Y_{0}\\mid X, T=0) \\end{aligned} \\] The so-called doubly robust estimators are consistent if one of the two sets of models is correctly specified. The doubly robust estimator for the average treatment effect is \\[ \\hat{\\theta}_{\\mathrm{DR}}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{T_{i} Y_{1i}-\\left(T_i-\\hat{\\tau}_i\\right)\\hat{m}_{1i}}{\\hat{\\tau}_{i}}-\\frac{1}{n} \\sum_{i=1}^{n} \\frac{(1-T_{i}) Y_{0i}+\\left(T_i-\\hat{\\tau}_i\\right)\\hat{m}_{0i}}{1-\\hat{\\tau}_{i}}\\, . \\] We can see that this estimator consists of the inverse probability weighted estimator and a second term used for augmenting. So, the doubly robust estimators are also called the augmented inverse probability weighted (AIPW) estimators. "],["pooled-p-values.html", "Chapter 8 Pooled p-values 8.1 A motivating example 8.2 Pooling \\(p\\)-values 8.3 Adjusting for dependence 8.4 Take aways", " Chapter 8 Pooled p-values Chris Salahub When presented with a large number of \\(p\\)-values from a large number of tests, we occasionally want to be lazy. Rather than fitting a big model or working very hard to try and make sense of each value individually in context, wed like to have a quick means of telling whether the \\(p\\)-values are worth looking at more closely. 8.1 A motivating example Lets say, for example, weve collected a bunch of genetic data and would like to identify which genes are related to a physical trait. Well start by defining some terms to make description easier. DNA: genetic information that is passed between generations and is stored in several big molecules made up of many small molecules called nucleotides genome: all the DNA of an organism gene: a segment of DNA that gives the blueprint for a particular protein chromosomes: one of the several big molecules making up the genome marker: a segment of DNA measured at a known position sex: the creation of an organism in a new generation by recombining DNA from two organisms in the previous generation recombination: when DNA is changed slightly between generations by segments of DNA literally crossing each other and switching or chromosomes being inherited independently, pictorially: centiMorgans: an additive measure of distance on DNA, for two markers which have probability \\(q\\) of being separated during sex the distance in centiMorgans is \\(d = 50 \\ln \\frac{1}{1 - 2q}\\) So, when we talk about genetic data, we mean markers at known positions on chromosomes which ideally span the whole genome of a sample of organisms. We pair this data with a physical measurement - in humans examples include height, eye colour, hair colour, or the presence of a disease - to determine how much physical variation can be explained by genetic variation. Physical traits that are linked to genetics are either Mendelian or non-Mendelian (you can guess which came first). Mendelian traits have variation explained by one (or very few) gene(s), take eye colour in humans which is mostly down to the gene OCA2. Non-Mendelian traits, like height, have variation which cannot be attributed to one (or a handful of) genetic region(s) alone. To identify which measured markers are related to a physical trait, a host of significance tests are typically computed to assay the impact of each. While this is not the best way to do this, a linear model would do much better, it is frequently the only option for genetic studies which collect hundred of thousands of markers for only a few dozen individuals. Difficulty fitting a model practically means we must filter these markers first. Before even pointing our finger at a subset of genes, however, we might want to save time by asking whether there is anything interesting happening at all in this collection of tests. That is where pooled \\(p\\)-values come in. 8.1.1 Some real data Our ideal data set would involve humans. Genomics mostly strives to better understand the genetic sources of human diseases and traits to help create better medicine and understand ourselves more fully. Unfortunately, human data is pretty hard to come by ethically, and mice data is a lot easier to find. The code below pulls some that is freely available from Mouse Genome Informatics. ## this script downloads mouse genomic data from the mouse genome ## database (MGD), an online repository of data on mouse genetics ## all the data is stored in text files at mgiUrl &lt;- &quot;https://www.informatics.jax.org/downloads/reports/&quot; ## we&#39;ll be looking at data from mgiPName &lt;- &quot;MGI_JAX_BSB_Panel.rpt&quot; ## and for reference we&#39;ll also need the marker lists in mgiMNames &lt;- c(&quot;MRK_List1.rpt&quot;, &quot;MRK_List2.rpt&quot;) ## this function reads in MGI text files, which are basically organized ## as a tab-separated table with extra information readMGIrpt &lt;- function(file) { raw &lt;- scan(file, what = character(), sep = &quot;\\n&quot;) # most basic R input leg &lt;- which(raw == &quot;Legend:&quot;) # identify the legend rows lenHead &lt;- leg + 4 # the +4 is from inspection of the legends if (length(leg) == 0) { # separate legend leg &lt;- which(grepl(&quot;^CHR&quot;, raw))[1] lenHead &lt;- leg } desc &lt;- paste(raw[1:lenHead], collapse = &quot;\\n&quot;) # data description dat &lt;- raw[(lenHead+1):length(raw)] # actual data ## each row in the data is associated with a reference, the study ## or paper that characterized or collected the corresponding ## observation, we want to separate these refPos &lt;- regexec(&quot;\\\\tJ\\\\:[0-9]+(?:, J\\\\:[0-9]+){0,4}&quot;, dat) refs &lt;- sapply(regmatches(dat, refPos), # extract references function(el) { if (length(el) == 0) { &quot;&quot; } else gsub(&quot;\\\\t&quot;, &quot;&quot;, el)}) data &lt;- unlist(regmatches(dat, refPos, invert = TRUE)) # remove refs mat &lt;- do.call(rbind, strsplit(data[data != &quot;&quot;], &quot;\\\\t&quot;)) # regularize rwnms &lt;- mat[1, -(1:3)] # animal numbers/ids colnms &lt;- mat[-1, 3] # symbol field colDesc &lt;- mat[-1, 1:3] # symbol details colnames(colDesc) &lt;- c(&quot;chr&quot;, &quot;mgiid&quot;, &quot;symbol&quot;) # informative names data &lt;- t(mat[-1,-(1:3)]) # observed values rownames(data) &lt;- rwnms # final data formatting colnames(data) &lt;- colnms list(summary = desc, # return everything in a big list markers = data.frame(colDesc, ref = refs[-1]), data = as.data.frame(data)) } ## this function looks at the marker reference data and processes it into ## a form easier for future analysis readMGIlists &lt;- function(fileList = paste0(mgiUrl, mgiMNames)) { lists &lt;- lapply(fileList, scan, # similar input to above what = character(), sep = &quot;\\n&quot;) lists &lt;- lapply(lists, gsub, # replace end of line tabs with periods pattern = &quot;\\t$&quot;, replacement= &quot;\\t.&quot;) splits &lt;- lapply(lists, strsplit, split = &quot;\\t&quot;) # split by tabs colnms &lt;- splits[[1]][[1]] # column names data &lt;- do.call(rbind, # covert into one big matrix lapply(splits, function(splt) do.call(rbind, splt[-1]))) colnames(data) &lt;- colnms as.data.frame(data) } ## using indices and a reference table, process centiMorgan positions processcMs &lt;- function(inds, tab) { sel &lt;- tab[inds] # take indices from table sel[grepl(&quot;syntenic&quot;, sel)] &lt;- &quot;Inf&quot; suppressWarnings(as.numeric(sel)) # warnings by design } ## this function goes through a processed panel from the above functions ## and drops the measurements that are NA or Inf filterPanelBycM &lt;- function(panel, locs) { locOrd &lt;- order(locs) # order position toKeep &lt;- is.finite(locs[locOrd]) # drop NAs and Infs outMrk &lt;- data.frame(panel$markers[locOrd[toKeep],], cMs = locs[locOrd[toKeep]]) outMrk &lt;- outMrk[order(outMrk$chr),] # order chromosome list(summary = panel$summary, markers = outMrk, data = panel$data[, outMrk$symbol]) } ## pull the marker reference mgiMarkers &lt;- readMGIlists() # descriptions of all markers ## pull the panel mgiPanel &lt;- readMGIrpt(paste0(mgiUrl, mgiPName)) ## match the marker names back to the marker reference mgiPanel.mrkr &lt;- match(names(mgiPanel$data), mgiMarkers$`Marker Symbol`) ## get cM positions mgiPanel.cMs &lt;- processcMs(mgiPanel.mrkr, tab = mgiMarkers$`cM Position`) ## Infs indicate markers localized to a chromosome but without a ## known cM position, while NAs indicate markers missing from the ## reference file or marked as unknown there ## filter the panels by markers with known positions mgiFiltered &lt;- filterPanelBycM(mgiPanel, mgiPanel.cMs) ## and convert the data to numeric values mgiFiltered$data &lt;- as.data.frame(lapply(mgiFiltered$data, function(mrkr) as.numeric(mrkr== &quot;b&quot;))) ## remove intermediate data rm(list = c(&quot;mgiMarkers&quot;, &quot;mgiPanel.cMs&quot;, &quot;mgiPanel.mrkr&quot;, &quot;mgiPanel&quot;)) summary(mgiFiltered) ## Length Class Mode ## summary 1 -none- character ## markers 5 data.frame list ## data 1598 data.frame list dim(mgiFiltered$data) ## [1] 94 1598 In the end, after reading and cleaning the data, we have 94 mice each measured at 1598 marker locations. We are still in the unfortunate position of lacking physical measurements for any of the 94 mice on this panel, but we wont let that stop our workshop! To mimic Mendelian traits, we can pick a marker and generate a trait based solely on its value, while mimicking non-Mendelian traits requires the selection of numerous locations which may not contribute equally. For now, lets keep this as simple as possible by ensuring independence and equal contributions of each marker. Chromosomes assort independently, so if we take one marker from each chromosome we should have independent markers. ## match in this context will give the index of the first marker on ## each chromosome mgiIndep &lt;- mgiFiltered$data[, match(c(1:19, &quot;X&quot;), mgiFiltered$markers$chr)] ## inspect dependence structure image(cor(mgiIndep)) plot of chunk unnamed-chunk-3 To simulate a Mendelian trait, well simply add noise to the first marker. For non-Mendelian inheritance, lets take the average of all markers and add noise. set.seed(2314) traitMendel &lt;- mgiIndep[, 1] + rnorm(94, sd = 0.3) plot(density(traitMendel)) # what mendelian traits look like traitNonMend &lt;- apply(mgiIndep[, 1:20], 1, mean) + rnorm(94, sd = 0.3) plot(density(traitNonMend)) # what non-mendelian traits look like Finally, we can compute \\(p\\)-values for each marker and the traits based on the correlation between them. ## based on the mendelian trait (pMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitMendel, col, method = &quot;kendall&quot;)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 ## 1.534175e-16 6.824098e-01 5.068242e-01 6.829543e-01 8.917066e-01 2.992248e-01 5.961899e-01 3.932094e-01 7.507340e-01 4.141103e-01 4.231891e-02 ## D12Mit37 D13Mit158 D14Mit179 D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## 2.401990e-01 5.082020e-01 2.662022e-01 4.665408e-01 1.844711e-01 1.360688e-01 4.732496e-01 5.909141e-01 9.307018e-01 ## based on the non-mendelian trait (pnonMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitNonMend, col, method = &quot;kendall&quot;)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 ## 0.89472070 0.09253545 0.13757128 0.22621058 0.07549440 0.56469712 0.65008451 0.04082007 0.12843907 0.11980696 0.03455362 0.25570435 0.28700806 ## D14Mit179 D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## 0.73358350 0.77494661 0.51304455 0.11368337 0.14611911 0.15727257 0.82347631 ## some null cases pNull &lt;- replicate(1000, apply(mgiIndep, 2, function(col) { cor.test(runif(94), col, method = &quot;kendall&quot;)$p.value })) head(t(pNull)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 ## [1,] 0.91268889 0.9155166 0.44057344 0.57053011 0.7109228 0.7104213 0.9361935 0.6625519 0.903676933 0.09295126 0.9066941 0.61167595 0.90315172 ## [2,] 0.69695226 0.4131551 0.33937625 0.54010605 0.2629633 0.8975040 0.3197514 0.8584467 0.939708593 0.60228265 0.9487503 0.65482901 0.47939849 ## [3,] 0.99698325 0.5393703 0.04396815 0.07304071 0.6663768 0.4531596 0.7632931 0.3486067 0.933692719 0.74391544 0.3237270 0.68797867 0.98786455 ## [4,] 0.79418075 0.3208888 0.33167468 0.67741095 0.6829543 0.4486095 0.4572592 0.1803964 0.008114586 0.21910493 0.5991995 0.03022549 0.91520882 ## [5,] 0.05429148 0.5494324 0.03791381 0.71092280 0.5351154 0.2401990 0.6175043 0.2260813 0.034191029 0.83974007 0.8117261 0.68797867 0.05928764 ## [6,] 0.47954305 0.1478201 0.83885530 0.77379283 0.8737982 0.6658075 0.9119715 0.5666431 0.710922799 0.98758873 0.1047974 0.92754962 0.78425210 ## D14Mit179 D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## [1,] 0.09763453 0.9847927 0.4206221 0.6318542 0.75279081 0.6391459 0.67471523 ## [2,] 0.02827514 0.7984038 0.9126889 0.9272855 0.56151556 0.3087532 0.07250225 ## [3,] 0.73358350 0.7516931 0.6097514 0.7667740 0.64067853 0.3386443 0.43382873 ## [4,] 0.02882409 0.7749466 0.5939547 0.7264643 0.56151556 0.9240737 0.12757793 ## [5,] 0.17099493 0.8878311 0.1373020 0.9636049 0.91237224 0.6391459 0.55277353 ## [6,] 0.44041744 0.8398827 0.8768064 0.6050557 0.04977771 0.4480823 0.62573218 8.2 Pooling \\(p\\)-values Whether inheritance is Mendelian, non-Mendelian, or something else, our selection of markers tested against a trait gives a collection of \\(M=20\\) \\(p\\)-values from 20 hypothesis tests, and wed like to know if further investigation is warranted for any of them. Directly, we want to test the null hypothesis \\[H_0 = \\cap_{i = 1}^M H_{0i}\\] where \\(H_{0i}\\) is the hypothesis of no association the \\(i\\)th marker. Another way of thinking about this is by considering the null distributions of our \\(M\\) \\(p\\)-values \\(p_1, \\dots, p_M\\) if there is nothing going on. In this case, wed expect \\[H_0 = p_1, p_2, \\dots, p_M \\overset{\\mathrm{iid}}{\\sim} U\\] where \\(U = Unif(0,1)\\). The basic idea of pooled \\(p\\)-values is to think of a function that takes \\(M\\) \\(p\\)-values as an argument and returns a \\(p\\)-value that behaves like a univariate \\(p\\)-value for a test of \\(H_0\\). There are two kinds both based on the null distributions of \\(\\mathbf{p} = p_1, \\dots, p_M\\). The first kind uses \\(p_{(k)}\\), the \\(k\\)th order statistic of \\(\\mathbf{p}\\). Under \\(H_0\\), the probability that \\(p_{(k)}\\) is less than or equal to \\(q\\) is the probability that \\(k\\) or more uniform \\(p\\)-values less than \\(q\\). In a formula, that is \\[F_{(k)}(q) = P(p_{(k)} \\leq q) = \\sum_{l = k}^M q^l (1 - q)^{M - l}.\\] Therefore, \\(F_{(k)}(p_{(k)}) \\sim U\\) and we have our first pooled \\(p\\)-value \\(g_{(k)}(\\mathbf{p}) = F_{(k)}(p_{(k)})\\). One example of this is from Tippett (1931), which uses \\(p_{(1)}\\) to obtain \\[g_{Tip}(\\mathbf{p}) = 1 - (1 - p_{(1)})^M,\\] the exact version of the commonly used approximation \\(Mp_{(1)}\\) known as the Bonferroni correction. The second kind of pooled \\(p\\)-value makes use of quantile transformations. For a continuous random variable \\(X\\) with cumulative distribution function (CDF) \\(F\\) and quantile function \\(F^{-1}\\), \\(F^{-1}(U) = X\\). If \\(X\\) is known to have a CDF \\(F_M\\) under summation, the quantities \\[g(\\mathbf{p}) = 1 - F_M \\left ( \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right )\\] and \\[g(\\mathbf{p}) = F_M \\left ( \\sum_{i = 1}^M F^{-1}(p_i) \\right )\\] will both behave exactly like a univariate \\(p\\)-value. Obvious choices for \\(X\\) are the normal distribution and gamma distribution because both of these distributions are closed under summation. The majority of proposed pooled \\(p\\)-values follow the first of these quantile transformation methods, including \\(g_{Fis}\\) of Fisher (1932) (using the \\(\\chi^2_2\\) distribution) and \\(g_{Sto}\\) of Stouffer et al. (1949) (using the \\(N(0,1)\\) distribution). Lets code all of these up: ## the order statistic function gord &lt;- function(p, k) { pk &lt;- sort(p, decreasing = FALSE)[k] # kth order statistic 1 - pbinom(k - 1, length(p), pk) } ## the quantile transformation function gquant &lt;- function(p, finv, fm) { 1 - fm(sum(finv(1 - p))) } ## make particular instances gtip &lt;- function(p) gord(p, 1) gfis &lt;- function(p) gquant(p, finv = function(p) qchisq(p, 2), fm = function(s) pchisq(s, 2 * length(p))) gsto &lt;- function(p) gquant(p, finv = qnorm, fm = function(s) pnorm(s, sd = sqrt(length(p)))) ## store together in a list pools &lt;- list(tippett = gtip, fisher = gfis, stouffer = gsto) We can now compute the pooled \\(p\\)-values of our earlier Mendelian and non-Mendelian cases: (poolMendel &lt;- lapply(pools, function(f) f(pMendel))) ## $tippett ## [1] 3.108624e-15 ## ## $fisher ## [1] 2.611221e-08 ## ## $stouffer ## [1] 0.01616983 (poolNonMend &lt;- lapply(pools, function(f) f(pnonMendel))) ## $tippett ## [1] 0.5050479 ## ## $fisher ## [1] 0.01956811 ## ## $stouffer ## [1] 0.007787462 All methods lead to rejection at \\(\\alpha = 0.05\\) for the for the first set of \\(p\\)-values (not too surprising given that the first \\(p\\)-value is less than \\(1 \\times 10^{-15}\\)), but the non-Mendelian case leads to some interesting differences. Lets look at the two sets of \\(p\\)-values again: rbind(Mendel = round(pMendel, 2), `Non-Mendel` = round(pnonMendel, 2)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 D15Mit12 ## Mendel 0.00 0.68 0.51 0.68 0.89 0.30 0.60 0.39 0.75 0.41 0.04 0.24 0.51 0.27 0.47 ## Non-Mendel 0.89 0.09 0.14 0.23 0.08 0.56 0.65 0.04 0.13 0.12 0.03 0.26 0.29 0.73 0.77 ## D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## Mendel 0.18 0.14 0.47 0.59 0.93 ## Non-Mendel 0.51 0.11 0.15 0.16 0.82 When only one marker is responsible for the variation in the trait, a single \\(p\\)-value is highly significant. In the case of many responsible markers the \\(p\\)-values are not so obviously significant, with the smallest one barely less than 0.05. Rather, this case has a greater number of small \\(p\\)-values than we would expect by chance if all are truly uniform and independent, a fact more easily seen by viewing a histogram of the two sets of \\(p\\)-values. hist(pMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 10)) hist(pnonMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 10)) Though the proof is beyond the scope of this workshop, it turns out the ordering \\(g_{Tip} &lt; g_{Fis} &lt; g_{Sto}\\) in the Mendelian case being flipped to \\(g_{Tip} &gt; g_{Fis} &gt; g_{Sto}\\) in the non-Mendelian case reflects a deeper concept. Broadly, any pooled \\(p\\)-value is sensitive to either a large number of weakly significant \\(p\\)-values or a small number of strongly significant \\(p\\)-values. There is a fundamental trade-off between these two cases for any pooled \\(p\\)-value, which we can visualize crudely using the mean and minimum values of each simulated null sample in pNull. The following plot shows the pooled \\(p\\)-value for each method by the mean of the sample, meant to represent the evidence spread broadly. Point sizes for each mean value are scaled by the minimum \\(p\\)-value, meant to represent concentrated evidence in one test. ## compute medians for each sample pNullMean &lt;- apply(t(pNull), 1, mean) ## and the minimum pNullMin &lt;- apply(t(pNull), 1, min) ## as well as each pooled p-value pNullPools &lt;- apply(t(pNull), 1, function(row) sapply(pools, function(f) f(row))) ## plot the pooled p-value by mean of sample plot(pNullMean, pNullPools[&quot;fisher&quot;, ], pch = 16, ylab = &quot;Pooled p-value&quot;, cex = pNullMin * 4 + 0.3, xlab = &quot;Mean of p-value sample&quot;, col = adjustcolor(&quot;firebrick&quot;, 0.3)) points(pNullMean, pNullPools[&quot;stouffer&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;steelblue&quot;, 0.3)) points(pNullMean, pNullPools[&quot;tippett&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;seagreen&quot;, 0.3)) legend(x = &quot;topleft&quot;, col = adjustcolor(c(&quot;firebrick&quot;, &quot;steelblue&quot;, &quot;seagreen&quot;), 0.3), legend = c(expression(g[Fis]), expression(g[Sto]), expression(g[Tip])), pch = 16) plot of chunk unnamed-chunk-11 Note how \\(g_{Sto}\\) is increasing almost monotonically in the mean, with large and small points spread mixed within the line for each mean value. \\(g_{Fis}\\), as suggested by its position giving a \\(p\\)-value between \\(g_{Sto}\\) and \\(g_{Tip}\\), follows this line somewhat, but is sensitive to the minimum. The large points, indicating samples with large minimum values, tend to have a larger \\(p\\)-value than the \\(g_{Sto}\\) line while the small points, samples with small minimum values, tend to have a smaller \\(p\\)-value. \\(g_{Tip}\\) barely follows the line at all, the only relationship we can see in the green points is between the size of the point and its \\(y\\)-value. All of this to say: \\(g_{Sto}\\) is not sensitive to the minimum, it follows the overall evidence spread among the \\(p\\)-values \\(g_{Fis}\\) balances sensitivity to the minimum and overall spread of \\(p\\)-values \\(g_{Tip}\\) is not sensitive to the overall spread of evidence, it only responds to the minimum Of course, these examples all assume independence of the \\(p\\)-values. In the case of dependence, the idea of what spread evidence means is a lot less clear. Are multiple small \\(p\\)-values an indication of multiple independent contributing variables, or simply an artifact of correlations between them? 8.3 Adjusting for dependence A comprehensive overview of the methods used to adjust for dependence can be found in the companion paper to the poolr package, Cinar and Viechtbauer (2022). It should come as no surprise that the above functions have been implemented in this package alongside the adjustments. Well take a brief aside to familiarize ourselves with them. Instead of using the functions we defined previously, we can take library(poolr) ## the naming is more convenient than ours (pfMend &lt;- fisher(pMendel, adjust = &quot;none&quot;)) ## combined p-values with: Fisher&#39;s method ## number of p-values combined: 20 ## test statistic: 108.358 ~ chi-square(df = 40) ## adjustment: none ## combined p-value: 3.232063e-08 (psMend &lt;- stouffer(pMendel, adjust = &quot;none&quot;)) # more accurate for small p-values ## combined p-values with: Stouffer&#39;s method ## number of p-values combined: 20 ## test statistic: 2.131 ~ N(0,1) ## adjustment: none ## combined p-value: 0.01652469 (ptMend &lt;- tippett(pMendel, adjust = &quot;none&quot;)) ## combined p-values with: Tippett&#39;s method ## number of p-values combined: 20 ## minimum p-value: 0 ## adjustment: none ## combined p-value: 2.220446e-15 ## non-Mendelian case (pfNM &lt;- fisher(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Fisher&#39;s method ## number of p-values combined: 20 ## test statistic: 60.542 ~ chi-square(df = 40) ## adjustment: none ## combined p-value: 0.01956811 (psNM &lt;- stouffer(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Stouffer&#39;s method ## number of p-values combined: 20 ## test statistic: 2.419 ~ N(0,1) ## adjustment: none ## combined p-value: 0.007787462 (ptNM &lt;- tippett(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Tippett&#39;s method ## number of p-values combined: 20 ## minimum p-value: 0.035 ## adjustment: none ## combined p-value: 0.5050479 We might immediately notice that this is more accurate for the small \\(p\\)-values occurring in the Mendelian example and has some slightly different values for the non-Mendelian example. This is a direct result of the way gquant is defined, we can achieve the same result as poolr by using the lower.tail argument rather than subtracting values from 1. Computational statistics is not quite the same as mathematical statistics (making all of our lives harder). ## our function for gsto gave 0.01616983 for the mendelian data, can ## we recreate poolr&#39;s 0.01652? pnorm(sum(qnorm(pMendel, lower.tail = FALSE))/sqrt(20), lower.tail = FALSE) ## [1] 0.01652469 In any case, the ordering with these more accurate \\(p\\)-values is identical to the ordering from our functions. When evidence is concentrated in one test \\(g_{Tip} &lt; g_{Fis} &lt; g_{Sto}\\) while when evidence is spread \\(g_{Tip} &gt; g_{Fis} &gt; g_{Sto}\\). With that brief exploration out of the way, we can define some correlated data using our filtered panel from before. Lets take a random sample of markers on chromosome 1. set.seed(124093) inds &lt;- sort(sample(which(mgiFiltered$markers$chr == &quot;1&quot;), 20)) mgiDep &lt;- mgiFiltered$data[, inds] # select sampled inds mgiDepCors &lt;- cor(mgiDep) # important for later ## image correlations between these image(mgiDepCors) plot of chunk unnamed-chunk-14 8.3.1 The wrong way If we check the first few lines of code for any of the poolr pooling functions, take fisher, well see several options for adjustment: head(fisher, 7) ## ## 1 function (p, adjust = &quot;none&quot;, R, m, size = 10000, threshold, ## 2 side = 2, batchsize, nearpd = TRUE, ...) ## 3 { ## 4 p &lt;- .check.p(p) ## 5 k &lt;- length(p) ## 6 adjust &lt;- match.arg(adjust, c(&quot;none&quot;, &quot;nyholt&quot;, &quot;liji&quot;, &quot;gao&quot;, ## 7 &quot;galwey&quot;, &quot;empirical&quot;, &quot;generalized&quot;)) The first four of these after none are all based on the concept of effective number of tests. Basically, these compute the eigenvalues of the correlation matrix between the tests used to generate \\(\\mathbf{p}\\) and use some function of these eigenvalues to estimate an effective number of independent tests. Much like in principal components analysis (PCA), this assumes that most of the information in \\(M\\) \\(p\\)-values can be summarized by a smaller number of tests, \\(m\\). Unlike PCA, however, these methods do not select the important tests, rather they just take the entirely ad-hoc \\[g_{adj}(\\mathbf{p}) = 1 - F_M \\left ( \\frac{m}{M} \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right )\\] instead of \\[g_{adj}(\\mathbf{p}) = 1 - F_M \\left ( \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right ).\\] Lets explore the result. ## start by generating traits as before set.seed(2314) depMendel &lt;- mgiDep[, 1] + rnorm(94, sd = 0.3) plot(density(depMendel)) depNonMend &lt;- apply(mgiDep[, 1:20], 1, mean) + rnorm(94, sd = 0.3) plot(density(depNonMend)) ## these look basically the same as before, despite the correlations ## what about the p-values? (pMendelDep &lt;- apply(mgiDep, 2, function(col) cor.test(depMendel, col, method = &quot;kendall&quot;)$p.value)) ## D1Mit475 D1Mit412 D1Mit245 D1Mit248 D1Mit214 D1Mit303 D1Mit480 D1Mit325 D1Mit180 D1Mit81 D1Mit305 ## 1.534175e-16 9.299242e-08 9.299242e-08 2.703365e-06 2.318341e-03 4.071072e-03 4.071072e-03 1.346609e-02 8.678426e-01 6.774110e-01 4.440854e-01 ## D1Mit188 D1Mit218 Cxcr4 D1Mit445 D1Mit346 Fmn2 D1Mit404 D1Mit273 D1Mit37 ## 2.794771e-01 2.480582e-01 5.058550e-01 2.471773e-01 3.469076e-01 3.355112e-01 5.185595e-01 4.276276e-01 4.456167e-01 ## based on the non-mendelian trait (pNonMenDep &lt;- apply(mgiDep, 2, function(col) cor.test(depNonMend, col, method = &quot;kendall&quot;)$p.value)) ## D1Mit475 D1Mit412 D1Mit245 D1Mit248 D1Mit214 D1Mit303 D1Mit480 D1Mit325 D1Mit180 D1Mit81 D1Mit305 ## 1.148746e-01 2.042236e-04 2.042236e-04 1.844895e-04 9.460200e-07 1.040595e-06 1.040595e-06 3.580850e-07 9.012084e-09 1.230827e-08 1.802381e-08 ## D1Mit188 D1Mit218 Cxcr4 D1Mit445 D1Mit346 Fmn2 D1Mit404 D1Mit273 D1Mit37 ## 1.402335e-08 4.300467e-08 1.911574e-07 1.440476e-09 1.113600e-08 2.161747e-05 8.946136e-06 3.646447e-05 1.550137e-04 Dependence between the tests pulls the \\(p\\)-values down universally. While in the previous case only a handful of tests would lead to rejection at the canonical \\(\\alpha = 0.05\\), almost most tests lead to rejection in both cases here. If we do not account for this known correlation, our pooled \\(p\\)-values will be universally smaller as a result and we will reject \\(H_0\\) far more often than we should. Do the adjustments make a difference? ## trying all effective test adjustments on both data sets adjustments &lt;- c(none = &quot;none&quot;, nyholt = &quot;nyholt&quot;, liji = &quot;liji&quot;, gao = &quot;gao&quot;, galwey = &quot;galwey&quot;) ## having to suppress warnings is always a good sign... suppressWarnings(adjMenFis &lt;- sapply(adjustments, function(a) fisher(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenTip &lt;- sapply(adjustments, function(a) tippett(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenSto &lt;- sapply(adjustments, function(a) stouffer(p = pMendelDep, adjust = a, R = mgiDepCors))) ## compare p-values between methods rbind(Tippett = adjMenTip[&quot;p&quot;, ], Fisher = adjMenFis[&quot;p&quot;, ], Stouffer = adjMenSto[&quot;p&quot;, ]) ## none nyholt liji gao galwey ## Tippett 2.220446e-15 1.554312e-15 7.771561e-16 1.554312e-15 7.771561e-16 ## Fisher 5.325435e-28 2.915668e-20 3.520612e-11 2.915668e-20 3.520612e-11 ## Stouffer 2.221926e-15 2.674898e-11 1.748554e-06 2.674898e-11 1.748554e-06 ## and &#39;effective number of tests&#39; rbind(Tippett = adjMenTip[&quot;m&quot;, ], Fisher = adjMenFis[&quot;m&quot;, ], Stouffer = adjMenSto[&quot;m&quot;, ]) ## none nyholt liji gao galwey ## Tippett NULL 14 7 14 7 ## Fisher NULL 14 7 14 7 ## Stouffer NULL 14 7 14 7 These adjustments definitely help reduce our problem, the pooled \\(p\\)-values for \\(g_{Sto}\\) and \\(g_{Fis}\\) increased, but they disagree on the effective number of tests. Its worth noting that this effective number of tests has nothing to do with our Mendelian generation of traits, it looks at the correlations alone. This makes its interpretation difficult, to say the least. Perhaps the non-Mendelian case will fare better. suppressWarnings(adjNMFis &lt;- sapply(adjustments, function(a) fisher(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMTip &lt;- sapply(adjustments, function(a) tippett(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMSto &lt;- sapply(adjustments, function(a) stouffer(p = pNonMenDep, adjust = a, R = mgiDepCors))) rbind(Tippett = adjNMTip[&quot;p&quot;, ], Fisher = adjNMFis[&quot;p&quot;, ], Stouffer = adjNMSto[&quot;p&quot;, ]) ## none nyholt liji gao galwey ## Tippett 2.880952e-08 2.016666e-08 1.008333e-08 2.016666e-08 1.008333e-08 ## Fisher 1.711263e-88 9.876385e-63 1.277073e-32 9.876385e-63 1.277073e-32 ## Stouffer 8.507174e-93 1.302981e-65 7.765426e-34 1.302981e-65 7.765426e-34 rbind(Tippett = adjNMTip[&quot;m&quot;, ], Fisher = adjNMFis[&quot;m&quot;, ], Stouffer = adjNMSto[&quot;m&quot;, ]) ## none nyholt liji gao galwey ## Tippett NULL 14 7 14 7 ## Fisher NULL 14 7 14 7 ## Stouffer NULL 14 7 14 7 As the correlation structure is identical, the effective number of tests have not changed at all. However, the much smaller \\(p\\)-values across the board in the non-Mendelian case lead to much smaller pooled \\(p\\)-values, even with adjustment. Using the correlation between markers as a proxy for correlation between tests is somewhat unfair (though it is exactly what the original proposals suggest), but even under more favourable assumptions these adjustments fail. Consider, for example, the case where \\(l\\) of the \\(M\\) \\(p\\)-values are identical, or nearly so. If we denote the independent tests with \\(x_1, \\dots, x_{k - l}\\) and the dependent tests with \\(y_1, \\dots, y_l\\), we can write \\[\\tilde{X}^2 = \\frac{m}{M} X^2 = \\frac{m}{M} \\left ( \\sum_{i = 1}^{M-l} x_i + \\sum_{j = 1}^l y_j \\right ).\\] Supposing that we correctly identify that \\(m = M - l + 1\\) and letting \\(y^*\\) represent the single unique value taken by \\(y_1, \\dots, y_l\\) gives \\[\\frac{M - l + 1}{M} \\left ( \\sum_{i=1}^{M-l} x_i + l y^* \\right ).\\] Now, if \\(x_i \\sim \\chi^2_{(2)}\\) and \\(y^* \\sim \\chi^2_{(2)}\\) all independently and identically (were using \\(g_{Fis}\\)), this sum is not generally \\(\\chi^2\\) distributed. The same is true for the rescaled statistic of Stouffers method. Why these theoretically invalid methods with vague interpretations were included in poolr is unclear, but they probably shouldnt be used. 8.3.2 A better way Instead, were better off relying on good old-fashioned simulation and normal approximations, as these are both at least asymptotically valid. The \"empirical\" and \"generalized\" methods offer these two options, respectively. ## the valid adjustment methods adjustments &lt;- c(adjustments, empirical = &quot;empirical&quot;, generalized = &quot;generalized&quot;) ## compare the resulting p-values suppressWarnings(adjMenFis &lt;- sapply(adjustments, function(a) fisher(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenTip &lt;- sapply(adjustments[-length(adjustments)], function(a) tippett(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenSto &lt;- sapply(adjustments, function(a) stouffer(p = pMendelDep, adjust = a, R = mgiDepCors))) rbind(Fisher = adjMenFis[&quot;p&quot;, ], Tippett = c(adjMenTip[&quot;p&quot;, ], NA), Stouffer = c(adjMenSto[&quot;p&quot;, ])) ## none nyholt liji gao galwey empirical generalized ## Fisher 5.325435e-28 2.915668e-20 3.520612e-11 2.915668e-20 3.520612e-11 0.00019998 6.821635e-14 ## Tippett 2.220446e-15 1.554312e-15 7.771561e-16 1.554312e-15 7.771561e-16 9.999e-05 NA ## Stouffer 2.221926e-15 2.674898e-11 1.748554e-06 2.674898e-11 1.748554e-06 0.00149985 0.004117298 For the Mendelian case, this makes a huge difference. We can see the highly inflated significance is reduced more by the empirical and generalized methods, which produce \\(p\\)-values closer to the independent case. This is a bit deceptive for the empirical version, as it is based on simulation and so cant report \\(p\\)-values less than \\(1/n_r\\) where \\(n_r\\) is the number of simulation runs. For the generalized method, at least, that the \\(p\\)-value for Stouffer is closer to the independent case suggests this combination is accounting for dependence more successfully: as the generation method is identical for the independent and dependent case, we might expect close \\(p\\)-values. ## compare the resulting p-values suppressWarnings(adjNMFis &lt;- sapply(adjustments, function(a) fisher(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMTip &lt;- sapply(adjustments[-length(adjustments)], function(a) tippett(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMSto &lt;- sapply(adjustments, function(a) stouffer(p = pNonMenDep, adjust = a, R = mgiDepCors))) rbind(Fisher = adjNMFis[&quot;p&quot;, ], Tippett = c(adjNMTip[&quot;p&quot;, ], NA), Stouffer = c(adjNMSto[&quot;p&quot;, ])) ## none nyholt liji gao galwey empirical generalized ## Fisher 1.711263e-88 9.876385e-63 1.277073e-32 9.876385e-63 1.277073e-32 9.999e-05 1.364281e-41 ## Tippett 2.880952e-08 2.016666e-08 1.008333e-08 2.016666e-08 1.008333e-08 9.999e-05 NA ## Stouffer 8.507174e-93 1.302981e-65 7.765426e-34 1.302981e-65 7.765426e-34 9.999e-05 3.13093e-12 The non-Mendelian case shows exactly how the empirical version breaks down. Not a single simulated example was less than the observed values for any of our pooled \\(p\\)-values, and so all report the same small \\(p\\)-value. We could solve this issue by running a larger simulation, but increasing the number of repetitions defeats the purpose of pooled \\(p\\)-values. These are meant to be quick diagnostics, and so should present no computational burden. 8.4 Take aways when youre feeling lazy and have a bunch of \\(p\\)-values, try pooling them pooled \\(p\\)-values have a fundamental trade-off between looking for a few highly significant values or many weakly significant values not all adjustments for dependence are equal, make sure you use theoretically valid and interpretable ones "],["chapter9.html", "Chapter 9 Active Learning Strategies with p-Values 9.1 Introduction 9.2 Interactive Activity 9.3 Annotated Bibliography", " Chapter 9 Active Learning Strategies with p-Values Luke Hagar 9.1 Introduction Welcome to this workshop on active learning strategies with p-values. Hopefully, this workshop helps improve your ability to Describe best practices for teaching statistics Explain the importance of developing statistical thinking skills via active learning Identify advantages of using guided simulation activites to improve understanding of p-values. The presentation component of this workshop is accompanied by an interactive activity. This interactive activity is described below. 9.2 Interactive Activity 9.2.1 Background We use an interactive application to explore some features of the p-value using simulation: https://lmhagar.shinyapps.io/CUT_App/ We use a mock experiment to assess the impact of active learning in the classroom. We are facilitating two versions of the same course, one with traditional lectures and the other with flipped lectures. We compare the effectiveness of the two lecture formats using the students final exam marks. To do this, we test the hypothesis that the average exam marks in the two versions of the course are the same. 9.2.2 Instructions The simulation settings can be controlled using the Simulation Inputs tab. We simulate an exam mark for each student in the experiment, such that the average mark in the traditional lecture is 70. The simulation has two inputs: Average mark in flipped lecture: marks in the flipped lecture are simulated such that the average mark is determined by this slider. It can take whole numbers between 64 and 76. Number of students per class: this input controls how many marks we simulate for each class. It can be increased from 5 to 125 in increments of 20. The active learning experiment with these settings is repeated 1000 times each time the Generate button is pressed. For each repetition, we obtain a p-value. We use the histogram of these 1000 p-values to explore features of the p-value. Each p-value tells us how compatible the simulated data are with the hypothesis of the two average marks being the same. The red line on the histogram shows a 5% cutoff. This cutoff is typically used in practice (i.e., the hypothesis of the two average marks being equal is rejected if the p-value is less than 5%). The plot title conveys the proportion of the 1000 simulations in which the p-value was less than 5%. The simulation can be rerun adjusting the slider inputs (if necessary) and pressing the Generate button. Activity 1: We start with the default settings (average mark in flipped lecture = 70, number of students per class = 5). Please complete this part before pressing Generate. For this setting, the hypothesis is true. After pressing Generate, you will be presented with a histogram of p-values. What do you think this histogram will look like? Repeat the simulation a few times with these settings. What patterns do you see in the histogram? Is this what you expected to see? Approximately what proportion of the p-values are less than the 5% cutoff? Please pause here. Please move the Number of students per class slider to a larger sample size. Before you press Generate, how do you think the histogram of p-values will change from those for the initial setting? Repeat the simulation a few times with these settings. What changes to the histogram do you notice? Is this what you expected to see? Approximately what proportion of the p-values are less than the 5% cutoff? Please pause here. What proportion of p-values do you think would be less than a 10% cutoff for this scenario? For this simulation setting, would rejecting the hypothesis be the correct decision? Why might 5% be a popular cutoff value then? Please pause here. Activity 2: Please return the Number of students per class slider to 5 students per class. Think of a difference d between the two average marks that would be of practical importance to you. For this simulation, d should be one of 1, 2, 3, 4, 5, or 6. For instance, if the difference between the two averages is less than d, we can consider the averages for the two classes to be practically the same. Otherwise, there is a notable difference in the average marks. Move your average mark in flipped lecture to 70 + d. Please complete this part before pressing Generate. For this setting, what do you think the histogram of p-values will look like? Repeat the simulation a few times with these settings. What patterns do you see in the histogram? Is this what you expected to see? Approximately what proportion of the p-values are less than the 5% cutoff? Please pause here. Please move the Number of students per class slider to 25. Before you press Generate, how do you think the histogram of p-values will change from those from part a)? Repeat the simulation a few times with these settings. You could also try this with larger sample sizes. What changes to the histogram do you notice? Is this what you expected to see? Approximately what proportion of the p-values are less than the 5% cutoff? For this simulation setting, what is the correct decision with regard to rejecting or not rejecting the hypothesis? Are you likely to make the correct decision if you have a small sample size of students? Activity complete! 9.3 Annotated Bibliography The following references were integral to the preparation of this workshop. 1: Abbasian, R. O., &amp; Czuchry, M. (2021). Investigation of Inverted and Active Pedagogies in Introductory Statistics. PRIMUS, 31(9), 975-994. This article summarizes results from an observational study comparing student achievement in an introductory statistics course delivered in inverted and traditional lecture formats. Students in traditional courses improved slightly more as the course progressed compared to those in inverted classes, particularly when considering the subpopulation of students who were eligible for Pell grants. This papers recommendations for inverting statistics classes were referred to in the workshop, and this source also emphasizes why it is important to consider the impact of academic interventions on disadvantaged populations. 2: Case, C., Battles, M., &amp; Jacobbe, T. (2019). Toward an understanding of p-values: Simulation-based inference in a traditional statistics course. Investigations in Mathematics Learning, 11(3), 195-206. This paper examines the impact of two simulation activities on undergraduate students understanding of p-values. Qualitative analysis of student assessments before and after these activities suggests that simulation activities may help improve conceptual understanding of p-values. However, students still held a wide range of misconceptions about p-values after the activities. This source supports the notion that simulation and guided discovery learning can be useful when teaching p-values, which was further incorporated in the interactive application made for this workshop. 3: Coetzee, W. (2021). Determining the needs of introductory statistics university students: A qualitative survey study. Perspectives in Education, 39(3), 197-213. This article presents findings from an anonymous survey administered to South African university students, in which students were asked to respond to the prompts I would enjoy statistics more if and I would perform better in statistics if. The students responses were compared to the recommended best practices in the literature (see the GAISE report in references 5 and 13). This paper is leveraged in the workshop to show alignment between students self-perceived needs and best practices, which include emphasizing how students can apply statistics in their field and incorporating active learning strategies. 4: Daniel, F., &amp; Braasch, J. L. (2013). Application exercises improve transfer of statistical knowledge in real-world situations. Teaching of Psychology, 40(3), 200-207. This paper summarizes results from an observational study involving two groups of undergraduate students enrolled psychology statistics courses in Indiana. One of the two groups completed several real-world application exercises; the results of this study indicated that students who participated in these exercises were better able to use statistical knowledge when answering far-transfer questions. In the workshop, this article provided empirical evidence that incorporating real-world application exercises into introductory statistics courses improves more than just student performance on course assessments. 5: GAISE College Group. Aliaga, M., Cobb, G., Cuff, C., Garfield, J., Gould, R., Lock, R., Moore, T., Rossman, A., Stephenson, B., Utts, J., Velleman, P., and Witmer, J. (2005), Guidelines for assessment and instruction in statistics education: College report. Alexandria, VA: American Statistical Association. This report was created from the Guidelines for Assessment and Instruction in Statistics Education (GAISE) Project, which gave rise to six recommendations for undergraduate statistics courses to promote statistical literacy. These recommendations were referenced throughout the other sources in this annotated bibliography. The updated guidelines from 2018 (see reference 13) were stated in the workshop, but this source was used to add credibility to the updated guidelines given that the updates are very minor. Several of these guidelines were also incorporated into the workshops interactive application made for this workshop. 6: Gordon, S. P., &amp; Gordon, F. S. (2020). Visualizing and Understanding Hypothesis Testing Using Dynamic Software. Primus, 30(2), 172-190. This article describes several dynamic applications that were created in Microsoft Excel to promote conceptual understanding of hypothesis testing over rote memorization of formulas and procedures. The use of this dynamic software aligns with the recommendations from the GAISE report. These applications are freely available and do not require users to have extensive coding experience; slider inputs are used to update the interactive graphical interface. The guidance for developing dynamic software detailed in this paper was incorporated into the interactive application created for this workshop. 7: Kalaian, S. A., &amp; Kasim, R. M. (2014). A meta-analytic review of studies of the effectiveness of small-group learning methods on statistics achievement. Journal of Statistics Education, 22(1). This meta-analytic study examines the effectiveness of small-group learning methods with respect to lecture-based instruction in undergraduate statistics courses. Cooperative and collaborative small-group learning activities were found to be positively associated with student performance; however, the meta-analysis suggested that this association is becoming weaker over time. Only nine studies satisfied the inclusion criteria for this meta-analysis, and this small sample size should be considered when interpreting the conclusions. This study was referenced in the workshop to suggest that the incorporation of active learning methods consistent with the GAISE report has improved student engagement and performance in statistics courses. 8: Reaburn, R. (2014). Introductory statistics course tertiary students understanding of p-values. Statistics Education Research Journal, 13(1), 53-65. This article presents findings from a four-semester study conducted to obtain knowledge about students beliefs and difficulties in understanding p-values. Each semester, new instructional methods were incorporated into the course, and students later answered two questions about p-values. A combination of interventions based on computer simulation, statistical communication, and scientific reasoning were found to be associated with an improved ability to define and use p-values in null hypothesis significance testing procedures. In the workshop, this source was leveraged to support the value of using computer simulation to introduce p-values and to inform the development of the workshops interactive application. 9: Rossman, A. J., &amp; Chance, B. L. (2014). Using simulationbased inference for learning introductory statistics. Wiley Interdisciplinary Reviews: Computational Statistics, 6(4), 211-221. This paper resolves to address the criticism that topics in introductory statistics classes are often compartmentalized and that key ideas of inference receive rushed treatment at the end of these courses. The authors propose resequencing introductory statistics courses based on data structure; they are proponents of moving inferential concepts to the beginning of such courses, which allows for complete and repeated exposure to these concepts. They also advocate for using simulation to introduce inferential methods. The workshop referred to this source because the proposed resequencing of statistics courses could allow for more active learning opportunities. 10: Shinaberger, L. (2017). Components of a flipped classroom influencing student success in an undergraduate business statistics course. Journal of Statistics Education, 25(3), 122-130. This article details a study that converted an undergraduate business statistics course from a traditional lecture to a flipped classroom course. Incremental changes to the course were made over 10 semesters. Of these changes, replacing face-to-face lectures with active learning exercises and using quizzes to verify student engagement with the lecture videos were most associated with improved student exam performance. In the workshop, the results and recommendations from this source were compared with those from another study about flipped statistics classrooms (see reference 1) to give a more balanced perspective. 11: Steel, E. A., Liermann, M., &amp; Guttorp, P. (2019). Beyond calculations: A course in statistical thinking. The American Statistician, 73(sup1), 392-401. This paper overviews a course in statistical thinking that was developed at the University of Washington for senior undergraduate statistics majors. This course is motivated by the GAISE report and the notion that a large proportion of statistical errors result from incorrect interpretations despite correct numerical calculations. The course aims to promote a gut-level understanding of statistical topics by providing students opportunities to engage with simulation, scientific communication, and common statistical misconceptions. Most other sources in this annotated bibliography pertain to introductory statistics courses, so the workshop used this article to demonstrate that active learning strategies are also helpful for more advanced students. 12: Tintle, N., Chance, B., Cobb, G., Roy, S., Swanson, T., &amp; VanderStoep, J. (2015). Combating anti-statistical thinking using simulation-based methods throughout the undergraduate curriculum. The American Statistician, 69(4), 362-370. This article acknowledges that many students are overconfident in or skeptical of inferential statistics upon completing their undergraduate statistics courses. This work recommends teaching more simulation-based methods in both introductory and senior undergraduate statistics courses. Simulation is conducive to active learning and promotes statistical thinking in introductory courses; moreover, computational methods must be more widely adopted in advanced statistics courses to reflect the changing job market. In the workshop, this paper complemented the information from reference 9, which advocates for greater incorporation of simulation into introductory statistics courses. 13: Wood, B. L., Mocko, M., Everson, M., Horton, N. J., &amp; Velleman, P. (2018). Updated guidelines, updated curriculum: the GAISE college report and introductory statistics for the modern student. Chance, 31(2), 53-59. This source updates the recommendations from the original GAISE report (see reference 5) to reflect the increased availability of technology and prevalence of alternative learning environments. The updated guidelines incorporate minor wording changes, and two of the original recommendations were reordered to prioritize what to teach in introductory statistics courses over how to teach those courses. These updated guidelines were introduced at the beginning of - and revisited throughout - the workshop and to ensure the workshops content was grounded in the literature. "],["references.html", "Chapter 10 References", " Chapter 10 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
